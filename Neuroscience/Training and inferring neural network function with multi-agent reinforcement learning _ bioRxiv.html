<!DOCTYPE html>
<html lang="en" dir="ltr" 
  xmlns="http://www.w3.org/1999/xhtml"
  xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <head prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# book: http://ogp.me/ns/book#" >
    <!--[if IE]><![endif]-->
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="dns-prefetch" href="//stats.g.doubleclick.net" />
<link rel="dns-prefetch" href="//www.google-analytics.com" />
<link rel="dns-prefetch" href="//scholar.google.com" />
<link rel="dns-prefetch" href="//d33xdlntwy0kbs.cloudfront.net" />
<link rel="shortcut icon" href="https://www.biorxiv.org/sites/default/files/images/favicon.ico" type="image/vnd.microsoft.icon" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1, user-scalable=yes" />
<meta name="article_thumbnail" content="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/embed/inline-graphic-1.gif" />
<meta name="type" content="article" />
<meta name="category" content="article" />
<meta name="HW.identifier" content="/biorxiv/early/2020/02/06/598086.atom" />
<meta name="HW.pisa" content="biorxiv;598086v3" />
<meta name="DC.Format" content="text/html" />
<meta name="DC.Language" content="en" />
<meta name="DC.Title" content="Training and inferring neural network function with multi-agent reinforcement learning" />
<meta name="DC.Identifier" content="10.1101/598086" />
<meta name="DC.Date" content="2020-02-06" />
<meta name="DC.Publisher" content="Cold Spring Harbor Laboratory" />
<meta name="DC.Rights" content="© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/" />
<meta name="DC.AccessRights" content="restricted" />
<meta name="DC.Description" content="A central goal in systems neuroscience is to understand the functions performed by neural circuits. Previous top-down models addressed this question by comparing the behaviour of an ideal model circuit, optimised to perform a given function, with neural recordings. However, this requires guessing in advance what function is being performed, which may not be possible for many neural systems. To address this, we propose a new framework for optimising a recurrent network using multi-agent reinforcement learning (RL). In this framework, a reward function quantifies how desirable each state of the network is for performing a given function. Each neuron is treated as an ‘agent’, which optimises its responses so as to drive the network towards rewarded states. Three applications follow from this. First, one can use multi-agent RL algorithms to optimise a recurrent neural network to perform diverse functions (e.g. efficient sensory coding or motor control). Second, one could use inverse RL to infer the function of a recorded neural network from data. Third, the theory predicts how neural networks should adapt their dynamics to maintain the same function when the external environment or network structure changes. This could lead to theoretical predictions about how neural network dynamics adapt to deal with cell death and/or varying sensory stimulus statistics." />
<meta name="DC.Contributor" content="Matthew Chalk" />
<meta name="DC.Contributor" content="Gasper Tkacik" />
<meta name="DC.Contributor" content="Olivier Marre" />
<meta name="article:published_time" content="2020-02-06" />
<meta name="article:section" content="New Results" />
<meta name="citation_title" content="Training and inferring neural network function with multi-agent reinforcement learning" />
<meta name="citation_abstract" lang="en" content="&lt;p&gt;A central goal in systems neuroscience is to understand the functions performed by neural circuits.  Previous top-down models addressed this question by comparing the behaviour of an ideal model circuit, optimised to perform a given function, with neural recordings. However, this requires guessing in advance what function is being performed, which may not be possible for many neural systems. To address this, we propose a new framework for optimising a recurrent network using multi-agent reinforcement learning (RL).  In this framework, a reward function quantifies how desirable each state of the network is for performing a given function. Each neuron is treated as an agent, which optimises its responses so as to drive the network towards rewarded states. Three applications follow from this. First, one can use multi-agent RL algorithms to optimise a recurrent neural network to perform diverse functions (e.g. efficient sensory coding or motor control). Second, one could use inverse RL to infer the function of a recorded neural network from data. Third, the theory predicts how neural networks should adapt their dynamics to maintain the same function when the external environment or network structure changes. This could lead to theoretical predictions about how neural network dynamics adapt to deal with cell death and/or varying sensory stimulus statistics.&lt;/p&gt;" />
<meta name="citation_journal_title" content="bioRxiv" />
<meta name="citation_publisher" content="Cold Spring Harbor Laboratory" />
<meta name="citation_publication_date" content="2020/01/01" />
<meta name="citation_mjid" content="biorxiv;598086v3" />
<meta name="citation_id" content="598086v3" />
<meta name="citation_public_url" content="https://www.biorxiv.org/content/10.1101/598086v3" />
<meta name="citation_abstract_html_url" content="https://www.biorxiv.org/content/10.1101/598086v3.abstract" />
<meta name="citation_full_html_url" content="https://www.biorxiv.org/content/10.1101/598086v3.full" />
<meta name="citation_pdf_url" content="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086.full.pdf" />
<meta name="citation_doi" content="10.1101/598086" />
<meta name="citation_num_pages" content="24" />
<meta name="citation_article_type" content="Article" />
<meta name="citation_section" content="New Results" />
<meta name="citation_firstpage" content="598086" />
<meta name="citation_author" content="Matthew Chalk" />
<meta name="citation_author_institution" content="Sorbonne Université, INSERM, CNRS, Institut de la Vision" />
<meta name="citation_author_email" content="matthew.chalk@inserm.fr" />
<meta name="citation_author_orcid" content="http://orcid.org/0000-0001-7782-4436" />
<meta name="citation_author" content="Gasper Tkacik" />
<meta name="citation_author_institution" content="IST Austria" />
<meta name="citation_author_orcid" content="http://orcid.org/0000-0002-6699-1455" />
<meta name="citation_author" content="Olivier Marre" />
<meta name="citation_author_institution" content="Sorbonne Université, INSERM, CNRS, Institut de la Vision" />
<meta name="citation_author_orcid" content="http://orcid.org/0000-0002-0090-6190" />
<meta name="citation_reference" content="Yang G R, Joglekar MR, Song HF, Newsome WT, Wang XJ. (2019). Task representations in neural networks trained to perform many cognitive tasks. Nat Neurosci, 22:297–306s" />
<meta name="citation_reference" content="Heeger D J (2017) Theory of cortical function. Proc Natl Acad Sci USA 114:1773–1782" />
<meta name="citation_reference" content="Sussillo D, Abbott L F (2009) Generating coherent patterns of activity from chaotic neural networks. Neuron 63:544–557." />
<meta name="citation_reference" content="Gütig R (2016) Spiking neurons can discover predictive features by aggregate–label learning. Science 351(6277):aab4113" />
<meta name="citation_reference" content="Hopfield JJ (1982) Neural networks and physical systems with emergent collective computational abilities Proc Natl Acad Sci USA 79:2554–2558" />
<meta name="citation_reference" content="Körding K (2007) Decision theory: what should the nervous system do? Science 318:606–610" />
<meta name="citation_reference" content="Boerlin M, Machens CK, Denève S (2013) Predictive coding of dynamical variables in balanced spiking networks. PLoS Comp Bio 9 e1003258." />
<meta name="citation_reference" content="Simoncelli EP, Olshausen BA (2001) Natural image statistics and neural representation. Ann Rev Neurosci 24:1193–1216" />
<meta name="citation_reference" content="Tkačik G, Prentice JS, Balasubramanian V, Schneidman E (2010) Optimal population coding by noisy spiking neurons. Proc Natl Acad Sci USA 107:14419–14424." />
<meta name="citation_reference" content="Chalk M, Marre O, Tkačik G (2018) Toward a unified theory of efficient, predictive, and sparse coding. Proc Natl Acad Sci USA 115:186–191." />
<meta name="citation_reference" content="citation_title=Sensory Communication;citation_pages=217-234;citation_year=1961" />
<meta name="citation_reference" content="Field DJ (1994) What is the goal of sensory coding? Neural Comput 6:559–601." />
<meta name="citation_reference" content="Gjorgjieva J, Sompolinsky H, Meister M (2014) Benefits of pathway splitting in sensory coding. J Neurosci 34:12127–12144." />
<meta name="citation_reference" content="Sutton RS, Barto AG (2018) Reinforcement learning: An introduction. MIT press." />
<meta name="citation_reference" content="Todorov E (2008) General duality between optimal control and estimation. Proc of the 47th IEEE Conference on Decision and Control 4286–4292" />
<meta name="citation_reference" content="Haarnoja T, Tang H, Abbeel P, Levine S (2017). Reinforcement learning with deep energy-based policies. Proc 34th International Conf on Machine Learning 70:1352–1361" />
<meta name="citation_reference" content="Mahadevan S. (1996). Average reward reinforcement learning: Foundations, algorithms, and empirical results. Machine learning 22:159–195." />
<meta name="citation_reference" content="Ng AY, Russell SJ (2000) Algorithms for inverse reinforcement learning. Proc of the 17th International Con on Machine Learning pp. 663–670" />
<meta name="citation_reference" content="Rothkopf CA, Dimitrakakis C (2011) Preference elicitation and inverse reinforcement learning. In Joint European conference on machine learning and knowledge discovery in databases Springer pp. 34–48." />
<meta name="citation_reference" content="Herman M, Gindele T, Wagner J, Schmitt F, Burgard W (2016) Inverse reinforcement learning with simultaneous estimation of rewards and dynamics. Artificial Intelligence and Statistics 102–110" />
<meta name="citation_reference" content="Berger T. Rate Distortion Theory. (1971) Englewood Clis." />
<meta name="citation_reference" content="Bialek W, van Steveninck RRDR, Tishby N (2006) Efficient representation as a design principle for neural coding and computation. IEEE international symposium on information theory 659–663" />
<meta name="citation_reference" content="Schneidman E, Berry MJ, Segev R, Bialek W (2006) Weak pairwise correlations imply strongly correlated network states in a neural population. Nature 440:1007–1012" />
<meta name="citation_reference" content="Tkačik G, Marre O, Amodei D, Schneidman E, Bialek W, Berry MJ (2014) Searching for collective behavior in a large network of sensory neurons. PLoS Comp Bio 10:e1003408." />
<meta name="citation_reference" content="Ben-Yishai R, Bar-Or RL, Sompolinsky H (1995) Theory of orientation tuning in visual cortex. Proc Natl Acad Sci, 92:3844–3848" />
<meta name="citation_reference" content="Zhang K (1996) Representation of spatial orientation by the intrinsic dynamics of the head-direction cell ensemble: a theory. J Neurosci 16:2112–2126." />
<meta name="citation_reference" content="Kim SS, Rouault H, Druckmann S, Jayaraman V (2017) Ring attractor dynamics in the Drosophila central brain. Science 356:849–853." />
<meta name="citation_reference" content="Pillow JW, Shlens J, Paninski L, Sher A, Litke AM, Chichilnisky EJ, Simoncelli EP (2008) Spatio-temporal correlations and visual signalling in a complete neuronal population. Nature 454:995–999" />
<meta name="citation_reference" content="McIntosh L, Maheswaranathan N, Nayebi A, Ganguli S, Baccus S (2016) Deep learning models of the retinal response to natural scenes. Adv Neur Inf Proc Sys 29:1369–1377" />
<meta name="citation_reference" content="Cunningham JP, Yu BM (2014) Dimensionality reduction for large-scale neural recordings. Nat Neurosci 17:1500–1509" />
<meta name="citation_reference" content="Rubin A, Sheintuch L, Brande-Eilat N, Pinchasof O, Rechavi Y, Geva N, Ziv Y (2019) Revealing neural correlates of behavior without behavioral measurements. bioRxiv:540195" />
<meta name="citation_reference" content="Chaudhuri R, Gercek B, Pandey B, Peyrache A, Fiete I (2019) The population dynamics of a canonical cognitive circuit. bioRxiv: 516021" />
<meta name="citation_reference" content="Goddard E, Klein C, Solomon SG, Hogendoorn H, Carlson TA (2018) Interpreting the dimensions of neural feature representations revealed by dimensionality reduction NeuroImage 180:41–67" />
<meta name="citation_reference" content="Sharpee T, Rust NT, Bialek W (2003) Maximally informative dimensions: analyzing neural responses to natural signals. Adv Neur Inf Proc Sys 277–284" />
<meta name="citation_reference" content="Niv Y (2009) Reinforcement learning in the brain. J Mathemat Psychol 53:139–154" />
<meta name="citation_reference" content="Dayan P, Niv Y (2008) Reinforcement learning: the good, the bad and the ugly. Curr Op Neurobio 18:185–196." />
<meta name="citation_reference" content="Daw ND, Doya K (2006) The computational neurobiology of learning and reward. Curr Op Neurobio 16:199–204." />
<meta name="citation_reference" content="Fairhall AL, Geoffrey DL, William B, de Ruyter van Steveninck RR. (2001) Efficiency and ambiguity in an adaptive neural code. Nature 412:787." />
<meta name="citation_reference" content="Benucci A, Saleem AB, Carandini M. (2013). Adaptation maintains population homeostasis in primary visual cortex. Nat Neurosci 16:724." />
<meta name="citation_reference" content="Li N, Kayvon D, Karel S, and Shaul D. (2016) Robust neuronal dynamics in premotor cortex during motor planning. Nature. 532:459." />
<meta name="citation_reference" content="Mlynarski W, Hledik M, Sokolowski TR, Tkacik G (2019). Statistical analysis and optimality of biological systems. bioRxiv:848374." />
<meta name="twitter:title" content="Training and inferring neural network function with multi-agent reinforcement learning" />
<meta name="twitter:site" content="@biorxivpreprint" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:image" content="https://www.biorxiv.org/sites/default/files/images/biorxiv_logo_homepage7-5-small.png" />
<meta name="twitter:description" content="A central goal in systems neuroscience is to understand the functions performed by neural circuits. Previous top-down models addressed this question by comparing the behaviour of an ideal model circuit, optimised to perform a given function, with neural recordings. However, this requires guessing in advance what function is being performed, which may not be possible for many neural systems. To address this, we propose a new framework for optimising a recurrent network using multi-agent reinforcement learning (RL). In this framework, a reward function quantifies how desirable each state of the network is for performing a given function. Each neuron is treated as an ‘agent’, which optimises its responses so as to drive the network towards rewarded states. Three applications follow from this. First, one can use multi-agent RL algorithms to optimise a recurrent neural network to perform diverse functions (e.g. efficient sensory coding or motor control). Second, one could use inverse RL to infer the function of a recorded neural network from data. Third, the theory predicts how neural networks should adapt their dynamics to maintain the same function when the external environment or network structure changes. This could lead to theoretical predictions about how neural network dynamics adapt to deal with cell death and/or varying sensory stimulus statistics." />
<meta name="og-title" property="og:title" content="Training and inferring neural network function with multi-agent reinforcement learning" />
<meta name="og-url" property="og:url" content="https://www.biorxiv.org/content/10.1101/598086v3" />
<meta name="og-site-name" property="og:site_name" content="bioRxiv" />
<meta name="og-description" property="og:description" content="A central goal in systems neuroscience is to understand the functions performed by neural circuits. Previous top-down models addressed this question by comparing the behaviour of an ideal model circuit, optimised to perform a given function, with neural recordings. However, this requires guessing in advance what function is being performed, which may not be possible for many neural systems. To address this, we propose a new framework for optimising a recurrent network using multi-agent reinforcement learning (RL). In this framework, a reward function quantifies how desirable each state of the network is for performing a given function. Each neuron is treated as an ‘agent’, which optimises its responses so as to drive the network towards rewarded states. Three applications follow from this. First, one can use multi-agent RL algorithms to optimise a recurrent neural network to perform diverse functions (e.g. efficient sensory coding or motor control). Second, one could use inverse RL to infer the function of a recorded neural network from data. Third, the theory predicts how neural networks should adapt their dynamics to maintain the same function when the external environment or network structure changes. This could lead to theoretical predictions about how neural network dynamics adapt to deal with cell death and/or varying sensory stimulus statistics." />
<meta name="og-type" property="og:type" content="article" />
<meta name="og-image" property="og:image" content="https://www.biorxiv.org/sites/default/files/images/biorxiv_logo_homepage7-5-small.png" />
<meta name="citation_date" content="2020-02-06" />
<link rel="alternate" type="application/pdf" title="Full Text (PDF)" href="/content/10.1101/598086v3.full.pdf" />
<link rel="alternate" type="text/plain" title="Full Text (Plain)" href="/content/10.1101/598086v3.full.txt" />
<link rel="alternate" type="application/vnd.ms-powerpoint" title="Powerpoint" href="/content/10.1101/598086v3.ppt" />
<meta name="description" content="bioRxiv - the preprint server for biology, operated by Cold Spring Harbor Laboratory, a research and educational institution" />
<meta name="generator" content="Drupal 7 (http://drupal.org)" />
<link rel="canonical" href="https://www.biorxiv.org/content/10.1101/598086v3" />
<link rel="shortlink" href="https://www.biorxiv.org/node/1137937" />
    <title>Training and inferring neural network function with multi-agent reinforcement learning | bioRxiv</title>  
    <link type="text/css" rel="stylesheet" href="https://www.biorxiv.org/sites/default/files/advagg_css/css__jMRAK66KMC1e4TQlwUNn3KiWVDC5AjueUAYEm1xBY_U__KSUjdT4jdcJ4qJz6fKY1K9WkYh1a5EcaWZxt_-zbTis__dPhHATyREQrnntSIgxU8XNoYny4rjInxP7lLKSNxQyY.css" media="all" />
<link type="text/css" rel="stylesheet" href="https://www.biorxiv.org/sites/all/modules/highwire/highwire/highwire.style.highwire.css?q5yzns" media="all" />
<link type="text/css" rel="stylesheet" href="https://www.biorxiv.org/sites/default/files/advagg_css/css__qlYh2rDEXmsOlJKnxoaEiN2PoZU7YzDXSSTk6K1VAQE__XkaAZGAgxyDmOmv0mrGL0sA-jzEgAbgA12amvVPlIf0__dPhHATyREQrnntSIgxU8XNoYny4rjInxP7lLKSNxQyY.css" media="all" />
<link type="text/css" rel="stylesheet" href="//cdn.jsdelivr.net/qtip2/2.2.1/jquery.qtip.min.css" media="all" />
<link type="text/css" rel="stylesheet" href="https://www.biorxiv.org/sites/default/files/advagg_css/css__GFpBMW7A95ZNl4oKhiI_0u7WD_8tFHiDfbKIVNkYgis__AQm9HArb2Y_D7UzgraiLIwAB7088fIq2iZD0Uigie_s__dPhHATyREQrnntSIgxU8XNoYny4rjInxP7lLKSNxQyY.css" media="all" />
<style type="text/css" media="all">
/* <![CDATA[ */
#sliding-popup.sliding-popup-bottom,#sliding-popup.sliding-popup-bottom .eu-cookie-withdraw-banner,.eu-cookie-withdraw-tab{background:gray}#sliding-popup.sliding-popup-bottom.eu-cookie-withdraw-wrapper{background:transparent}#sliding-popup .popup-content #popup-text h1,#sliding-popup .popup-content #popup-text h2,#sliding-popup .popup-content #popup-text h3,#sliding-popup .popup-content #popup-text p,.eu-cookie-compliance-secondary-button,.eu-cookie-withdraw-tab{color:#fff !important}.eu-cookie-withdraw-tab{border-color:#fff}.eu-cookie-compliance-more-button{color:#fff !important}
/* ]]> */
</style>

<!--[if lte IE 7]>
<link type="text/css" rel="stylesheet" href="https://www.biorxiv.org/sites/default/files/advagg_css/css__ElJr3PIJEvw3qLXc1cnYiLj2G4KgDPSXFOfm6Phf8hw__JdWGm15cDWjsK6KrFlQVXQix9YgNeYysf22XZHj-Y-c__dPhHATyREQrnntSIgxU8XNoYny4rjInxP7lLKSNxQyY.css" media="all" />
<![endif]-->
<link type="text/css" rel="stylesheet" href="https://www.biorxiv.org/sites/default/files/advagg_css/css__B6tGdsQSt1vT8d8j-19-oS4FKJe83RrSNBkjPs9jfos__6VkE5WcQmmQpSVTQx-xS2-zn_nlwFlDm-EN1HLJNjrA__dPhHATyREQrnntSIgxU8XNoYny4rjInxP7lLKSNxQyY.css" media="all" />
<link type="text/css" rel="stylesheet" href="https://www.biorxiv.org/sites/default/files/advagg_css/css__Cm80tkaB8yhCBXUaefYJe7GLLFWYsVxjke4GKdtw7XY__F3sVKlGyHKCaAPI_WdiHD7l_-05B6b9bLdUwzi1me90__dPhHATyREQrnntSIgxU8XNoYny4rjInxP7lLKSNxQyY.css" media="all" />

<!--[if (lt IE 9)&(!IEMobile)]>
<link type="text/css" rel="stylesheet" href="https://www.biorxiv.org/sites/default/files/advagg_css/css__XH6bpcI0f2dImc-p674DLCZtWBGb-QwxJK1YexVGtno__vUceGprdo5nIhV6DH93X7fI3r8RcTJbChbas9TQXeW4__dPhHATyREQrnntSIgxU8XNoYny4rjInxP7lLKSNxQyY.css" media="all" />
<![endif]-->

<!--[if gte IE 9]><!-->
<link type="text/css" rel="stylesheet" href="https://www.biorxiv.org/sites/default/files/advagg_css/css__2WBMox6sOrN42ss5lCnH7WWVRdFdJCxtTKnQJYRwTE4__yqNvNYLvMpjy3ffuJrjjm9uW2i-Me1c23KLYuWHaqio__dPhHATyREQrnntSIgxU8XNoYny4rjInxP7lLKSNxQyY.css" media="all" />
<!--<![endif]-->
<link type="text/css" rel="stylesheet" href="https://www.biorxiv.org/sites/default/files/advagg_css/css__O9AieMsHBlZYeQh-sAOhdoS_nAEDhkj0jDZjGc_kUug__f_9pwrTE8mmshoJRfurwkQ36QonD_Jiwbop2sscnkrE__dPhHATyREQrnntSIgxU8XNoYny4rjInxP7lLKSNxQyY.css" media="all" />
<link type="text/css" rel="stylesheet" href="https://d33xdlntwy0kbs.cloudfront.net/cshl_custom.css" media="all" />
    <script type="text/javascript">
<!--//--><![CDATA[//><!--
/*!
 * yepnope1.5.4
 * (c) WTFPL, GPLv2
 */
(function(a,b,c){function d(a){return"[object Function]"==o.call(a)}function e(a){return"string"==typeof a}function f(){}function g(a){return!a||"loaded"==a||"complete"==a||"uninitialized"==a}function h(){var a=p.shift();q=1,a?a.t?m(function(){("c"==a.t?B.injectCss:B.injectJs)(a.s,0,a.a,a.x,a.e,1)},0):(a(),h()):q=0}function i(a,c,d,e,f,i,j){function k(b){if(!o&&g(l.readyState)&&(u.r=o=1,!q&&h(),l.onload=l.onreadystatechange=null,b)){"img"!=a&&m(function(){t.removeChild(l)},50);for(var d in y[c])y[c].hasOwnProperty(d)&&y[c][d].onload()}}var j=j||B.errorTimeout,l=b.createElement(a),o=0,r=0,u={t:d,s:c,e:f,a:i,x:j};1===y[c]&&(r=1,y[c]=[]),"object"==a?l.data=c:(l.src=c,l.type=a),l.width=l.height="0",l.onerror=l.onload=l.onreadystatechange=function(){k.call(this,r)},p.splice(e,0,u),"img"!=a&&(r||2===y[c]?(t.insertBefore(l,s?null:n),m(k,j)):y[c].push(l))}function j(a,b,c,d,f){return q=0,b=b||"j",e(a)?i("c"==b?v:u,a,b,this.i++,c,d,f):(p.splice(this.i++,0,a),1==p.length&&h()),this}function k(){var a=B;return a.loader={load:j,i:0},a}var l=b.documentElement,m=a.setTimeout,n=b.getElementsByTagName("script")[0],o={}.toString,p=[],q=0,r="MozAppearance"in l.style,s=r&&!!b.createRange().compareNode,t=s?l:n.parentNode,l=a.opera&&"[object Opera]"==o.call(a.opera),l=!!b.attachEvent&&!l,u=r?"object":l?"script":"img",v=l?"script":u,w=Array.isArray||function(a){return"[object Array]"==o.call(a)},x=[],y={},z={timeout:function(a,b){return b.length&&(a.timeout=b[0]),a}},A,B;B=function(a){function b(a){var a=a.split("!"),b=x.length,c=a.pop(),d=a.length,c={url:c,origUrl:c,prefixes:a},e,f,g;for(f=0;f<d;f++)g=a[f].split("="),(e=z[g.shift()])&&(c=e(c,g));for(f=0;f<b;f++)c=x[f](c);return c}function g(a,e,f,g,h){var i=b(a),j=i.autoCallback;i.url.split(".").pop().split("?").shift(),i.bypass||(e&&(e=d(e)?e:e[a]||e[g]||e[a.split("/").pop().split("?")[0]]),i.instead?i.instead(a,e,f,g,h):(y[i.url]?i.noexec=!0:y[i.url]=1,f.load(i.url,i.forceCSS||!i.forceJS&&"css"==i.url.split(".").pop().split("?").shift()?"c":c,i.noexec,i.attrs,i.timeout),(d(e)||d(j))&&f.load(function(){k(),e&&e(i.origUrl,h,g),j&&j(i.origUrl,h,g),y[i.url]=2})))}function h(a,b){function c(a,c){if(a){if(e(a))c||(j=function(){var a=[].slice.call(arguments);k.apply(this,a),l()}),g(a,j,b,0,h);else if(Object(a)===a)for(n in m=function(){var b=0,c;for(c in a)a.hasOwnProperty(c)&&b++;return b}(),a)a.hasOwnProperty(n)&&(!c&&!--m&&(d(j)?j=function(){var a=[].slice.call(arguments);k.apply(this,a),l()}:j[n]=function(a){return function(){var b=[].slice.call(arguments);a&&a.apply(this,b),l()}}(k[n])),g(a[n],j,b,n,h))}else!c&&l()}var h=!!a.test,i=a.load||a.both,j=a.callback||f,k=j,l=a.complete||f,m,n;c(h?a.yep:a.nope,!!i),i&&c(i)}var i,j,l=this.yepnope.loader;if(e(a))g(a,0,l,0);else if(w(a))for(i=0;i<a.length;i++)j=a[i],e(j)?g(j,0,l,0):w(j)?B(j):Object(j)===j&&h(j,l);else Object(a)===a&&h(a,l)},B.addPrefix=function(a,b){z[a]=b},B.addFilter=function(a){x.push(a)},B.errorTimeout=1e4,null==b.readyState&&b.addEventListener&&(b.readyState="loading",b.addEventListener("DOMContentLoaded",A=function(){b.removeEventListener("DOMContentLoaded",A,0),b.readyState="complete"},0)),a.yepnope=k(),a.yepnope.executeStack=h,a.yepnope.injectJs=function(a,c,d,e,i,j){var k=b.createElement("script"),l,o,e=e||B.errorTimeout;k.src=a;for(o in d)k.setAttribute(o,d[o]);c=j?h:c||f,k.onreadystatechange=k.onload=function(){!l&&g(k.readyState)&&(l=1,c(),k.onload=k.onreadystatechange=null)},m(function(){l||(l=1,c(1))},e),i?k.onload():n.parentNode.insertBefore(k,n)},a.yepnope.injectCss=function(a,c,d,e,g,i){var e=b.createElement("link"),j,c=i?h:c||f;e.href=a,e.rel="stylesheet",e.type="text/css";for(j in d)e.setAttribute(j,d[j]);g||(n.parentNode.insertBefore(e,n),m(c,0))}})(this,document);

//--><!]]>
</script>
<script type="text/javascript" src="https://www.biorxiv.org/sites/all/libraries/modernizr/modernizr.min.js?q5yzns"></script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
yepnope({
  test: Modernizr.matchmedia,
  nope: '/sites/all/libraries/media-match/media.match.min.js'
});
//--><!]]>
</script>
<script type="text/javascript" src="https://www.biorxiv.org/sites/default/files/advagg_js/js__yOhrJUBmMpN-jsuQlJYmS8ovkQU6I6KInr295HrSTb0__c--zHhbuOWmkdWkJYXiNWDmkpDRPesmbBjGlQcyqBOY__dPhHATyREQrnntSIgxU8XNoYny4rjInxP7lLKSNxQyY.js"></script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
var _prum=[['id', '52e95df3abe53d2e33000000'], ['mark', 'firstbyte',
      (new Date()).getTime()]]; (function() {
      var s=document.getElementsByTagName('script')[0],
      p=document.createElement('script');
      p.async='async'; p.src='//rum-static.pingdom.net/prum.min.js';s.parentNode.insertBefore(p,s);})();
//--><!]]>
</script>
<script type="text/javascript" src="https://www.biorxiv.org/sites/default/files/advagg_js/js___KyvuEvfmewWxdOLBXTYsSUo8Pfm_DijV6fWtGvSL_k__HSKkTgIr7nDcdvQYewipf6v4ak6ZLUOQkwPj44wKouA__dPhHATyREQrnntSIgxU8XNoYny4rjInxP7lLKSNxQyY.js"></script>
<script type="text/javascript" async="async" src="https://scholar.google.com/scholar_js/casa.js"></script>
<script type="text/javascript" src="https://www.biorxiv.org/sites/default/files/advagg_js/js__OGPr3wJNGw2BAY3Tj6TjIBwKmjQh5i-7muOaPrr0MFA__fCWiU_pDm6xztXUSccH7zrf5422-kgOkuLdWyafrH_0__dPhHATyREQrnntSIgxU8XNoYny4rjInxP7lLKSNxQyY.js"></script>
<script type="text/javascript" src="//cdn.jsdelivr.net/qtip2/2.2.1/jquery.qtip.min.js"></script>
<script type="text/javascript" src="https://www.biorxiv.org/sites/default/files/advagg_js/js__hqoKSXYZqkcZkXP--ltzsy0Y3txW5TJBw6HT1khr30Y__6_HqNYoBVMxlkWZwdscqCLCtCxpOhl9_rIOpTDdaI0k__dPhHATyREQrnntSIgxU8XNoYny4rjInxP7lLKSNxQyY.js"></script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
if(typeof window.MathJax === "undefined") window.MathJax = { menuSettings: { zoom: "Click" } };
//--><!]]>
</script>
<script type="text/javascript" src="https://www.biorxiv.org/sites/default/files/advagg_js/js__lrItkPGmrIuH-E_sdUWEK2i9eTqLar16BAHVxNTWqrc__FhtnljBrjuzYWWrT_f4OYV4jtMBituhMpusb3h0m0rY__dPhHATyREQrnntSIgxU8XNoYny4rjInxP7lLKSNxQyY.js"></script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,"script","//www.google-analytics.com/analytics.js","ga");ga("create", "UA-45638026-1", {"cookieDomain":"auto"});ga("set", "page", location.pathname + location.search + location.hash);ga("send", "pageview");ga('create', 'UA-189672-38', 'auto', {'name': 'hwTracker'});
ga('set', 'anonymizeIp', true);
ga('hwTracker.send', 'pageview');
//--><!]]>
</script>
<script type="text/javascript" src="https://www.biorxiv.org/sites/default/files/advagg_js/js__1PtAOZ8x7l6QAi8D7X6Kj_Y7HcUOFzvWRvvYHUcCT98__U8IB92u_CnnGwJ-X_xaUa9lzsC8OzUfN0WkK2JQfirg__dPhHATyREQrnntSIgxU8XNoYny4rjInxP7lLKSNxQyY.js"></script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
jQuery.extend(Drupal.settings,{"basePath":"\/","pathPrefix":"","ajaxPageState":{"theme":"jcore_1","theme_token":"w1Rs8uQWhVnbzCT6HJ8-eP4FVp0_JmPqRsGLsRozlqg","css":{"modules\/system\/system.base.css":1,"modules\/system\/system.menus.css":1,"modules\/system\/system.messages.css":1,"modules\/system\/system.theme.css":1,"misc\/ui\/jquery.ui.core.css":1,"misc\/ui\/jquery.ui.theme.css":1,"misc\/ui\/jquery.ui.button.css":1,"misc\/ui\/jquery.ui.resizable.css":1,"misc\/ui\/jquery.ui.dialog.css":1,"misc\/ui\/jquery.ui.tooltip.css":1,"modules\/comment\/comment.css":1,"sites\/all\/modules\/contrib\/date\/date_api\/date.css":1,"sites\/all\/modules\/contrib\/date\/date_popup\/themes\/datepicker.1.7.css":1,"modules\/field\/theme\/field.css":1,"sites\/all\/modules\/highwire\/highwire\/highwire_draw\/css\/highwire-draw.css":1,"sites\/all\/modules\/highwire\/highwire\/highwire_folders\/highwire_folders.css":1,"sites\/all\/modules\/highwire\/highwire\/highwire_responsive\/css\/highwire-responsive.css":1,"sites\/all\/modules\/highwire\/highwire\/highwire_saved_searches\/highwire_saved_searches.css":1,"sites\/all\/modules\/highwire\/highwire\/highwire_user\/highwire_user.css":1,"modules\/node\/node.css":1,"modules\/search\/search.css":1,"modules\/user\/user.css":1,"modules\/forum\/forum.css":1,"sites\/all\/modules\/contrib\/views\/css\/views.css":1,"sites\/all\/modules\/contrib\/ckeditor\/css\/ckeditor.css":1,"sites\/all\/modules\/contrib\/colorbox\/styles\/default\/colorbox_style.css":1,"sites\/all\/modules\/contrib\/ctools\/css\/ctools.css":1,"sites\/all\/modules\/highwire\/highwire\/highwire.style.highwire.css":1,"sites\/all\/modules\/highwire\/highwire\/css\/nlm-elements.css":1,"sites\/all\/modules\/contrib\/panels\/css\/panels.css":1,"sites\/default\/modules\/biorxiv\/css\/biorxiv.css":1,"public:\/\/ctools\/css\/263bfdb18936424f0ade19d6bd885f37.css":1,"\/\/cdn.jsdelivr.net\/qtip2\/2.2.1\/jquery.qtip.min.css":1,"sites\/all\/modules\/contrib\/panels_ajax_tab\/css\/panels_ajax_tab.css":1,"sites\/all\/modules\/highwire\/highwire\/highwire.style.markup.css":1,"sites\/all\/modules\/highwire\/highwire\/plugins\/layouts\/highwire_2col_stacked\/highwire-2col-stacked.css":1,"sites\/all\/modules\/contrib\/forward\/forward.css":1,"sites\/all\/modules\/highwire\/highwire\/plugins\/content_types\/css\/highwire_share_link.css":1,"sites\/all\/modules\/highwire\/highwire\/plugins\/content_types\/css\/highwire_copy_permalink.css":1,"sites\/all\/modules\/contrib\/panels\/plugins\/layouts\/flexible\/flexible.css":1,"sites\/all\/modules\/highwire\/highwire\/highwire_collection\/css\/highwire_subject_collection.css":1,"sites\/all\/modules\/highwire\/highwire\/css\/highwire.highwire-list-expand.css":1,"public:\/\/ctools\/css\/bab59090150caf0c64e27afa1b36eef1.css":1,"sites\/all\/modules\/contrib\/panels\/plugins\/layouts\/onecol\/onecol.css":1,"sites\/all\/modules\/contrib\/eu_cookie_compliance\/css\/eu_cookie_compliance.css":1,"sites\/all\/modules\/highwire\/highwire\/highwire_theme_tools\/css\/font-hwicons.css":1,"sites\/all\/modules\/highwire\/highwire\/highwire_theme_tools\/css\/font-hwicons-glyphs.css":1,"0":1,"sites\/all\/themes\/highwire\/jcore_1\/eu_cookie_compliance.css":1,"sites\/all\/themes\/highwire\/jcore_1\/css\/font-awesome-ie7.css":1,"sites\/all\/themes\/contrib\/omega\/alpha\/css\/alpha-reset.css":1,"sites\/all\/themes\/contrib\/omega\/alpha\/css\/alpha-mobile.css":1,"sites\/all\/themes\/contrib\/omega\/alpha\/css\/alpha-alpha.css":1,"sites\/all\/themes\/contrib\/omega\/omega\/css\/formalize.css":1,"sites\/all\/themes\/contrib\/omega\/omega\/css\/omega-text.css":1,"sites\/all\/themes\/contrib\/omega\/omega\/css\/omega-branding.css":1,"sites\/all\/themes\/contrib\/omega\/omega\/css\/omega-menu.css":1,"sites\/all\/themes\/contrib\/omega\/omega\/css\/omega-forms.css":1,"sites\/all\/modules\/highwire\/highwire\/css\/highwire-forms.css":1,"sites\/all\/themes\/highwire\/jcore_1\/css\/mobile-reset.css":1,"sites\/all\/themes\/highwire\/jcore_1\/css\/font-awesome.css":1,"sites\/all\/themes\/highwire\/jcore_1\/css\/forms.css":1,"sites\/all\/themes\/highwire\/jcore_1\/css\/global.css":1,"sites\/all\/themes\/highwire\/jcore_1\/css\/jquery-ui-elements.css":1,"sites\/all\/themes\/highwire\/jcore_1\/css\/text.css":1,"sites\/all\/themes\/highwire\/jcore_1\/css\/colors.css.less":1,"ie::normal::sites\/all\/themes\/highwire\/jcore_1\/css\/jcore-1-jcore-default.css":1,"ie::normal::sites\/all\/themes\/highwire\/jcore_1\/css\/jcore-1-jcore-default-normal.css":1,"ie::normal::sites\/all\/themes\/highwire\/jcore_1\/css\/grid\/jcore_default\/normal\/jcore-default-normal-30.css":1,"narrow::sites\/all\/themes\/highwire\/jcore_1\/css\/jcore-1-jcore-default.css":1,"narrow::sites\/all\/themes\/highwire\/jcore_1\/css\/jcore-1-jcore-default-narrow.css":1,"sites\/all\/themes\/highwire\/jcore_1\/css\/grid\/jcore_default\/narrow\/jcore-default-narrow-30.css":1,"normal::sites\/all\/themes\/highwire\/jcore_1\/css\/jcore-1-jcore-default.css":1,"normal::sites\/all\/themes\/highwire\/jcore_1\/css\/jcore-1-jcore-default-normal.css":1,"sites\/all\/themes\/highwire\/jcore_1\/css\/grid\/jcore_default\/normal\/jcore-default-normal-30.css":1,"wide::sites\/all\/themes\/highwire\/jcore_1\/css\/jcore-1-jcore-default.css":1,"wide::sites\/all\/themes\/highwire\/jcore_1\/css\/jcore-1-jcore-default-wide.css":1,"sites\/all\/themes\/highwire\/jcore_1\/css\/grid\/jcore_default\/wide\/jcore-default-wide-30.css":1,"sites\/default\/modules\/jnl_biorxiv_styles\/css\/biorxiv-styles.css.less":1,"https:\/\/d33xdlntwy0kbs.cloudfront.net\/cshl_custom.css":1},"js":{"sites\/all\/libraries\/enquire.js\/enquire.min.js":1,"sites\/all\/modules\/highwire\/highwire\/plugins\/content_types\/js\/highwire_panels_ajax_tab.js":1,"sites\/all\/modules\/highwire\/highwire\/plugins\/highwire_markup_process\/js\/highwire_openurl.js":1,"sites\/all\/libraries\/lazysizes\/lazysizes.min.js":1,"sites\/all\/modules\/highwire\/highwire\/plugins\/highwire_markup_process\/js\/highwire_figures.js":1,"sites\/all\/modules\/highwire\/highwire\/plugins\/highwire_markup_process\/js\/highwire_google_scholar_sprinkle.js":1,"http:\/\/d33xdlntwy0kbs.cloudfront.net\/cshl_custom.js":1,"sites\/all\/modules\/contrib\/eu_cookie_compliance\/js\/eu_cookie_compliance.js":1,"sites\/default\/modules\/jnl_biorxiv_styles\/js\/adv-search-collection-toggle.js":1,"sites\/all\/libraries\/modernizr\/modernizr.min.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/jquery\/1.8\/jquery.min.js":1,"misc\/jquery-extend-3.4.0.js":1,"misc\/jquery.once.js":1,"misc\/drupal.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.core.min.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.widget.min.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.button.min.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.mouse.min.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.draggable.min.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.position.min.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.resizable.min.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.dialog.min.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.tooltip.min.js":1,"sites\/all\/modules\/contrib\/eu_cookie_compliance\/js\/jquery.cookie-1.4.1.min.js":1,"sites\/all\/modules\/highwire\/highwire\/js\/highwire.equal-heights.js":1,"sites\/all\/modules\/highwire\/highwire\/js\/highwire.panels-ajax-tab.js":1,"sites\/default\/modules\/jnl_biorxiv_styles\/js\/biorxiv-scripts.js":1,"sites\/all\/libraries\/colorbox\/jquery.colorbox-min.js":1,"sites\/all\/modules\/contrib\/colorbox\/js\/colorbox.js":1,"sites\/all\/modules\/contrib\/colorbox\/styles\/default\/colorbox_style.js":1,"https:\/\/scholar.google.com\/scholar_js\/casa.js":1,"sites\/all\/modules\/highwire\/highwire\/highwire_log\/highwire_log.js":1,"sites\/all\/modules\/highwire\/highwire\/highwire_nps_survey\/js\/nps_survey.js":1,"sites\/all\/modules\/highwire\/highwire\/highwire_user\/js\/highwire_user_meta.js":1,"sites\/all\/modules\/contrib\/panels\/js\/panels.js":1,"\/\/cdn.jsdelivr.net\/qtip2\/2.2.1\/jquery.qtip.min.js":1,"sites\/all\/modules\/highwire\/highwire\/js\/highwire.article-citation-author-tooltip.js":1,"sites\/all\/modules\/contrib\/panels_ajax_tab\/js\/panels_ajax_tab.js":1,"sites\/all\/modules\/highwire\/highwire\/plugins\/highwire_markup_process\/js\/highwire_article_reference_popup.js":1,"sites\/all\/modules\/highwire\/highwire\/plugins\/highwire_markup_process\/js\/highwire_at_symbol.js":1,"sites\/default\/modules\/contrib\/disqus\/disqus.js":1,"misc\/textarea.js":1,"sites\/all\/modules\/highwire\/highwire\/plugins\/content_types\/js\/highwire_share_dialog.js":1,"sites\/all\/modules\/highwire\/highwire\/plugins\/content_types\/js\/clipboard.min.js":1,"sites\/all\/modules\/highwire\/highwire\/plugins\/content_types\/js\/highwire_clipboard.js":1,"sites\/all\/modules\/highwire\/highwire\/highwire_citation\/plugins\/content_types\/js\/highwire_citation_export.js":1,"sites\/all\/modules\/highwire\/highwire\/plugins\/content_types\/js\/minipanel_dialog_link.js":1,"sites\/all\/modules\/contrib\/service_links\/js\/twitter_button.js":1,"sites\/all\/modules\/contrib\/service_links\/js\/facebook_like.js":1,"sites\/all\/modules\/contrib\/service_links\/js\/google_plus_one.js":1,"sites\/all\/modules\/highwire\/highwire\/js\/highwire.highwire-list-expand.js":1,"sites\/all\/modules\/contrib\/google_analytics\/googleanalytics.js":1,"sites\/all\/themes\/highwire\/jcore_1\/js\/theme-scripts.js":1,"sites\/all\/themes\/contrib\/omega\/omega\/js\/jquery.formalize.js":1,"sites\/all\/themes\/contrib\/omega\/omega\/js\/omega-mediaqueries.js":1,"sites\/all\/themes\/contrib\/omega\/omega\/js\/omega-equalheights.js":1,"sites\/all\/modules\/highwire\/highwire\/highwire_responsive\/js\/highwire-mediaqueries.js":1}},"colorbox":{"opacity":"0.85","current":"{current} of {total}","previous":"\u00ab Prev","next":"Next \u00bb","close":"Close","maxWidth":"98%","maxHeight":"98%","fixed":true,"mobiledetect":true,"mobiledevicewidth":"480px"},"highwire":{"nid":"1137937","apath":"\/biorxiv\/early\/2020\/02\/06\/598086.atom","pisa":"biorxiv;598086v3","processed":["highwire_math"],"markup":[{"requested":"full-text","variant":"full-text","view":"full","pisa":"biorxiv;598086v3"}],"modal_window_width":"560","share_modal_width":"560","share_modal_title":"Share this Article"},"jcarousel":{"ajaxPath":"\/jcarousel\/ajax\/views"},"instances":"{\u0022highwire_abstract_tooltip\u0022:{\u0022content\u0022:{\u0022text\u0022:\u0022\u0022},\u0022style\u0022:{\u0022tip\u0022:{\u0022width\u0022:20,\u0022height\u0022:20,\u0022border\u0022:1,\u0022offset\u0022:0,\u0022corner\u0022:true},\u0022classes\u0022:\u0022qtip-custom hw-tooltip hw-abstract-tooltip qtip-shadow qtip-rounded\u0022,\u0022classes_custom\u0022:\u0022hw-tooltip hw-abstract-tooltip\u0022},\u0022position\u0022:{\u0022at\u0022:\u0022right center\u0022,\u0022my\u0022:\u0022left center\u0022,\u0022viewport\u0022:true,\u0022adjust\u0022:{\u0022method\u0022:\u0022shift\u0022}},\u0022show\u0022:{\u0022event\u0022:\u0022mouseenter click \u0022,\u0022solo\u0022:true},\u0022hide\u0022:{\u0022event\u0022:\u0022mouseleave \u0022,\u0022fixed\u0022:1,\u0022delay\u0022:\u0022100\u0022}},\u0022highwire_author_tooltip\u0022:{\u0022content\u0022:{\u0022text\u0022:\u0022\u0022},\u0022style\u0022:{\u0022tip\u0022:{\u0022width\u0022:15,\u0022height\u0022:15,\u0022border\u0022:1,\u0022offset\u0022:0,\u0022corner\u0022:true},\u0022classes\u0022:\u0022qtip-custom hw-tooltip hw-author-tooltip qtip-shadow qtip-rounded\u0022,\u0022classes_custom\u0022:\u0022hw-tooltip hw-author-tooltip\u0022},\u0022position\u0022:{\u0022at\u0022:\u0022top center\u0022,\u0022my\u0022:\u0022bottom center\u0022,\u0022viewport\u0022:true,\u0022adjust\u0022:{\u0022method\u0022:\u0022\u0022}},\u0022show\u0022:{\u0022event\u0022:\u0022mouseenter \u0022,\u0022solo\u0022:true},\u0022hide\u0022:{\u0022event\u0022:\u0022mouseleave \u0022,\u0022fixed\u0022:1,\u0022delay\u0022:\u0022100\u0022}},\u0022highwire_reflinks_tooltip\u0022:{\u0022content\u0022:{\u0022text\u0022:\u0022\u0022},\u0022style\u0022:{\u0022tip\u0022:{\u0022width\u0022:15,\u0022height\u0022:15,\u0022border\u0022:1,\u0022mimic\u0022:\u0022top center\u0022,\u0022offset\u0022:0,\u0022corner\u0022:true},\u0022classes\u0022:\u0022qtip-custom hw-tooltip hw-ref-link-tooltip qtip-shadow qtip-rounded\u0022,\u0022classes_custom\u0022:\u0022hw-tooltip hw-ref-link-tooltip\u0022},\u0022position\u0022:{\u0022at\u0022:\u0022bottom left\u0022,\u0022my\u0022:\u0022top left\u0022,\u0022viewport\u0022:true,\u0022adjust\u0022:{\u0022method\u0022:\u0022flip\u0022}},\u0022show\u0022:{\u0022event\u0022:\u0022mouseenter \u0022,\u0022solo\u0022:true},\u0022hide\u0022:{\u0022event\u0022:\u0022mouseleave \u0022,\u0022fixed\u0022:1,\u0022delay\u0022:\u0022100\u0022}}}","qtipDebug":"{\u0022leaveElement\u0022:0}","panel_ajax_tab":{"path":"sites\/all\/modules\/contrib\/panels_ajax_tab"},"disqus":{"domain":"biorxivstage","url":"https:\/\/www.biorxiv.org\/content\/10.1101\/598086v3","title":"Training and inferring neural network function with multi-agent reinforcement learning","identifier":"node\/1137937"},"panels_ajax_pane":{"new-41":"{\u0022encrypted\u0022:\u0022{\\\u0022encrypted\\\u0022:\\\u0022hOs\\\\\\\/ZcR0OJtN39HBHULo8jGRHIXxFex4qzCHQbvjH+uECn85v80sfeTUsCgF\\\\\\\/AZf0kMUN\\\\\\\/kJ3aXbAFHyogLYVmyTD4Vb7ysXQ6kfYLaJ4PSYo9y7oIUCcpA3lEfonF5CAEIdQRC6Fc2wKSi4Ayz+1XoXIVkjoYCBw4euQvHYypz3dzubJ+PZ+lPAuoNLJp0ZJO\\\\\\\/g7S7DD2aX9xEODDRDgkmA3ad+FFRx7mMoNGO+1\\\\\\\/AqGuSTiO40+KyynmdTRRkV5U0+zHiElDcVnW8DgUIcBwKb7v09WD3tGVKrq11Yu4aQEjALYo6NVO5HnLU4WshSUVLKZ7zzbKwYrkF4CTKvf7yyEKIhslhinqu1FxoIy9QRPf\\\\\\\/ZcCJHAR2OShnGumw18yM+FlyflZpPRvwWFU9EHgdxZrvt\\\\\\\/\\\\\\\/MLawzD\\\\\\\/ODO5iG39ivQAxXG+hJxcJn6lwUrAkHhNkico6B3hTFoXCukNvvvi7zgD1r0KxchqP5GKyI6l\\\\\\\/QEaWFLdS9aqemZj0GXInstRgrGpSKiMQEdL\\\\\\\/h3soGWAx\\\\\\\/QWYcR3ZpoYjpua+umm9JofLeAEq6tx6sjvjdcXICkDhQ11LfjFgUzt3dV6o9ppXcY47ZKMHNKwrKGU72SpgwAVrHhcEmBW7fSAKLeOw4e7cd5RLtKeKhCk1LaOWundzlFQmEjCloxiJqthw5YX2bCKLgAe8wxHa73CfQgg7A+0S6nKAYxMrvcbibWdxGfl+9uCQDL7ylQ0TwL3x0Glko1ca2YpnggZKFJ5AY8o8PBGWmAvwzZRZqIKbXnCKbzHS\\\\\\\/oAO\\\\\\\/F9Di3IvNC11Qgxl\\\\\\\/CjC+qrFiGyJF4MVbgXmpCgjrnnrMWdRMah5Ne\\\\\\\/9HrfjhdW\\\\\\\/Eu7qMPVfSKiFmFYEgQvpdkzU4tASnL5pcY\\\\\\\/3D5JxCH4Nu9pAG0hEL8xcwO2DYKBFKo3MOJyiGV3GJOX23RIrUq0VIpEz9\\\\\\\/ChTym1IsaXgDmG1B2m2AKmb7snAlvNkNayxTtkZrXf7tGbcX6L4ZftgPJz6k6gW1fe+\\\\\\\/T1NGaETp+7tL9qSl0Psv9mCjNTq3TzcY1Mo352eUNjcW5e9LUjF2\\\\\\\/F1Ytqaf5u8AmebR+uMspFW2bt9i4Rkqp4NFeJQQq8BMm+3FTy1K3w3OVH3\\\\\\\/5jnGQnky3ux2WUmUQrthU0TsmLiwyOal\\\\\\\/k4hSHWxv+o2hkWrhtVRR9\\\\\\\/n2F7fsEJ3FOq5P32IzbRaMU14+10ECUjosq7exwBStGz24tBTqpV5KQzbImNsCbIfq1Rz1vNHgtXOiDieKC\\\\\\\/kUoRVW\\\\\\\/rnsFtpb4ZgOKdaqC0SJQDNTt\\\\\\\/bAWBgf1qFybMf+S3NoG2wSM8se9BKUsQXI\\\\\\\/rD\\\u0022,\\\u0022iv\\\u0022:\\\u0022+oyC2K5FUXV+1tfzS6IuXA==\\\u0022,\\\u0022salt\\\u0022:\\\u0022f9d8845712f72c9fee5db7d9ff111eab\\\u0022}\u0022,\u0022hmac\u0022:\u0022ae7dc770806ff825a08f2ef14ae7bf71ff341116063c725348424eb586f98485\u0022}"},"urlIsAjaxTrusted":{"\/content\/10.1101\/598086v3.full":true},"ws_fl":{"width":100,"height":21},"ws_gpo":{"size":"","annotation":"","lang":"","callback":"","width":300},"color":{"logo":"https:\/\/www.biorxiv.org\/sites\/default\/files\/bioRxiv_article.jpg"},"highwire_list_expand":{"is_collapsed":"1"},"highwireResponsive":{"enquire_enabled":1,"breakpoints_configured":1,"breakpoints":{"zero":"all and (min-width: 0px)","xsmall":"all and (min-width: 380px)","narrow":"all and (min-width: 768px) and (min-device-width: 768px), (max-device-width: 800px) and (min-width: 768px) and (orientation:landscape)","normal":"all and (min-width: 980px) and (min-device-width: 980px), all and (max-device-width: 1024px) and (min-width: 1024px) and (orientation:landscape)","wide":"all and (min-width: 1220px)"}},"eu_cookie_compliance":{"popup_enabled":1,"popup_agreed_enabled":0,"popup_hide_agreed":0,"popup_clicking_confirmation":1,"popup_scrolling_confirmation":false,"popup_html_info":"\u003Cdiv\u003E\n  \u003Cdiv class =\u0022popup-content info\u0022\u003E\n    \u003Cdiv id=\u0022popup-text\u0022\u003E\n      \u003Cp\u003EWe use cookies on this site to enhance your user experience. By clicking any link on this page you are giving your consent for us to set cookies.\u003C\/p\u003E\n    \u003C\/div\u003E\n    \u003Cdiv id=\u0022popup-buttons\u0022\u003E\n      \u003Cbutton type=\u0022button\u0022 class=\u0022agree-button eu-cookie-compliance-default-button\u0022\u003EContinue\u003C\/button\u003E\n                    \u003Cbutton type=\u0022button\u0022 class=\u0022find-more-button eu-cookie-compliance-more-button\u0022\u003EFind out more\u003C\/button\u003E\n          \u003C\/div\u003E\n  \u003C\/div\u003E\n\u003C\/div\u003E","use_mobile_message":false,"mobile_popup_html_info":"\u003Cdiv\u003E\n  \u003Cdiv class =\u0022popup-content info\u0022\u003E\n    \u003Cdiv id=\u0022popup-text\u0022\u003E\n          \u003C\/div\u003E\n    \u003Cdiv id=\u0022popup-buttons\u0022\u003E\n      \u003Cbutton type=\u0022button\u0022 class=\u0022agree-button eu-cookie-compliance-default-button\u0022\u003EContinue\u003C\/button\u003E\n                    \u003Cbutton type=\u0022button\u0022 class=\u0022find-more-button eu-cookie-compliance-more-button\u0022\u003EFind out more\u003C\/button\u003E\n          \u003C\/div\u003E\n  \u003C\/div\u003E\n\u003C\/div\u003E\n","mobile_breakpoint":"768","popup_html_agreed":"\u003Cdiv\u003E\n  \u003Cdiv class=\u0022popup-content agreed\u0022\u003E\n    \u003Cdiv id=\u0022popup-text\u0022\u003E\n      \u003Ch2\u003EThank you for accepting cookies\u003C\/h2\u003E\u003Cp\u003EYou can now hide this message or find out more about cookies.\u003C\/p\u003E    \u003C\/div\u003E\n    \u003Cdiv id=\u0022popup-buttons\u0022\u003E\n      \u003Cbutton type=\u0022button\u0022 class=\u0022hide-popup-button eu-cookie-compliance-hide-button\u0022\u003EHide\u003C\/button\u003E\n              \u003Cbutton type=\u0022button\u0022 class=\u0022find-more-button eu-cookie-compliance-more-button-thank-you\u0022 \u003EMore info\u003C\/button\u003E\n          \u003C\/div\u003E\n  \u003C\/div\u003E\n\u003C\/div\u003E","popup_use_bare_css":false,"popup_height":"auto","popup_width":"100%","popup_delay":1000,"popup_link":"\/help\/cookie-policy","popup_link_new_window":1,"popup_position":null,"popup_language":"en","store_consent":false,"better_support_for_screen_readers":0,"reload_page":0,"domain":"","popup_eu_only_js":0,"cookie_lifetime":"365","cookie_session":false,"disagree_do_not_show_popup":0,"method":"default","whitelisted_cookies":"","withdraw_markup":"\u003Cbutton type=\u0022button\u0022 class=\u0022eu-cookie-withdraw-tab\u0022\u003EPrivacy settings\u003C\/button\u003E\n\u003Cdiv class=\u0022eu-cookie-withdraw-banner\u0022\u003E\n  \u003Cdiv class=\u0022popup-content info\u0022\u003E\n    \u003Cdiv id=\u0022popup-text\u0022\u003E\n      \u003Cp\u003E\u0026lt;h2\u0026gt;We use cookies on this site to enhance your user experience\u0026lt;\/h2\u0026gt;\u0026lt;p\u0026gt;You have given your consent for us to set cookies.\u0026lt;\/p\u0026gt;\u003C\/p\u003E\n    \u003C\/div\u003E\n    \u003Cdiv id=\u0022popup-buttons\u0022\u003E\n      \u003Cbutton type=\u0022button\u0022 class=\u0022eu-cookie-withdraw-button\u0022\u003EWithdraw consent\u003C\/button\u003E\n    \u003C\/div\u003E\n  \u003C\/div\u003E\n\u003C\/div\u003E\n","withdraw_enabled":false},"googleanalytics":{"trackOutbound":1,"trackMailto":1,"trackDownload":1,"trackDownloadExtensions":"7z|aac|arc|arj|asf|asx|avi|bin|csv|doc(x|m)?|dot(x|m)?|exe|flv|gif|gz|gzip|hqx|jar|jpe?g|js|mp(2|3|4|e?g)|mov(ie)?|msi|msp|pdf|phps|png|ppt(x|m)?|pot(x|m)?|pps(x|m)?|ppam|sld(x|m)?|thmx|qtm?|ra(m|r)?|sea|sit|tar|tgz|torrent|txt|wav|wma|wmv|wpd|xls(x|m|b)?|xlt(x|m)|xlam|xml|z|zip","trackColorbox":1,"trackUrlFragments":1},"jnl_biorxiv_styles":{"defaultJCode":"biorxiv"},"omega":{"layouts":{"primary":"normal","order":["narrow","normal","wide"],"queries":{"narrow":"all and (min-width: 768px) and (min-device-width: 768px), (max-device-width: 800px) and (min-width: 768px) and (orientation:landscape)","normal":"all and (min-width: 980px) and (min-device-width: 980px), all and (max-device-width: 1024px) and (min-width: 1024px) and (orientation:landscape)","wide":"all and (min-width: 1220px)"}}}});
//--><!]]>
</script>
    <!--[if lt IE 9]><script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  </head>
  <body class="html not-front not-logged-in page-node page-node- page-node-1137937 node-type-highwire-article context-content hw-default-jcode-biorxiv hw-article-type-article hw-article-category-new-results">
<!-- Google Tag Manager -->
<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-M677548" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script type="text/javascript">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0];var j=d.createElement(s);var dl=l!='dataLayer'?'&l='+l:'';j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;j.type='text/javascript';j.async=true;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-M677548');</script>
<!-- End Google Tag Manager -->
    <div id="skip-link">
      <a href="#main-content" class="element-invisible element-focusable">Skip to main content</a>
    </div>
        <div class="page clearfix page-box-shadows footer-borders panels-page panels-layout-jcore_2col" id="page">
      <header id="section-header" class="section section-header">
    
  <div id="zone-branding" class="zone zone-branding clearfix print-display-block container-30">
    <div class="grid-15 prefix-1 region region-branding print-display-block" id="region-branding">
  <div class="region-inner region-branding-inner">
        <div class="branding-data clearfix">
            <div class="logo-img">
        <a href="/" rel="home" class="" data-icon-position="" data-hide-link-title="0"><img alt="bioRxiv" src="https://www.biorxiv.org/sites/default/files/bioRxiv_article.jpg" /></a>      </div>
                </div>
          </div>
</div><div class="grid-11 suffix-1 region region-branding-second print-hidden" id="region-branding-second">
  <div class="region-inner region-branding-second-inner">
    <div class="block block-system block-menu block-main-menu block-system-main-menu odd block-without-title" id="block-system-main-menu">
  <div class="block-inner clearfix">
                
    <div class="content clearfix">
      <nav class="menubar-nav"><ul class="menu" role="menu"><li class="first leaf" role="menuitem"><a href="/" title="" class="" data-icon-position="" data-hide-link-title="0">Home</a></li>
<li class="leaf" role="menuitem"><a href="/about-biorxiv" class="" data-icon-position="" data-hide-link-title="0">About</a></li>
<li class="leaf" role="menuitem"><a href="/submit-a-manuscript" class="" data-icon-position="" data-hide-link-title="0">Submit</a></li>
<li class="last leaf" role="menuitem"><a href="/content/alertsrss" title="" class="" data-icon-position="" data-hide-link-title="0">ALERTS / RSS</a></li>
</ul></nav>    </div>
  </div>
</div><div class="block block-panels-mini block-biorxiv-search-box block-panels-mini-biorxiv-search-box even block-without-title" id="block-panels-mini-biorxiv-search-box">
  <div class="block-inner clearfix">
                
    <div class="content clearfix">
      <div class="panel-display panel-1col clearfix" id="mini-panel-biorxiv_search_box">
  <div class="panel-panel panel-col">
    <div><div class="panel-pane pane-highwire-seach-quicksearch" >
  
      
  
  <div class="pane-content">
    <form class="highwire-quicksearch button-style-mini button-style-mini" action="/content/10.1101/598086v3.full" method="post" id="highwire-search-quicksearch-form-0" accept-charset="UTF-8"><div><div class="form-item form-item-label-invisible form-type-textfield form-item-keywords">
  <label class="element-invisible" for="edit-keywords">Search for this keyword </label>
 <input placeholder="Search" type="text" id="edit-keywords" name="keywords" value="" size="60" maxlength="128" class="form-text" />
</div>
<div class="button-wrapper button-mini"><i class="icon-search"></i><input data-icon-only="1" data-font-icon="icon-search" data-icon-position="after" type="submit" id="edit-submit--2" name="op" value="Search" class="form-submit" /></div><input type="hidden" name="form_build_id" value="form-evBjVHwaTZ1zP1F1WsZy7NSrjtD2AXsuIDC4wyA7BbI" />
<input type="hidden" name="form_id" value="highwire_search_quicksearch_form_0" />
</div></form>  </div>

  
  </div>
<div class="panel-separator"></div><div class="panel-pane pane-custom pane-2 advanced-search-link" >
  
      
  
  <div class="pane-content">
    <a href="/search">Advanced Search</a>  </div>

  
  </div>
</div>
  </div>
</div>
    </div>
  </div>
</div>  </div>
</div>  </div>
  
  <div id="zone-header" class="zone zone-header clearfix container-30">
  	      </div>
</header>    
      <section id="section-content" class="section section-content">
    
  <div id="zone-content" class="zone zone-content clearfix container-30">    
        
    <div class="grid-28 suffix-1 prefix-1 region region-content" id="region-content">
  <div class="region-inner region-content-inner">
    <a id="main-content"></a>
                        <div class="block block-system block-main block-system-main odd block-without-title" id="block-system-main">
  <div class="block-inner clearfix">
                
    <div class="content clearfix">
      <div class="panel-display panels-960-layout jcore-2col-layout" >
	  
  <div class="panel-row-wrapper clearfix">
		
		<div class="main-content-wrapper grid-17 suffix-1 alpha">
			<div class="panel-panel panel-region-content">
			  <div class="inside"><div class="panel-pane pane-highwire-article-citation" >
  
      
  
  <div class="pane-content">
    <div class="highwire-article-citation highwire-citation-type-highwire-article" data-node-nid="1137937" id="node1137937" data-pisa="biorxiv;598086v3" data-pisa-master="biorxiv;598086" data-apath="/biorxiv/early/2020/02/06/598086.atom" data-hw-author-tooltip-instance="highwire_author_tooltip"><div  class="highwire-cite highwire-cite-highwire-article highwire-citation-biorxiv-article-top clearfix has-author-tooltip" >

      <span class="biorxiv-article-type">
        New Results    </span>
  
        <h1 class="highwire-cite-title" id="page-title">Training and inferring neural network function with multi-agent reinforcement learning</h1>  
      <div  class="highwire-cite-authors" ><span  class="highwire-citation-authors"><span class="highwire-citation-author first hw-author-orcid-logo-wrapper" data-delta="0"><a href="http://orcid.org/0000-0001-7782-4436" target="_blank" class="hw-author-orcid-logo link-icon-only link-icon"><i class="hw-icon-orcid hw-icon-color-orcid"></i> <span class="title element-invisible">View ORCID Profile</span></a><span class="nlm-given-names">Matthew</span> <span class="nlm-surname">Chalk</span></span>, <span class="highwire-citation-author hw-author-orcid-logo-wrapper" data-delta="1"><a href="http://orcid.org/0000-0002-6699-1455" target="_blank" class="hw-author-orcid-logo link-icon-only link-icon"><i class="hw-icon-orcid hw-icon-color-orcid"></i> <span class="title element-invisible">View ORCID Profile</span></a><span class="nlm-given-names">Gasper</span> <span class="nlm-surname">Tkacik</span></span>, <span class="highwire-citation-author hw-author-orcid-logo-wrapper" data-delta="2"><a href="http://orcid.org/0000-0002-0090-6190" target="_blank" class="hw-author-orcid-logo link-icon-only link-icon"><i class="hw-icon-orcid hw-icon-color-orcid"></i> <span class="title element-invisible">View ORCID Profile</span></a><span class="nlm-given-names">Olivier</span> <span class="nlm-surname">Marre</span></span></span></div>
  
      <div  class="highwire-cite-metadata" ><span  class="highwire-cite-metadata-doi highwire-cite-metadata"><span class="label">doi:</span> https://doi.org/10.1101/598086 </span></div>
  
  
  </div>
<div id="hw-article-author-popups-node1137937" style="display: none;"><div class="author-tooltip-0"><div class="author-tooltip-name">Matthew Chalk </div><div class="author-tooltip-affiliation"><span class="author-tooltip-text"><div class='author-affiliation'><span class='nlm-institution'>Sorbonne Université, INSERM, CNRS, Institut de la Vision</span>, 17 rue Moreau, F-75012 Paris, <span class='nlm-country'>France</span></div></span></div><ul class="author-tooltip-find-more"><li class="author-tooltip-gs-link first"><a href="/lookup/google-scholar?link_type=googlescholar&amp;gs_type=author&amp;author%5B0%5D=Matthew%2BChalk%2B" target="_blank" class="" data-icon-position="" data-hide-link-title="0">Find this author on Google Scholar</a></li><li class="author-tooltip-pubmed-link"><a href="/lookup/external-ref?access_num=Chalk%20M&amp;link_type=AUTHORSEARCH" target="_blank" class="" data-icon-position="" data-hide-link-title="0">Find this author on PubMed</a></li><li class="author-site-search-link"><a href="/search/author1%3AMatthew%2BChalk%2B" rel="nofollow" class="" data-icon-position="" data-hide-link-title="0">Search for this author on this site</a></li><li class="author-orcid-link"><a href="http://orcid.org/0000-0001-7782-4436" target="_blank" class="" data-icon-position="" data-hide-link-title="0">ORCID record for Matthew Chalk</a></li><li class="author-corresp-email-link last"><span>For correspondence: 
<a href="mailto:matthew.chalk@inserm.fr" class="" data-icon-position="" data-hide-link-title="0">matthew.chalk@inserm.fr</a></span></li></ul></div><div class="author-tooltip-1"><div class="author-tooltip-name">Gasper Tkacik </div><div class="author-tooltip-affiliation"><span class="author-tooltip-text"><div class='author-affiliation'><span class='nlm-institution'>IST Austria</span>, A-3400, Klosterneuburg, <span class='nlm-country'>Austria</span></div></span></div><ul class="author-tooltip-find-more"><li class="author-tooltip-gs-link first"><a href="/lookup/google-scholar?link_type=googlescholar&amp;gs_type=author&amp;author%5B0%5D=Gasper%2BTkacik%2B" target="_blank" class="" data-icon-position="" data-hide-link-title="0">Find this author on Google Scholar</a></li><li class="author-tooltip-pubmed-link"><a href="/lookup/external-ref?access_num=Tkacik%20G&amp;link_type=AUTHORSEARCH" target="_blank" class="" data-icon-position="" data-hide-link-title="0">Find this author on PubMed</a></li><li class="author-site-search-link"><a href="/search/author1%3AGasper%2BTkacik%2B" rel="nofollow" class="" data-icon-position="" data-hide-link-title="0">Search for this author on this site</a></li><li class="author-orcid-link last"><a href="http://orcid.org/0000-0002-6699-1455" target="_blank" class="" data-icon-position="" data-hide-link-title="0">ORCID record for Gasper Tkacik</a></li></ul></div><div class="author-tooltip-2"><div class="author-tooltip-name">Olivier Marre </div><div class="author-tooltip-affiliation"><span class="author-tooltip-text"><div class='author-affiliation'><span class='nlm-institution'>Sorbonne Université, INSERM, CNRS, Institut de la Vision</span>, 17 rue Moreau, F-75012 Paris, <span class='nlm-country'>France</span></div></span></div><ul class="author-tooltip-find-more"><li class="author-tooltip-gs-link first"><a href="/lookup/google-scholar?link_type=googlescholar&amp;gs_type=author&amp;author%5B0%5D=Olivier%2BMarre%2B" target="_blank" class="" data-icon-position="" data-hide-link-title="0">Find this author on Google Scholar</a></li><li class="author-tooltip-pubmed-link"><a href="/lookup/external-ref?access_num=Marre%20O&amp;link_type=AUTHORSEARCH" target="_blank" class="" data-icon-position="" data-hide-link-title="0">Find this author on PubMed</a></li><li class="author-site-search-link"><a href="/search/author1%3AOlivier%2BMarre%2B" rel="nofollow" class="" data-icon-position="" data-hide-link-title="0">Search for this author on this site</a></li><li class="author-orcid-link last"><a href="http://orcid.org/0000-0002-0090-6190" target="_blank" class="" data-icon-position="" data-hide-link-title="0">ORCID record for Olivier Marre</a></li></ul></div></div></div>  </div>

  
  </div>
<div class="panel-separator"></div><div class="panel-pane pane-highwire-panel-tabs pane-panels-ajax-tab-tabs" >
  
      
  
  <div class="pane-content">
    <div class="item-list"><ul class="tabs inline panels-ajax-tab"><li class="first"><a href="/content/10.1101/598086v3" class="panels-ajax-tab-tab" data-panel-name="biorxiv_tab_art" data-target-id="highwire_article_tabs" data-entity-context="node:1137937" data-trigger="" data-url-enabled="1">Abstract</a><a href="/panels_ajax_tab/biorxiv_tab_art/node:1137937/1" rel="nofollow" style="display:none" class="js-crawler-link"></a></li><li><a href="/content/10.1101/598086v3.full-text" class="panels-ajax-tab-tab" data-panel-name="article_tab_full_text" data-target-id="highwire_article_tabs" data-entity-context="node:1137937" data-trigger="full-text" data-url-enabled="1">Full Text</a><a href="/panels_ajax_tab/article_tab_full_text/node:1137937/1" rel="nofollow" style="display:none" class="js-crawler-link"></a></li><li><a href="/content/10.1101/598086v3.article-info" class="panels-ajax-tab-tab" data-panel-name="biorxiv_tab_info" data-target-id="highwire_article_tabs" data-entity-context="node:1137937" data-trigger="article-info" data-url-enabled="1">Info/History</a><a href="/panels_ajax_tab/biorxiv_tab_info/node:1137937/1" rel="nofollow" style="display:none" class="js-crawler-link"></a></li><li><a href="/content/10.1101/598086v3.article-metrics" class="panels-ajax-tab-tab" data-panel-name="article_tab_metrics" data-target-id="highwire_article_tabs" data-entity-context="node:1137937" data-trigger="article-metrics" data-url-enabled="1">Metrics</a><a href="/panels_ajax_tab/article_tab_metrics/node:1137937/1" rel="nofollow" style="display:none" class="js-crawler-link"></a></li><li class="last"><a href="/content/10.1101/598086v3.full.pdf+html" class="panels-ajax-tab-tab" data-panel-name="biorxiv_tab_pdf" data-target-id="highwire_article_tabs" data-entity-context="node:1137937" data-trigger="full.pdf+html" data-url-enabled="1"><i class="icon-file-alt"></i> Preview PDF</a><a href="/panels_ajax_tab/biorxiv_tab_pdf/node:1137937/1" rel="nofollow" style="display:none" class="js-crawler-link"></a></li></ul></div>  </div>

  
  </div>
<div class="panel-separator"></div><div class="panel-pane pane-highwire-panel-tabs-container" >
  
      
  
  <div class="pane-content">
    <div data-panels-ajax-tab-preloaded="article_tab_full_text" id="panels-ajax-tab-container-highwire_article_tabs" class="panels-ajax-tab-container"><div class="panels-ajax-tab-loading" style ="display:none"><img class="loading" src="https://www.biorxiv.org/sites/all/modules/contrib/panels_ajax_tab/images/loading.gif" alt="Loading" title="Loading" /></div><div class="panels-ajax-tab-wrap-article_tab_full_text"><div class="panel-display panel-1col clearfix" >
  <div class="panel-panel panel-col">
    <div><div class="panel-pane pane-highwire-markup" >
  
      
  
  <div class="pane-content">
    <div class="highwire-markup"><div xmlns="http://www.w3.org/1999/xhtml" id="content-block-markup" data-highwire-cite-ref-tooltip-instance="highwire_reflinks_tooltip" xmlns:xhtml="http://www.w3.org/1999/xhtml"><div class="article fulltext-view "><span class="highwire-journal-article-marker-start"></span><div class="section abstract" id="abstract-1"><h2 class="">Abstract</h2><p id="p-3">A central goal in systems neuroscience is to understand the functions performed by neural circuits. Previous top-down models addressed this question by comparing the behaviour of an ideal model circuit, optimised to perform a given function, with neural recordings. However, this requires guessing in advance what function is being performed, which may not be possible for many neural systems. To address this, we propose a new framework for optimising a recurrent network using multi-agent reinforcement learning (RL). In this framework, a reward function quantifies how desirable each state of the network is for performing a given function. Each neuron is treated as an ‘agent’, which optimises its responses so as to drive the network towards rewarded states. Three applications follow from this. First, one can use multi-agent RL algorithms to optimise a recurrent neural network to perform diverse functions (e.g. efficient sensory coding or motor control). Second, one could use inverse RL to infer the function of a recorded neural network from data. Third, the theory predicts how neural networks should adapt their dynamics to maintain the same function when the external environment or network structure changes. This could lead to theoretical predictions about how neural network dynamics adapt to deal with cell death and/or varying sensory stimulus statistics.</p></div><div class="section" id="sec-1"><h2 class="">Introduction</h2><p id="p-5">Neural circuits have evolved to perform a range of different functions, from sensory coding to muscle control and decision making. A central goal of systems neuroscience is to elucidate what these functions are and how neural circuits implement them. A common ‘top-down’ approach starts by formulating a hypothesis about the function performed by a given neural system (e.g. efficient coding/decision making), which can be formalised via an objective function [<a id="xref-ref-1-1" class="xref-bibr" href="#ref-1">1</a>–<a id="xref-ref-10-1" class="xref-bibr" href="#ref-10">10</a>]. This hypothesis is then tested by comparing the predicted behaviour of a model circuit that maximises the assumed objective function (possibly given constraints, such as noise/metabolic costs etc.) with recorded responses.</p><p id="p-6">One of the earliest applications of this approach was sensory coding, where neural circuits are thought to efficiently encode sensory stimuli, with limited information loss [<a id="xref-ref-7-1" class="xref-bibr" href="#ref-7">7</a>–<a id="xref-ref-13-1" class="xref-bibr" href="#ref-13">13</a>]. Over the years, top-down models have also been proposed for many central functions performed by neural circuits, such as generating the complex patterns of activity necessary for initiating motor commands [<a id="xref-ref-3-1" class="xref-bibr" href="#ref-3">3</a>], detecting predictive features in the environment [<a id="xref-ref-4-1" class="xref-bibr" href="#ref-4">4</a>], or memory storage [<a id="xref-ref-5-1" class="xref-bibr" href="#ref-5">5</a>]. Nevertheless, it has remained difficult to make quantitative contact between top-down model predictions and data, in particular, to rigorously test which (if any) of the proposed functions is actually being carried out by a real neural circuit.</p><p id="p-7">The first problem is that a pure top-down approach requires us to hypothesise the function performed by a given neural circuit, which is often not possible. Second, even if our hypothesis is correct, there may be multiple ways for a neural circuit to perform the same function, so that the predictions of the top-down model may not match the data.</p><p id="p-8">Here we propose a new framework for considering optimal coding by a recurrent neural network, that aims to overcome these problems. First, we show how optimal coding by a recurrent neural network can be re-cast as a multi-agent reinforcement learning (RL) problem [<a id="xref-ref-14-1" class="xref-bibr" href="#ref-14">14</a>–<a id="xref-ref-18-1" class="xref-bibr" href="#ref-18">18</a>] (<a id="xref-fig-1-1" class="xref-fig" href="#F1">Fig 1</a>). In this framework, a reward function quantifies how desirable each state of the network is for performing a given computation. Each neuron is then treated as a separate ‘agent’, which optimises its responses (i.e. when to fire a spike) so as to drive the network towards rewarded states, given a constraint on the information each neuron encodes about its inputs. This framework is very general – different choices of reward function result in the network performing diverse functions, from efficient coding to decision making and optimal control – and thus has the potential to unify many previous theories of neural coding.</p><div id="F1" class="fig pos-float type-figure  odd"><div class="highwire-figure"><div class="fig-inline-img-wrapper"><div class="fig-inline-img"><a href="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F1.large.jpg?width=800&amp;height=600&amp;carousel=1" title="General approach. (A) Top-down models use an assumed objective function to derive the optimal neural dynamics. The inverse problem is to infer the objective function from observed neural responses. (B) RL uses an assumed reward function to derive an optimal set of actions that an agent should perform in a given environment. Inverse RL infers the reward function from the agent&#x2019;s actions. (C-D) A mapping between the neural network and textbook RL setup. (E) Both problems can be formulated as MDPs, where an agent (or neuron) can choose which actions, a, to perform to alter their state, s, and increase their reward. (F) Given a reward function and coding cost (which penalises complex policies), we can use entropy-regularised RL to derive the optimal policy (left). Here we plot a single trajectory sampled from the optimal policy (red), as well as how often the agent visits each location (shaded). Conversely, we can use inverse RL to infer the reward function from the agent&#x2019;s policy (centre). We can then use the inferred reward to predict how the agent&#x2019;s policy will change when we increase the coding cost to favour simpler (but less rewarded) trajectories (top right), or move the walls of the maze (bottom right)." class="highwire-fragment fragment-images colorbox-load" rel="gallery-fragment-images-5248071" data-figure-caption="&lt;div class=&quot;highwire-markup&quot;&gt;&lt;div xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&gt;&lt;span class=&quot;caption-title&quot;&gt;General approach.&lt;/span&gt; (&lt;strong&gt;A&lt;/strong&gt;) Top-down models use an assumed objective function to derive the optimal neural dynamics. The inverse problem is to infer the objective function from observed neural responses. (&lt;strong&gt;B&lt;/strong&gt;) RL uses an assumed reward function to derive an optimal set of actions that an agent should perform in a given environment. Inverse RL infers the reward function from the agent&#x2019;s actions. (&lt;strong&gt;C-D&lt;/strong&gt;) A mapping between the neural network and textbook RL setup. (&lt;strong&gt;E&lt;/strong&gt;) Both problems can be formulated as MDPs, where an agent (or neuron) can choose which actions, &lt;em&gt;a&lt;/em&gt;, to perform to alter their state, &lt;em&gt;s&lt;/em&gt;, and increase their reward. (&lt;strong&gt;F&lt;/strong&gt;) Given a reward function and coding cost (which penalises complex policies), we can use entropy-regularised RL to derive the optimal policy (left). Here we plot a single trajectory sampled from the optimal policy (red), as well as how often the agent visits each location (shaded). Conversely, we can use inverse RL to infer the reward function from the agent&#x2019;s policy (centre). We can then use the inferred reward to predict how the agent&#x2019;s policy will change when we increase the coding cost to favour simpler (but less rewarded) trajectories (top right), or move the walls of the maze (bottom right).&lt;/div&gt;&lt;/div&gt;" data-icon-position="" data-hide-link-title="0"><span class="hw-responsive-img"><img class="highwire-fragment fragment-image lazyload" alt="Fig 1." src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F1.medium.gif" width="440" height="352"/><noscript><img class="highwire-fragment fragment-image" alt="Fig 1." src="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F1.medium.gif" width="440" height="352"/></noscript></span></a></div></div><ul class="highwire-figure-links inline"><li class="download-fig first"><a href="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F1.large.jpg?download=true" class="highwire-figure-link highwire-figure-link-download" title="Download Fig 1." data-icon-position="" data-hide-link-title="0">Download figure</a></li><li class="new-tab last"><a href="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F1.large.jpg" class="highwire-figure-link highwire-figure-link-newtab" target="_blank" data-icon-position="" data-hide-link-title="0">Open in new tab</a></li></ul></div><div class="fig-caption" xmlns:xhtml="http://www.w3.org/1999/xhtml"><span class="fig-label">Fig 1.</span> <span class="caption-title">General approach.</span><p id="p-9" class="first-child">(<strong>A</strong>) Top-down models use an assumed objective function to derive the optimal neural dynamics. The inverse problem is to infer the objective function from observed neural responses. (<strong>B</strong>) RL uses an assumed reward function to derive an optimal set of actions that an agent should perform in a given environment. Inverse RL infers the reward function from the agent’s actions. (<strong>C-D</strong>) A mapping between the neural network and textbook RL setup. (<strong>E</strong>) Both problems can be formulated as MDPs, where an agent (or neuron) can choose which actions, <em>a</em>, to perform to alter their state, <em>s</em>, and increase their reward. (<strong>F</strong>) Given a reward function and coding cost (which penalises complex policies), we can use entropy-regularised RL to derive the optimal policy (left). Here we plot a single trajectory sampled from the optimal policy (red), as well as how often the agent visits each location (shaded). Conversely, we can use inverse RL to infer the reward function from the agent’s policy (centre). We can then use the inferred reward to predict how the agent’s policy will change when we increase the coding cost to favour simpler (but less rewarded) trajectories (top right), or move the walls of the maze (bottom right).</p><div class="sb-div caption-clear"></div></div></div><p id="p-10">Next, we show how our proposed framework could be used to tackle the inverse problem, of inferring the reward function from the observed network dynamics. Previous work has proposed ‘inverse RL’ algorithms for inferring the original reward function from an agent’s actions [<a id="xref-ref-20-1" class="xref-bibr" href="#ref-20">20</a>–<a id="xref-ref-24-1" class="xref-bibr" href="#ref-24">24</a>]. Here we show how this framework can be adapted to infer the reward function optimised by a recurrent neural network. Further, given certain conditions we show that the reward function can be expressed as a closed-form expression of the observed network dynamics.</p><p id="p-11">We hypothesise that the inferred reward function, rather than e.g. the properties of individual neurons, is the most succinct mathematical summary of the network, that generalises across different contexts and conditions. Thus we could use our framework to quantitatively predict how the network will adapt or learn in order to perform the same function when the external context (e.g. stimulus statistics), constraints (e.g. noise level) or the structure of the network (e.g. due to cell death or experimental manipulation) change. Our framework could thus not only allows RL to be used to train neural networks and use inverse RL to infer their function, but also could generate predictions for a wide range of experimental manipulations.</p></div><div class="section" id="sec-2"><h2 class="">Results</h2><div id="sec-3" class="subsection"><h3>General approach</h3><p id="p-12">We can quantify how well a network performs a specific function (e.g. sensory coding/decision making) via an objective function <em>L</em><sub><em>π</em></sub> (where <em>π</em> denotes the parameters that determine the network dynamics) (<a id="xref-fig-1-2" class="xref-fig" href="#F1">Fig 1A</a>). There is a large literature describing how to optimise the dynamics of a neural network, <em>π</em>, to maximise specific objective functions, <em>L</em><sub><em>π</em></sub>, given constraints (e.g. metabolic cost/wiring constraints etc.) [<a id="xref-ref-1-2" class="xref-bibr" href="#ref-1">1</a>–<a id="xref-ref-10-2" class="xref-bibr" href="#ref-10">10</a>]. However, it is generally much harder to go in the opposite direction, to infer the objective function, <em>L</em><sub><em>π</em></sub>, from observations of the network dynamics.</p><p id="p-13">To address this question, we looked to the field of reinforcement learning (RL) [<a id="xref-ref-14-2" class="xref-bibr" href="#ref-14">14</a>–<a id="xref-ref-18-2" class="xref-bibr" href="#ref-18">18</a>], which describes how an agent should choose actions so as to maximise the reward they receive from their environment (<a id="xref-fig-1-3" class="xref-fig" href="#F1">Fig 1B</a>). Conversely, another paradigm, called inverse RL [<a id="xref-ref-20-2" class="xref-bibr" href="#ref-20">20</a>–<a id="xref-ref-24-2" class="xref-bibr" href="#ref-24">24</a>], explains how to go in the opposite direction, to infer the reward associated with different states of the environment from observations of the agent’s actions. We reasoned that, if we could establish a mapping between optimising neural network dynamics (<a id="xref-fig-1-4" class="xref-fig" href="#F1">Fig 1A</a>) and optimising an agent’s actions via RL (<a id="xref-fig-1-5" class="xref-fig" href="#F1">Fig 1B</a>), then we could use inverse RL to infer the objective function optimised by a neural network from its observed dynamics.</p><p id="p-14">To illustrate this, let us compare the problem faced by a single neuron embedded within a recurrent neural network (<a id="xref-fig-1-6" class="xref-fig" href="#F1">Fig 1C</a>) to the textbook RL problem of an agent navigating a maze (<a id="xref-fig-1-7" class="xref-fig" href="#F1">Fig 1D</a>). The neuron’s environment is determined by the activity of other neurons in the network and its external input; the agent’s environment is determined by the walls of the maze. At each time, the neuron can choose whether to fire a spike, so as to drive the network towards states that are ‘desirable’ for performing a given function; at each time, the agent in the maze can choose which direction to move in, so as to reach ‘desirable’ locations, associated with a high reward.</p><p id="p-15">Both problems can be formulated mathematically as Markov Decision Processes (MDPs) (<a id="xref-fig-1-8" class="xref-fig" href="#F1">Fig 1E</a>). Each state of the system, <em>s</em> (i.e. the agent’s position in the maze, or the state of the network and external input), is associated with a reward, <em>r</em>(<em>s</em>). At each time, the agent can choose to perform an action, <em>a</em> (i.e. moving in a particular direction, or firing a spike), so as to reach a new state <em>s</em>′ with probability, <em>p</em>(<em>s</em>′|<em>a, s</em>). The probability that the agent performs a given action in each state, <em>π</em>(<em>a</em>|<em>s</em>), is called their policy.</p><p id="p-16">We assume that the agent (or neuron) optimises their policy to maximise their average reward, <span class="inline-formula" id="inline-formula-1"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-1.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-1.gif"/></noscript></span></span> (where <span class="inline-formula" id="inline-formula-2"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-2.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-2.gif"/></noscript></span></span> denotes the average over the steady state distribution, <em>p</em><sub><em>π</em></sub> (<em>s</em>), with a policy <em>π</em> (<em>a</em>|<em>s</em>)), given a constraint on the information they can encode about their state, <em>I</em><sub><em>π</em></sub>(<em>a</em>; <em>s</em>) (this corresponds, for example, to constraining how much a neuron can encode about the rest of the network and external input). This can be achieved by maximising the following objective function:
<span class="disp-formula" id="disp-formula-1"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-2.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-2.gif"/></noscript></span>

</span>
where <em>λ</em> is a constant that controls the strength of the constraint. Note that in the special case where the agent’s state does not depend on previous actions (i.e. <em>p</em> (<em>s</em>′|<em>a, s</em>) = <em>p</em> (<em>s</em>′<em>s</em>)) and the reward depends on their current state and action, this is the same as the objective function used in rate-distortion theory [<a id="xref-ref-25-1" class="xref-bibr" href="#ref-25">25</a>, <a id="xref-ref-26-1" class="xref-bibr" href="#ref-26">26</a>]. We can also write the objective function as:
<span class="disp-formula" id="disp-formula-2"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-3.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-3.gif"/></noscript></span>

</span>
where <em>c</em><sub><em>π</em></sub>(<em>s</em>) is a ‘coding cost’, equal to the Kullback-Leibler divergence between the agent’s policy and the steady-state distribution over actions, <em>D</em><sub><em>KL</em></sub> [<em>π</em> (<em>a</em>|<em>s</em>)‖<em>p</em><sub><em>π</em></sub> (<em>a</em>)]. We hereon refer to the difference, <em>r</em>(<em>s</em>) − <em>λc</em><sub><em>π</em></sub>(<em>s</em>), as the ‘return’ associated with each state.</p><p id="p-17">In Methods section we show how this objective function can be maximised via entropy-regularised RL [<a id="xref-ref-15-1" class="xref-bibr" href="#ref-15">15</a>, <a id="xref-ref-18-3" class="xref-bibr" href="#ref-18">18</a>] to obtain the optimal policy, which satisfies the relation:
<span class="disp-formula" id="disp-formula-3"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-4.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-4.gif"/></noscript></span>

</span>
where <em>v</em><sub><em>π</em></sub> (<em>s</em>) is the ‘value’ associated with each state, defined as the total return predicted in the future if the agent starts in a given state, minus the average return, <em>L</em><sub><em>π</em></sub>:
<span class="disp-formula" id="disp-formula-4"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-5.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-5.gif"/></noscript></span>

</span>
where <em>s, s</em>′ and <em>s</em>″ denote three consecutive states of the agent. Subtracting the average return, <em>L</em><sub><em>π</em></sub>, from each term in the sum ensures that this series converges to a finite value [<a id="xref-ref-19-1" class="xref-bibr" href="#ref-19">19</a>]. Thus, actions that drive the agent towards high-value states are preferred over actions that drive the agent towards low value states. Note the difference between a state’s value, <em>v</em> (<em>s</em>), and its return, <em>r</em> (<em>s</em>) − <em>λc</em><sub><em>π</em></sub> (<em>s</em>): a state with low return can nonetheless have a high-value if it allows the agent to transition to other states associated with a high return in the future.</p><p id="p-18">Let us return to our toy example of the agent in a maze. <a id="xref-fig-1-9" class="xref-fig" href="#F1">Figure 1F</a> (left) shows the agent’s trajectory through the maze after optimising their policy using entropy regularized RL to maximise <em>L</em><sub><em>π</em></sub> (Methods, section). In this example, a single location, in the lower-right corner of the maze, has a non-zero reward (<a id="xref-fig-1-10" class="xref-fig" href="#F1">Fig 1F</a>, centre). However, suppose we didn’t know this; could we infer the reward at each location just by observing the agent’s trajectory in the maze? In Methods section we show that this can be done by finding the reward function that maximises the log-likelihood of the optimal policy, averaged over observed actions and states, ⟨log <em>π</em>* (<em>a</em>|<em>s</em>) ⟩<sub>data</sub>. If the coding cost is non-zero (<em>λ</em> &gt; 0), this problem is generally well-posed, meaning there is a unique solution for <em>r</em>(<em>s</em>).</p><p id="p-19">Once we have inferred the reward function optimised by the agent, we can then use it to predict how their behaviour will change when we alter their external environment or internal constraints. For example, we can predict how the agent’s trajectory through the maze will change when we move the position of the walls (<a id="xref-fig-1-11" class="xref-fig" href="#F1">Fig 1F</a>, lower right), or increase the coding cost so as to favour simpler (but less rewarded) trajectories (<a id="xref-fig-1-12" class="xref-fig" href="#F1">Fig 1F</a>, upper right).</p></div><div id="sec-4" class="subsection"><h3>Optimising neural network dynamics</h3><p id="p-20">We used these principles to infer the function performed by a recurrent neural network. We considered a model network of <em>n</em> neurons, each described by a binary variable, <em>σ</em><sub><em>i</em></sub> = −1<em>/</em>1, denoting whether the neuron is silent or spiking respectively (Methods section). The network receives an external input, <strong><em>x</em></strong>. The network state is described by an <em>n</em>-dimensional vector of binary values, <strong><em>σ</em></strong> = (<em>σ</em><sub>1</sub>, <em>σ</em><sub>2</sub>, …, <em>σ</em><sub><em>n</em></sub>)<sup><em>T</em></sup>. Both the network and external input have Markov dynamics. Neurons are updated asynchronously: at each time-step a neuron is selected at random, and its state updated by sampling from <span class="inline-formula" id="inline-formula-3"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-3.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-3.gif"/></noscript></span></span>. The dynamics of the network are fully specified by the set of response probabilities, <span class="inline-formula" id="inline-formula-4"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-4.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-4.gif"/></noscript></span></span>, and input statistics, <em>p</em>(<strong><em>x</em></strong>′ <strong><em>x</em></strong>).</p><p id="p-21">As before, we use a reward function, <em>r</em> (<strong><em>σ, x</em></strong>), to express how desirable each state of the network is to perform a given functional objective. For example, if the objective of the network is to faithfully encode the external input, then an appropriate reward function might be the negative squared error: <span class="inline-formula" id="inline-formula-5"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-5.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-5.gif"/></noscript></span></span>, where <span class="inline-formula" id="inline-formula-6"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-6.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-6.gif"/></noscript></span></span> denotes an estimate of <strong><em>x</em></strong>, inferred from the network state, <strong><em>σ</em></strong>. More generally, different choices of reward function can be used to describe a large range of functions that may be performed by the network.</p><p id="p-22">The dynamics of the network, <em>π</em>, are said to be optimal if they maximise the average reward, <span class="inline-formula" id="inline-formula-7"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-7.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-7.gif"/></noscript></span></span>, given a constraint on the information each neuron encodes about the rest of the network and external inputs, <span class="inline-formula" id="inline-formula-8"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-8.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-8.gif"/></noscript></span></span>. This corresponds to maximising the objective function:
<span class="disp-formula" id="disp-formula-5"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-6.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-6.gif"/></noscript></span>

</span>
where <em>λ</em> controls the strength of the constraint. For each neuron, we can frame this optimisation problem as an MDP, where the state, action, and policy correspond to the network state and external input {<strong><em>σ, x</em></strong>}, the neuron’s proposed update <span class="inline-formula" id="inline-formula-9"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-9.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-9.gif"/></noscript></span></span>, and the response probability <span class="inline-formula" id="inline-formula-10"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-10.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-10.gif"/></noscript></span></span>, respectively. Thus, we can optimise the network dynamics, by treating each neuron as an agent, and optimising its response probability, <span class="inline-formula" id="inline-formula-11"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-11.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-11.gif"/></noscript></span></span>, via entropy-regularised RL, as we did for the agent in the maze. Further, as each update increases the objective function <em>L</em><sub><em>π</em></sub>, we can alternate updates for different neurons to optimise the dynamics of the entire network (as in multi-agent RL). In Methods section, we show that this results in optimal response probabilities that satisfy the relation:
<span class="disp-formula" id="disp-formula-6"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-7.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-7.gif"/></noscript></span>

</span>
where <strong><em>σ</em></strong><sub><em>/i</em></sub> denotes the state of all neurons except for neuron <em>i</em>, and <em>v</em><sub><em>π</em></sub> (<strong><em>σ, x</em></strong>) is the value associated with each state:
<span class="disp-formula" id="disp-formula-7"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-8.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-8.gif"/></noscript></span>

</span>
</p><p id="p-23">The coding cost, <span class="inline-formula" id="inline-formula-12"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-12.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-12.gif"/></noscript></span></span> penalises deviations from each neuron’s average firing rate. The network dynamics are optimised by alternately updating the value function and neural response probabilities until convergence (Methods section).</p><p id="p-24">To see how this works in practice, we simulated a network of 8 neurons that receive a binary input <em>x</em> (<a id="xref-fig-2-1" class="xref-fig" href="#F2">Fig 2A</a>). The assumed goal of the network is to fire exactly 2 spikes when <em>x</em> = −1, and 6 spikes when <em>x</em> = 1, while minimising the coding cost. To achieve this, the reward was set to unity when the network fired the desired number of spikes, and zero otherwise (<a id="xref-fig-2-2" class="xref-fig" href="#F2">Fig 2B</a>). Using entropy-regularised RL, we derived optimal tuning curves for each neuron, which show how their spiking probability should optimally vary depending on the input, <em>x</em>, and number of spikes fired by other neurons (<a id="xref-fig-2-3" class="xref-fig" href="#F2">Fig 2C</a>). We confirmed that after optimisation the number of spikes fired by the network was tightly peaked around the target values (<a id="xref-fig-2-4" class="xref-fig" href="#F2">Fig 2D</a>). Decreasing the coding cost reduced noise in the network, decreasing variability in the total spike count.</p><div id="F2" class="fig pos-float type-figure  odd"><div class="highwire-figure"><div class="fig-inline-img-wrapper"><div class="fig-inline-img"><a href="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F2.large.jpg?width=800&amp;height=600&amp;carousel=1" title="Training and inferring the function performed by a neural network. (A) A recurrent neural network receives a binary input, x. (B) The reward function equals 1 if the network fires 2 spikes when x = &#x2212;1, or 6 spikes when x = 1. (C) After optimisation, neural tuning curves depend on the input, x, and total spike count. (D) Simulated dynamics of the network with 8 neurons (left). The total spike count (below) is tightly peaked around the rewarded values. (E) Using inverse RL on the observed network dynamics, we infer the original reward function used to optimise the network from its observed dynamics. (F) The inferred reward function is used to predict how neural tuning curves will adapt depending on contextual changes, such as varying the input statistics (e.g. decreasing p(x = 1)) (top right), or cell death (bottom right). Thick/thin lines show adapted/original tuning curves, respectively." class="highwire-fragment fragment-images colorbox-load" rel="gallery-fragment-images-5248071" data-figure-caption="&lt;div class=&quot;highwire-markup&quot;&gt;&lt;div xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&gt;&lt;span class=&quot;caption-title&quot;&gt;Training and inferring the function performed by a neural network.&lt;/span&gt; (&lt;strong&gt;A&lt;/strong&gt;) A recurrent neural network receives a binary input, &lt;em&gt;x&lt;/em&gt;. (&lt;strong&gt;B&lt;/strong&gt;) The reward function equals 1 if the network fires 2 spikes when &lt;em&gt;x&lt;/em&gt; = &#x2212;1, or 6 spikes when &lt;em&gt;x&lt;/em&gt; = 1. (&lt;strong&gt;C&lt;/strong&gt;) After optimisation, neural tuning curves depend on the input, &lt;em&gt;x&lt;/em&gt;, and total spike count. (&lt;strong&gt;D&lt;/strong&gt;) Simulated dynamics of the network with 8 neurons (left). The total spike count (below) is tightly peaked around the rewarded values. (&lt;strong&gt;E&lt;/strong&gt;) Using inverse RL on the observed network dynamics, we infer the original reward function used to optimise the network from its observed dynamics. (&lt;strong&gt;F&lt;/strong&gt;) The inferred reward function is used to predict how neural tuning curves will adapt depending on contextual changes, such as varying the input statistics (e.g. decreasing &lt;em&gt;p&lt;/em&gt;(&lt;em&gt;x&lt;/em&gt; = 1)) (top right), or cell death (bottom right). Thick/thin lines show adapted/original tuning curves, respectively.&lt;/div&gt;&lt;/div&gt;" data-icon-position="" data-hide-link-title="0"><span class="hw-responsive-img"><img class="highwire-fragment fragment-image lazyload" alt="Fig 2." src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F2.medium.gif" width="440" height="332"/><noscript><img class="highwire-fragment fragment-image" alt="Fig 2." src="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F2.medium.gif" width="440" height="332"/></noscript></span></a></div></div><ul class="highwire-figure-links inline"><li class="download-fig first"><a href="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F2.large.jpg?download=true" class="highwire-figure-link highwire-figure-link-download" title="Download Fig 2." data-icon-position="" data-hide-link-title="0">Download figure</a></li><li class="new-tab last"><a href="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F2.large.jpg" class="highwire-figure-link highwire-figure-link-newtab" target="_blank" data-icon-position="" data-hide-link-title="0">Open in new tab</a></li></ul></div><div class="fig-caption"><span class="fig-label">Fig 2.</span> <span class="caption-title">Training and inferring the function performed by a neural network.</span><p id="p-25" class="first-child">(<strong>A</strong>) A recurrent neural network receives a binary input, <em>x</em>. (<strong>B</strong>) The reward function equals 1 if the network fires 2 spikes when <em>x</em> = −1, or 6 spikes when <em>x</em> = 1. (<strong>C</strong>) After optimisation, neural tuning curves depend on the input, <em>x</em>, and total spike count. (<strong>D</strong>) Simulated dynamics of the network with 8 neurons (left). The total spike count (below) is tightly peaked around the rewarded values. (<strong>E</strong>) Using inverse RL on the observed network dynamics, we infer the original reward function used to optimise the network from its observed dynamics. (<strong>F</strong>) The inferred reward function is used to predict how neural tuning curves will adapt depending on contextual changes, such as varying the input statistics (e.g. decreasing <em>p</em>(<em>x</em> = 1)) (top right), or cell death (bottom right). Thick/thin lines show adapted/original tuning curves, respectively.</p><div class="sb-div caption-clear"></div></div></div></div><div id="sec-5" class="subsection"><h3>Inferring the objective function from the neural dynamics</h3><p id="p-26">We next asked if we could use inverse RL to infer the reward function optimised by a neural network, just from its observed dynamics (<a id="xref-fig-2-5" class="xref-fig" href="#F2">Fig 2D</a>). For simplicity, let us first consider a recurrent network that receives no external input. In this case, the optimal dynamics (<a id="xref-disp-formula-6-1" class="xref-disp-formula" href="#disp-formula-6">Eqn 6</a>) correspond to Gibbs sampling from a steady-state distribution: <span class="inline-formula" id="inline-formula-13"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-13.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-13.gif"/></noscript></span></span>. We can combine this with the Bellmann equality, which relates the reward, value and cost functions (according to: <em>r</em> (<strong><em>σ</em></strong>) = <em>v</em><sub><em>π</em></sub> (<strong><em>σ</em></strong>) + <em>λc</em><sub><em>π</em></sub> (<strong><em>σ</em></strong>) − ⟨<em>v</em><sub><em>π</em></sub> (<strong><em>σ</em></strong>′)⟩<sub><em>p</em>(<strong><em>σ′</em>|<em>σ</em></strong>)</sub> + <em>L</em><sub><em>π</em></sub>; see Methods) to derive an expression for the reward function:
<span class="disp-formula" id="disp-formula-8"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-10.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-10.gif"/></noscript></span>

</span>
where <em>p</em> (<em>σ</em><sub><em>i</em></sub>|<strong><em>σ</em></strong><sub><em>/i</em></sub>) denotes the probability that neuron <em>i</em> is in state <em>σ</em><sub><em>i</em></sub>, given the current state of all the other neurons and <em>C</em> is an irrelevant constant (see Methods). Without loss of generality, we can set the coding cost, <em>λ</em>, to 1 (since altering <em>λ</em> rescales the inferred reward and coding cost by the same factor, rescaling the objective function without changing its shape). In the Methods, we show how we can recover the reward function when there is an external input. In this case, we do not obtain a closed-form expression for the reward function, but must instead infer it via maximum likelihood.</p><p id="p-27"><a id="xref-fig-2-6" class="xref-fig" href="#F2">Figure 2E</a> shows how we can use inverse RL to infer the reward function optimised by a model network from its observed dynamics, in the presence of an external input. Note, that our method did not make any <em>a priori</em> assumptions about the parametric form of the reward function, which was allowed to vary freely as a function of the network state and input, (<strong><em>σ, x</em></strong>). Nonetheless, we can use a simple clustering algorithm (e.g. k-means) to recover the fact that the inferred reward took two binary values; further analysis reveals that the reward is only non-zero when the network fired exactly 2 spikes when <em>x</em> = −1, and 6 spikes when <em>x</em> = 1. As for the agent in the maze, we can use this inferred reward function to predict how the network dynamics will vary depending on the internal/external constraints. For example, we can predict how neural tuning curves will vary if we alter the input statistics (<a id="xref-fig-2-7" class="xref-fig" href="#F2">Fig 2F</a>, upper), or remove a cell from the network (<a id="xref-fig-2-8" class="xref-fig" href="#F2">Fig 2F</a>, lower).</p><p id="p-28">Our ability to correctly infer the reward function optimised by the network will be fundamentally limited by the amount of available data. <a id="xref-fig-3-1" class="xref-fig" href="#F3">Fig 3A</a> shows how the correlation between the inferred and true reward increases with the amount of data samples used to infer the reward. (Note that each discrete time-step is considered to be one data sample.) As the number of samples is increased, the distribution of inferred rewards becomes more tightly peaked around two values (<a id="xref-fig-3-2" class="xref-fig" href="#F3">Fig 3B</a>), reflecting the fact that the true reward function was binary. Of course, with real neural data we will not have access to the ‘true’ reward function. In this case, we can test how well our inferred reward function is able to predict neural responses in different conditions. <a id="xref-fig-3-3" class="xref-fig" href="#F3">Figure 3C-D</a> shows how the predicted response distribution (when we alter the input statistics, <a id="xref-fig-3-4" class="xref-fig" href="#F3">Fig 3C</a>, or remove cells, <a id="xref-fig-3-5" class="xref-fig" href="#F3">Fig 3D</a>) becomes more accurate as we increase the number of samples used to estimate the reward function.</p><div id="F3" class="fig pos-float type-figure  odd"><div class="highwire-figure"><div class="fig-inline-img-wrapper"><div class="fig-inline-img"><a href="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F3.large.jpg?width=800&amp;height=600&amp;carousel=1" title="Inferring the reward from limited data. (A) The r2-goodness of fit between the true reward, and the reward inferred using a finite number of samples (a sample is defined as an observation of the network state at a single time-point). The solid line indicates the r2 value averaged over 20 different simulations, while the shaded areas indicate the standard error on the mean. (B) Distribution of rewards inferred from a variable numbers of data samples. As the number of data samples is increased, the distribution of inferred rewards becomes more sharply peaked around 0 and 1 (reflecting the fact that the true reward was binary). (C) The KL-divergence between the optimal response distribution with altered input statistics (see Fig 2F, upper) and the response distribution predicted using the reward inferred in the initial condition from a variable number of samples. The solid line indicates the KL-divergence averaged over 20 different simulations, while the shaded areas indicate the standard error on the mean. A horizontal dashed line indicates the KL-divergence between the response distribution with biased input and the original condition (that was used to infer the reward). (D) Same as panel (C), but where instead of altering the input statistics, we remove cells from the network (see Fig 2F, lower)." class="highwire-fragment fragment-images colorbox-load" rel="gallery-fragment-images-5248071" data-figure-caption="&lt;div class=&quot;highwire-markup&quot;&gt;&lt;div xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&gt;&lt;span class=&quot;caption-title&quot;&gt;Inferring the reward from limited data.&lt;/span&gt; (&lt;strong&gt;A&lt;/strong&gt;) The &lt;em&gt;r&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt;-goodness of fit between the true reward, and the reward inferred using a finite number of samples (a sample is defined as an observation of the network state at a single time-point). The solid line indicates the &lt;em&gt;r&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt; value averaged over 20 different simulations, while the shaded areas indicate the standard error on the mean. (&lt;strong&gt;B&lt;/strong&gt;) Distribution of rewards inferred from a variable numbers of data samples. As the number of data samples is increased, the distribution of inferred rewards becomes more sharply peaked around 0 and 1 (reflecting the fact that the true reward was binary). (&lt;strong&gt;C&lt;/strong&gt;) The KL-divergence between the optimal response distribution with altered input statistics (see Fig 2F, upper) and the response distribution predicted using the reward inferred in the initial condition from a variable number of samples. The solid line indicates the KL-divergence averaged over 20 different simulations, while the shaded areas indicate the standard error on the mean. A horizontal dashed line indicates the KL-divergence between the response distribution with biased input and the original condition (that was used to infer the reward). (&lt;strong&gt;D&lt;/strong&gt;) Same as panel (C), but where instead of altering the input statistics, we remove cells from the network (see Fig 2F, lower).&lt;/div&gt;&lt;/div&gt;" data-icon-position="" data-hide-link-title="0"><span class="hw-responsive-img"><img class="highwire-fragment fragment-image lazyload" alt="Fig 3." src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F3.medium.gif" width="440" height="412"/><noscript><img class="highwire-fragment fragment-image" alt="Fig 3." src="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F3.medium.gif" width="440" height="412"/></noscript></span></a></div></div><ul class="highwire-figure-links inline"><li class="download-fig first"><a href="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F3.large.jpg?download=true" class="highwire-figure-link highwire-figure-link-download" title="Download Fig 3." data-icon-position="" data-hide-link-title="0">Download figure</a></li><li class="new-tab last"><a href="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F3.large.jpg" class="highwire-figure-link highwire-figure-link-newtab" target="_blank" data-icon-position="" data-hide-link-title="0">Open in new tab</a></li></ul></div><div class="fig-caption"><span class="fig-label">Fig 3.</span> <span class="caption-title">Inferring the reward from limited data.</span><p id="p-29" class="first-child">(<strong>A</strong>) The <em>r</em><sup>2</sup>-goodness of fit between the true reward, and the reward inferred using a finite number of samples (a sample is defined as an observation of the network state at a single time-point). The solid line indicates the <em>r</em><sup>2</sup> value averaged over 20 different simulations, while the shaded areas indicate the standard error on the mean. (<strong>B</strong>) Distribution of rewards inferred from a variable numbers of data samples. As the number of data samples is increased, the distribution of inferred rewards becomes more sharply peaked around 0 and 1 (reflecting the fact that the true reward was binary). (<strong>C</strong>) The KL-divergence between the optimal response distribution with altered input statistics (see <a id="xref-fig-2-9" class="xref-fig" href="#F2">Fig 2F</a>, upper) and the response distribution predicted using the reward inferred in the initial condition from a variable number of samples. The solid line indicates the KL-divergence averaged over 20 different simulations, while the shaded areas indicate the standard error on the mean. A horizontal dashed line indicates the KL-divergence between the response distribution with biased input and the original condition (that was used to infer the reward). (<strong>D</strong>) Same as panel (C), but where instead of altering the input statistics, we remove cells from the network (see <a id="xref-fig-2-10" class="xref-fig" href="#F2">Fig 2F</a>, lower).</p><div class="sb-div caption-clear"></div></div></div></div><div id="sec-6" class="subsection"><h3>Inferring efficiently encoded stimulus features</h3><p id="p-30">An influential hypothesis, called ‘efficient coding’, posits that sensory neural circuits have evolved to encode maximal information about sensory stimuli, given internal constraints [<a id="xref-ref-7-2" class="xref-bibr" href="#ref-7">7</a>–<a id="xref-ref-13-2" class="xref-bibr" href="#ref-13">13</a>]. However, the theory does not specify which stimulus features are relevant to the organism, and thus should be encoded. Here we show how one could use inverse RL to: (i) infer which stimulus features are encoded by a recorded neural network, and (ii) test whether these features are encoded efficiently.</p><p id="p-31">Efficient coding posits that neurons maximise information encoded about some relevant feature, <strong><em>y</em></strong> (<strong><em>x</em></strong>), given constraints on the information encoded by each neuron about their inputs, <strong><em>x</em></strong> (<a id="xref-fig-4-1" class="xref-fig" href="#F4">Fig 4A</a>). This corresponds to maximising:
<span class="disp-formula" id="disp-formula-9"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-12.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-12.gif"/></noscript></span>

</span>
where <em>λ</em> controls the strength of the constraint. Noting that the second term is equal to the coding cost we used previously (<a id="xref-disp-formula-5-1" class="xref-disp-formula" href="#disp-formula-5">Eqn 5</a>), we can rewrite this objective function as:
<span class="disp-formula" id="disp-formula-10"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-13.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-13.gif"/></noscript></span>

</span>
where we have omitted terms which don’t depend on <em>π</em>. Now this is exactly the same as the objective function we have been using so far (<a id="xref-disp-formula-5-2" class="xref-disp-formula" href="#disp-formula-5">Eqn 5</a>), in the special case where the reward function, <em>r</em> (<strong><em>σ, x</em></strong>), is equal to the log-posterior, log <em>p</em><sub><em>π</em></sub> (<strong><em>y</em></strong> (<strong><em>x</em></strong>)|<strong><em>σ</em></strong>). As a result we can maximise <em>L</em><sub><em>π</em></sub> via an iterative algorithm, where on each iteration we update the reward function by setting <em>r</em> (<strong><em>x, σ</em></strong>) ←log <em>p</em><sub><em>π</em></sub>(<strong><em>y</em></strong> (<strong><em>x</em></strong>) |<strong><em>σ</em></strong>), before then optimising the network dynamics, via entropy-regularised RL. Thus, thanks to the correspondence between entropy-regularised RL and efficient coding we could derive an algorithm to optimise the dynamics of a recurrent network to perform efficient coding [<a id="xref-ref-28-1" class="xref-bibr" href="#ref-28">28</a>].</p><div id="F4" class="fig pos-float type-figure  odd"><div class="highwire-figure"><div class="fig-inline-img-wrapper"><div class="fig-inline-img"><a href="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F4.large.jpg?width=800&amp;height=600&amp;carousel=1" title="Efficient coding and inverse RL. (A) The neural code was optimised to efficiently encode an external input, x, so as to maximise information about a relevant stimulus feature y (x). (B) The input, x consisted of 7 binary pixels. The relevant feature, y (x), was equal to 1 if &gt;3x pixels were active, and &#x2212;1 otherwise. (C) Optimising a network of 7 neurons to efficiently encode y (x) resulted in all neurons having identical tuning curves, which depended on the number of active pixels and total spike count. (D) The posterior probability that y = 1 varied monotonically with the spike count. (E) The optimised network encoded significantly more information about y (x) than a network of independent neurons with matching stimulus-dependent spiking probabilities, p (&#x3C3;i = 1|x). The coding cost used for the simulations in the other panels is indicated by a red circle. (F-G) We use the observed responses of the network (F) to infer the reward function optimised by the network, r (&#x3C3;, x) (G). If the network efficiently encodes a relevant feature, y (x), then the inferred reward (solid lines) should be proportional to the log-posterior, log p (y (x) |&#x3C3;) (empty circles). This allows us to (i) recover y (x) from observed neural responses, (ii) test whether this feature is encoded efficiently by the network. (H) We can use the inferred objective to predict how varying the input statistics, by reducing the probability that pixels are active, causes the population to split into two cell types, with different tuning curves and mean firing rates (right)." class="highwire-fragment fragment-images colorbox-load" rel="gallery-fragment-images-5248071" data-figure-caption="&lt;div class=&quot;highwire-markup&quot;&gt;&lt;div xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&gt;&lt;span class=&quot;caption-title&quot;&gt;Efficient coding and inverse RL.&lt;/span&gt; (&lt;strong&gt;A&lt;/strong&gt;) The neural code was optimised to efficiently encode an external input, &lt;strong&gt;&lt;em&gt;x&lt;/em&gt;&lt;/strong&gt;, so as to maximise information about a relevant stimulus feature &lt;em&gt;y&lt;/em&gt; (&lt;strong&gt;&lt;em&gt;x&lt;/em&gt;&lt;/strong&gt;). (&lt;strong&gt;B&lt;/strong&gt;) The input, &lt;strong&gt;&lt;em&gt;x&lt;/em&gt;&lt;/strong&gt; consisted of 7 binary pixels. The relevant feature, &lt;em&gt;y&lt;/em&gt; (&lt;strong&gt;&lt;em&gt;x&lt;/em&gt;&lt;/strong&gt;), was equal to 1 if &gt;3x pixels were active, and &#x2212;1 otherwise. (&lt;strong&gt;C&lt;/strong&gt;) Optimising a network of 7 neurons to efficiently encode &lt;em&gt;y&lt;/em&gt; (&lt;strong&gt;&lt;em&gt;x&lt;/em&gt;&lt;/strong&gt;) resulted in all neurons having identical tuning curves, which depended on the number of active pixels and total spike count. (&lt;strong&gt;D&lt;/strong&gt;) The posterior probability that &lt;em&gt;y&lt;/em&gt; = 1 varied monotonically with the spike count. (&lt;strong&gt;E&lt;/strong&gt;) The optimised network encoded significantly more information about &lt;em&gt;y&lt;/em&gt; (&lt;strong&gt;&lt;em&gt;x&lt;/em&gt;&lt;/strong&gt;) than a network of independent neurons with matching stimulus-dependent spiking probabilities, &lt;em&gt;p&lt;/em&gt; (&lt;em&gt;&#x3C3;&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt; = 1|&lt;strong&gt;&lt;em&gt;x&lt;/em&gt;&lt;/strong&gt;). The coding cost used for the simulations in the other panels is indicated by a red circle. (&lt;strong&gt;F&lt;/strong&gt;-&lt;strong&gt;G&lt;/strong&gt;) We use the observed responses of the network (&lt;strong&gt;F&lt;/strong&gt;) to infer the reward function optimised by the network, &lt;em&gt;r&lt;/em&gt; (&lt;strong&gt;&lt;em&gt;&#x3C3;, x&lt;/em&gt;&lt;/strong&gt;) (&lt;strong&gt;G&lt;/strong&gt;). If the network efficiently encodes a relevant feature, &lt;em&gt;y&lt;/em&gt; (&lt;strong&gt;&lt;em&gt;x&lt;/em&gt;&lt;/strong&gt;), then the inferred reward (solid lines) should be proportional to the log-posterior, log &lt;em&gt;p&lt;/em&gt; (&lt;em&gt;y&lt;/em&gt; (&lt;strong&gt;&lt;em&gt;x&lt;/em&gt;&lt;/strong&gt;) |&lt;strong&gt;&lt;em&gt;&#x3C3;&lt;/em&gt;&lt;/strong&gt;) (empty circles). This allows us to (i) recover &lt;em&gt;y&lt;/em&gt; (&lt;strong&gt;&lt;em&gt;x&lt;/em&gt;&lt;/strong&gt;) from observed neural responses, (ii) test whether this feature is encoded efficiently by the network. (&lt;strong&gt;H&lt;/strong&gt;) We can use the inferred objective to predict how varying the input statistics, by reducing the probability that pixels are active, causes the population to split into two cell types, with different tuning curves and mean firing rates (right).&lt;/div&gt;&lt;/div&gt;" data-icon-position="" data-hide-link-title="0"><span class="hw-responsive-img"><img class="highwire-fragment fragment-image lazyload" alt="Fig 4." src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F4.medium.gif" width="440" height="400"/><noscript><img class="highwire-fragment fragment-image" alt="Fig 4." src="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F4.medium.gif" width="440" height="400"/></noscript></span></a></div></div><ul class="highwire-figure-links inline"><li class="download-fig first"><a href="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F4.large.jpg?download=true" class="highwire-figure-link highwire-figure-link-download" title="Download Fig 4." data-icon-position="" data-hide-link-title="0">Download figure</a></li><li class="new-tab last"><a href="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F4.large.jpg" class="highwire-figure-link highwire-figure-link-newtab" target="_blank" data-icon-position="" data-hide-link-title="0">Open in new tab</a></li></ul></div><div class="fig-caption"><span class="fig-label">Fig 4.</span> <span class="caption-title">Efficient coding and inverse RL.</span><p id="p-32" class="first-child">(<strong>A</strong>) The neural code was optimised to efficiently encode an external input, <strong><em>x</em></strong>, so as to maximise information about a relevant stimulus feature <em>y</em> (<strong><em>x</em></strong>). (<strong>B</strong>) The input, <strong><em>x</em></strong> consisted of 7 binary pixels. The relevant feature, <em>y</em> (<strong><em>x</em></strong>), was equal to 1 if &gt;3x pixels were active, and −1 otherwise. (<strong>C</strong>) Optimising a network of 7 neurons to efficiently encode <em>y</em> (<strong><em>x</em></strong>) resulted in all neurons having identical tuning curves, which depended on the number of active pixels and total spike count. (<strong>D</strong>) The posterior probability that <em>y</em> = 1 varied monotonically with the spike count. (<strong>E</strong>) The optimised network encoded significantly more information about <em>y</em> (<strong><em>x</em></strong>) than a network of independent neurons with matching stimulus-dependent spiking probabilities, <em>p</em> (<em>σ</em><sub><em>i</em></sub> = 1|<strong><em>x</em></strong>). The coding cost used for the simulations in the other panels is indicated by a red circle. (<strong>F</strong>-<strong>G</strong>) We use the observed responses of the network (<strong>F</strong>) to infer the reward function optimised by the network, <em>r</em> (<strong><em>σ, x</em></strong>) (<strong>G</strong>). If the network efficiently encodes a relevant feature, <em>y</em> (<strong><em>x</em></strong>), then the inferred reward (solid lines) should be proportional to the log-posterior, log <em>p</em> (<em>y</em> (<strong><em>x</em></strong>) |<strong><em>σ</em></strong>) (empty circles). This allows us to (i) recover <em>y</em> (<strong><em>x</em></strong>) from observed neural responses, (ii) test whether this feature is encoded efficiently by the network. (<strong>H</strong>) We can use the inferred objective to predict how varying the input statistics, by reducing the probability that pixels are active, causes the population to split into two cell types, with different tuning curves and mean firing rates (right).</p><div class="sb-div caption-clear"></div></div></div><p id="p-33">As an illustration, we simulated a network of 7 neurons that receive a sensory input consisting of 7 binary pixels (<a id="xref-fig-4-2" class="xref-fig" href="#F4">Fig 4B</a>, top). In this example, the ‘relevant feature’, <em>y</em> (<strong><em>x</em></strong>) was a single binary variable, which was equal to 1 if 4 or more pixels were active, and −1 otherwise (<a id="xref-fig-4-3" class="xref-fig" href="#F4">Fig 4B</a>, bottom). Using the efficient-coding algorithm described above, we derived optimal tuning curves, showing how each neuron’s spiking probability should vary with both the number of active pixels and number of spikes fired by other neurons (<a id="xref-fig-4-4" class="xref-fig" href="#F4">Fig 4C</a>). We also derived how the optimal readout, <em>p</em> (<em>y</em>|<strong><em>σ</em></strong>), should depend on the number of spiking neurons (<a id="xref-fig-4-5" class="xref-fig" href="#F4">Fig 4D</a>). Finally, we verified that the optimised network encodes significantly more information about the relevant feature than a network of independent neurons, over a large range of coding costs (<a id="xref-fig-4-6" class="xref-fig" href="#F4">Fig 4E</a>).</p><p id="p-34">Now, imagine that we just observe the stimulus and neural responses (<a id="xref-fig-4-7" class="xref-fig" href="#F4">Fig 4F</a>). Can we recover the relevant feature, <em>y</em> (<strong><em>x</em></strong>)? To do this, we first use inverse RL to infer the reward function from observed neural responses (in exactly the same way as described in the previous section) (<a id="xref-fig-4-8" class="xref-fig" href="#F4">Fig 4G</a>). As before, we made no <em>a priori</em> assumptions about the parametric form of the reward function, which was allowed to vary freely as a function of the network state and input, (<strong><em>σ, x</em></strong>). Now, as described above, if the network is performing efficient coding then the inferred reward, <em>r</em> (<strong><em>σ, x</em></strong>) should be proportional to the log-posterior, log <em>p</em> (<em>y</em> (<strong><em>x</em></strong>)|<strong><em>σ</em></strong>). Thus, given <strong><em>σ</em></strong>, the inferred reward, <em>r</em> (<strong><em>σ, x</em></strong>) should only depend on changes to the input, <strong><em>x</em></strong>, that alter <em>y</em> (<strong><em>x</em></strong>). As a result, we can use the inferred reward to uncover all inputs, <strong><em>x</em></strong>, that map onto the same value of <em>y</em> (<strong><em>x</em></strong>). In our example, we see that the inferred reward collapses onto two curves only (blue and red in <a id="xref-fig-4-9" class="xref-fig" href="#F4">Fig 4G</a>), depending on the total number of pixels in the stimulus. This allows us to deduce that the relevant coded variable, <em>y</em> (<strong><em>x</em></strong>), must be a sharp threshold on the number of simultaneously active pixels. In contrast, the neural tuning curves vary smoothly with the number of active pixels (<a id="xref-fig-4-10" class="xref-fig" href="#F4">Fig 4C</a>). Next, having recovered <em>y</em> (<strong><em>x</em></strong>), we can check whether it is encoded efficiently by seeing whether the inferred reward, <em>r</em> (<strong><em>σ, x</em></strong>) is proportional to the log-posterior, log <em>p</em> (<em>y</em> (<strong><em>x</em></strong>)|<strong><em>σ</em></strong>).</p><p id="p-35">Note that our general approach could also generalise to more complex efficient coding models, where the encoded variable, <em>y</em>, is not a binary function of the input, <strong><em>x</em></strong>. In this case, we can perform a cluster analysis (e.g. k-means) to reveal which inputs, <strong><em>x</em></strong>, map onto similar reward. If the network is performing efficient coding then these inputs should also map onto the same encoded feature, <em>y</em> (<strong><em>x</em></strong>).</p><p id="p-36">Finally, once we have inferred the function performed by the network, we can predict how its dynamics will vary with context, such as when we alter the input statistics. For example, in our simulation, reducing the probability that input pixels are active causes the neural population to split into two cell-types, with distinct tuning curves and mean firing rates (<a id="xref-fig-4-11" class="xref-fig" href="#F4">Fig 4H</a>) [<a id="xref-ref-13-3" class="xref-bibr" href="#ref-13">13</a>].</p></div><div id="sec-7" class="subsection"><h3>Parametric model of neural responses</h3><p id="p-37">The basic framework described above is limited by the fact that the number of states, <em>n</em><sub><em>s</em></sub>, scales exponentially with the number of neurons (<em>n</em><sub><em>s</em></sub> = 2<sup><em>n</em></sup>). Thus, it will quickly become infeasible to compute the optimal dynamics as the number of neurons increases. Likewise, we will need an exponential amount of data to reliably estimate the sufficient statistics of the network, required to infer the reward function (<a id="xref-fig-3-6" class="xref-fig" href="#F3">Fig 3</a>).</p><p id="p-38">For larger networks, this problem can be circumvented by using tractable parametric approximation of the value function and reward functions. As an illustration, let us consider a network with no external input. If we approximate the value function by a quadratic function of the responses, our framework predicts a steady-state response distribution of the form: <span class="inline-formula" id="inline-formula-14"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-14.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-14.gif"/></noscript></span></span>, where <em>J</em><sub><em>ij</em></sub> denotes the pairwise couplings between neurons, and <em>h</em><sub><em>i</em></sub> is the bias. This corresponds to a pairwise Ising model, which has been used previously to model recorded neural responses [<a id="xref-ref-27-1" class="xref-bibr" href="#ref-27">27</a>, <a id="xref-ref-28-2" class="xref-bibr" href="#ref-28">28</a>]. (Note that different value functions could be used to give different neural models; e.g. choosing <em>v</em> (<strong><em>x</em></strong>) = <em>f</em> (<strong><em>w</em> ·<em>x</em></strong>), where <strong><em>x</em></strong> is the feed-forward input, results in a linear-nonlinear neural model.) In Methods section we derive an algorithm to optimise the coupling matrix, <strong><em>J</em></strong>, for a given reward function and coding cost.</p><p id="p-39">To illustrate this, we simulated a network of 12 neurons arranged in a ring, with reward function equal to 1 if exactly 4 adjacent neurons are active together, and 0 otherwise. After optimisation, nearby neurons were found to have positive couplings, while distant neurons had negative couplings (<a id="xref-fig-5-1" class="xref-fig" href="#F5">Fig 5A</a>). The network dynamics generate a single hill of activity which drifts smoothly in time. This is reminiscent of ring attractor models, which have been influential in modeling neural functions such as the rodent and fly head direction system [<a id="xref-ref-29-1" class="xref-bibr" href="#ref-29">29</a>–<a id="xref-ref-31-1" class="xref-bibr" href="#ref-31">31</a>]. (Indeed, <a id="xref-disp-formula-6-2" class="xref-disp-formula" href="#disp-formula-6">eqn. 6</a> suggests why our framework generally leads to attractor dynamics, as each transition will tend to drive the network to higher-value ‘attractor states’.)</p><div id="F5" class="fig pos-float type-figure  odd"><div class="highwire-figure"><div class="fig-inline-img-wrapper"><div class="fig-inline-img"><a href="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F5.large.jpg?width=800&amp;height=600&amp;carousel=1" title="Pairwise coupled network. (A) We optimized the parameters of a pairwise coupled network, using a reward function that was equal to 1 when exactly 4 adjacent neurons were simultaneously active, and 0 otherwise. The resulting couplings between neurons are schematized on the left, with positive couplings in red and negative couplings in blue. The exact coupling strengths are plotted in the centre. On the right we show an example of the network dynamics. Using inverse RL, we can infer the original reward function used to optimise the network from its observed dynamics. We can then use this inferred reward to predict how the network dynamics will vary when we increase the coding cost (B), remove connections between distant neurons (C) or selectively activate certain neurons (D)." class="highwire-fragment fragment-images colorbox-load" rel="gallery-fragment-images-5248071" data-figure-caption="&lt;div class=&quot;highwire-markup&quot;&gt;&lt;div xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&gt;&lt;span class=&quot;caption-title&quot;&gt;Pairwise coupled network.&lt;/span&gt; (&lt;strong&gt;A&lt;/strong&gt;) We optimized the parameters of a pairwise coupled network, using a reward function that was equal to 1 when exactly 4 adjacent neurons were simultaneously active, and 0 otherwise. The resulting couplings between neurons are schematized on the left, with positive couplings in red and negative couplings in blue. The exact coupling strengths are plotted in the centre. On the right we show an example of the network dynamics. Using inverse RL, we can infer the original reward function used to optimise the network from its observed dynamics. We can then use this inferred reward to predict how the network dynamics will vary when we increase the coding cost (&lt;strong&gt;B&lt;/strong&gt;), remove connections between distant neurons (&lt;strong&gt;C&lt;/strong&gt;) or selectively activate certain neurons (&lt;strong&gt;D&lt;/strong&gt;).&lt;/div&gt;&lt;/div&gt;" data-icon-position="" data-hide-link-title="0"><span class="hw-responsive-img"><img class="highwire-fragment fragment-image lazyload" alt="Fig 5." src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F5.medium.gif" width="278" height="440"/><noscript><img class="highwire-fragment fragment-image" alt="Fig 5." src="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F5.medium.gif" width="278" height="440"/></noscript></span></a></div></div><ul class="highwire-figure-links inline"><li class="download-fig first"><a href="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F5.large.jpg?download=true" class="highwire-figure-link highwire-figure-link-download" title="Download Fig 5." data-icon-position="" data-hide-link-title="0">Download figure</a></li><li class="new-tab last"><a href="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F5.large.jpg" class="highwire-figure-link highwire-figure-link-newtab" target="_blank" data-icon-position="" data-hide-link-title="0">Open in new tab</a></li></ul></div><div class="fig-caption"><span class="fig-label">Fig 5.</span> <span class="caption-title">Pairwise coupled network.</span><p id="p-40" class="first-child">(<strong>A</strong>) We optimized the parameters of a pairwise coupled network, using a reward function that was equal to 1 when exactly 4 adjacent neurons were simultaneously active, and 0 otherwise. The resulting couplings between neurons are schematized on the left, with positive couplings in red and negative couplings in blue. The exact coupling strengths are plotted in the centre. On the right we show an example of the network dynamics. Using inverse RL, we can infer the original reward function used to optimise the network from its observed dynamics. We can then use this inferred reward to predict how the network dynamics will vary when we increase the coding cost (<strong>B</strong>), remove connections between distant neurons (<strong>C</strong>) or selectively activate certain neurons (<strong>D</strong>).</p><div class="sb-div caption-clear"></div></div></div><p id="p-41">As before, we can then use inverse RL to infer the reward function from the observed network dynamics. However, note that when we use a parametric approximation of the value function this problem is not well-posed, and we have to make additional assumptions about the form of the reward function. We first assumed a ‘sparse’ reward function, where only a small number of states, <strong><em>σ</em></strong>, are assumed to be associated with non-zero positive reward (see Methods). Using this assumption, we could well recover the true reward function from observations of the optimised neural responses (with an <em>r</em><sup>2</sup> value greater than 0.9).</p><p id="p-42">Having inferred the reward function optimised by the network, we can then use it to predict how the coupling matrix, <strong><em>J</em></strong>, and network dynamics will vary if we alter the internal/external constraints. For example, we can use the inferred reward to predict how increasing the coding cost will result in stronger positive couplings between nearby neurons and a hill of activity that sometimes jumps discontinuously between locations (<a id="xref-fig-5-2" class="xref-fig" href="#F5">Fig 5B</a>); removing connections between distant neurons will result in two uncoordinated peaks of activity (<a id="xref-fig-5-3" class="xref-fig" href="#F5">Fig 5C</a>); finally, selectively activating certain neurons will ‘pin’ the hill of activity to a single location (<a id="xref-fig-5-4" class="xref-fig" href="#F5">Fig 5D</a>).</p><p id="p-43">To illustrate the effect of assuming different reward functions, we considered two different sets of assumptions (in addition to the sparse model, described above): a ‘pairwise model’, where the reward is assumed to be a quadratic function of the network state, and a ‘global model’ where the reward is assumed to depend only on global spike count (see Methods). In all three cases, the inferred reward function provided a reasonable fit to the true reward function, (averaged over states visited by the network; <a id="xref-fig-6-1" class="xref-fig" href="#F6">Fig 6A</a>). However, only the sparse and pairwise models were able to predict how neural responses changed when, for example, we optimised the network with a higher coding-cost (<a id="xref-fig-6-2" class="xref-fig" href="#F6">Fig 6B</a>).</p><div id="F6" class="fig pos-float type-figure  odd"><div class="highwire-figure"><div class="fig-inline-img-wrapper"><div class="fig-inline-img"><a href="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F6.large.jpg?width=800&amp;height=600&amp;carousel=1" title="Effect of assuming different types of reward function. We compared the inferred reward when we assumed a sparse model (i.e. a small number of states associated with non-zero positive reward) a pairwise model (i.e. the reward depends on the first and second-order response statistics) and a global model (i.e. the reward depends on the total number of active neurons only). (A) r2 goodness of fit between the true and the inferred reward, assuming a sparse, pairwise, or global model. (B) The KL-divergence between the optimal response distribution with high coding cost (see Fig 5B) and the response distribution predicted using the reward inferred in the initial condition, assuming a sparse, pairwise, or global model. A horizontal dashed line indicates the KL-divergence between the response distribution with high-coding cost and the original condition (that was used to infer the reward)." class="highwire-fragment fragment-images colorbox-load" rel="gallery-fragment-images-5248071" data-figure-caption="&lt;div class=&quot;highwire-markup&quot;&gt;&lt;div xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&gt;&lt;span class=&quot;caption-title&quot;&gt;Effect of assuming different types of reward function.&lt;/span&gt; We compared the inferred reward when we assumed a sparse model (i.e. a small number of states associated with non-zero positive reward) a pairwise model (i.e. the reward depends on the first and second-order response statistics) and a global model (i.e. the reward depends on the total number of active neurons only). (&lt;strong&gt;A&lt;/strong&gt;) &lt;em&gt;r&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt; goodness of fit between the true and the inferred reward, assuming a sparse, pairwise, or global model. (&lt;strong&gt;B&lt;/strong&gt;) The KL-divergence between the optimal response distribution with high coding cost (see Fig 5B) and the response distribution predicted using the reward inferred in the initial condition, assuming a sparse, pairwise, or global model. A horizontal dashed line indicates the KL-divergence between the response distribution with high-coding cost and the original condition (that was used to infer the reward).&lt;/div&gt;&lt;/div&gt;" data-icon-position="" data-hide-link-title="0"><span class="hw-responsive-img"><img class="highwire-fragment fragment-image lazyload" alt="Fig 6." src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F6.medium.gif" width="440" height="221"/><noscript><img class="highwire-fragment fragment-image" alt="Fig 6." src="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F6.medium.gif" width="440" height="221"/></noscript></span></a></div></div><ul class="highwire-figure-links inline"><li class="download-fig first"><a href="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F6.large.jpg?download=true" class="highwire-figure-link highwire-figure-link-download" title="Download Fig 6." data-icon-position="" data-hide-link-title="0">Download figure</a></li><li class="new-tab last"><a href="https://www.biorxiv.org/content/biorxiv/early/2020/02/06/598086/F6.large.jpg" class="highwire-figure-link highwire-figure-link-newtab" target="_blank" data-icon-position="" data-hide-link-title="0">Open in new tab</a></li></ul></div><div class="fig-caption"><span class="fig-label">Fig 6.</span> <span class="caption-title">Effect of assuming different types of reward function.</span><p id="p-44" class="first-child">We compared the inferred reward when we assumed a sparse model (i.e. a small number of states associated with non-zero positive reward) a pairwise model (i.e. the reward depends on the first and second-order response statistics) and a global model (i.e. the reward depends on the total number of active neurons only). (<strong>A</strong>) <em>r</em><sup>2</sup> goodness of fit between the true and the inferred reward, assuming a sparse, pairwise, or global model. (<strong>B</strong>) The KL-divergence between the optimal response distribution with high coding cost (see <a id="xref-fig-5-5" class="xref-fig" href="#F5">Fig 5B</a>) and the response distribution predicted using the reward inferred in the initial condition, assuming a sparse, pairwise, or global model. A horizontal dashed line indicates the KL-divergence between the response distribution with high-coding cost and the original condition (that was used to infer the reward).</p><div class="sb-div caption-clear"></div></div></div></div></div><div class="section" id="sec-8"><h2 class="">Discussion</h2><p id="p-45">A large research effort has been devoted to developing ‘top-down’ models, which describe the network dynamics required to optimally perform a given function (e.g. decision making [<a id="xref-ref-6-1" class="xref-bibr" href="#ref-6">6</a>], control [<a id="xref-ref-3-2" class="xref-bibr" href="#ref-3">3</a>], efficient sensory coding [<a id="xref-ref-8-1" class="xref-bibr" href="#ref-8">8</a>] etc.). Here we show that optimising a recurrent neural network can often be formulated as a multi-agent RL problem. This insight can provide new ways to: (i) train recurrent networks to perform a given function; (ii) infer the function optimised by a network from recorded data; (iii) predict the dynamics of a network upon perturbation.</p><p id="p-46">An alternative ‘bottom-up’ approach is to construct phenomenological models describing how neurons respond to given sensory stimuli and/or neural inputs [<a id="xref-ref-27-2" class="xref-bibr" href="#ref-27">27</a>, <a id="xref-ref-28-3" class="xref-bibr" href="#ref-28">28</a>, <a id="xref-ref-32-1" class="xref-bibr" href="#ref-32">32</a>, <a id="xref-ref-33-1" class="xref-bibr" href="#ref-33">33</a>]. In common with our work, such models are directly fitted to neural data. However, these models generally do not set out to reveal the function performed by the network. Further, they are often poor at predicting neural responses in different contexts (e.g. varying stimulus statistics). Here we hypothesize that it is the function performed by a neural circuit that remains invariant, not its dynamics or individual cell properties. Thus, if we can infer what this function is, we should be able to predict how the network dynamics will adapt depending on the context, so as to perform the same function under different constraints. As a result, our theory could predict how the dynamics of a recorded network will adapt in response to a large range of experimental manipulations, such as varying the stimulus statistics, blocking connections, knocking out/stimulating cells etc. (Note however, that to predict how the network adapts to these changes, we will need to infer the ‘full’ reward function; in some cases, this may require measuring neural responses in multiple environments.)</p><p id="p-47">Another approach is to use statistical methods, such as dimensionality reduction techniques, to infer information about the network dynamics (such as which states are most frequently visited), which can then be used to try and gain insight about the function it performs [<a id="xref-ref-34-1" class="xref-bibr" href="#ref-34">34</a>–<a id="xref-ref-37-1" class="xref-bibr" href="#ref-37">37</a>]. For example, in the context of sensory coding, an approach called ‘maximally informative dimensions’ seeks to find a low-dimensional projection of the stimulus that is most informative about a neuron’s responses [<a id="xref-ref-38-1" class="xref-bibr" href="#ref-38">38</a>]. However, in contrast to our approach, such approaches do not allow us to recover the objective function optimised by the network. As a result, they do not predict how neural responses will alter depending on the the internal/external constraints. It is also not clear how to relate existing dimensionality reduction methods, such as PCA, to the dynamics of a <em>recurrent</em> neural network (e.g. are certain states visited frequently because of the reward, external stimulus, or internal dynamics?). Nonetheless, in future work it could be interesting to see if dimensionality reduction techniques could be used to first recover a compressed version of the data, from which we could more easily use inverse RL methods to infer the objective function.</p><p id="p-48">There is an extensive literature on how neural networks could perform RL [<a id="xref-ref-39-1" class="xref-bibr" href="#ref-39">39</a>–<a id="xref-ref-41-1" class="xref-bibr" href="#ref-41">41</a>]. Our focus here was different: we sought to use tools from RL and inverse RL to infer the function performed by a recurrent neural network. Thus, we do not assume the network receives an explicit reward signal: the reward function is simply a way of expressing which states of the network are useful for performing a given function. In contrast to previous work, we treat each neuron as an independent agent, which optimises their responses to maximise the reward achieved by the network, given a constraint on how much they can encode about their inputs. As well as being required for biological realism, the coding constraint has the benefit of making the inverse RL problem well-posed. Indeed, under certain assumptions, we show that it is possible to write a closed form expression for the reward function optimised by the network, given its steady-state distribution (<a id="xref-disp-formula-40-1" class="xref-disp-formula" href="#disp-formula-40">Eqn 40</a>).</p><p id="p-49">Our framework relies on several assumptions about the network dynamics. First, we assume that the network has Markov dynamics, such that its state depends only on the preceding time-step. To relax this assumption, we could redefine the network state to include spiking activity in several time-steps. For example, we could thus include the fact that neurons are unlikely to fire two spikes within a given temporal window, called their refractory period. Of course, this increase in complexity would come at the expense of decreased computational tractability, which may necessitate approximations. Second, we assume the only constraint that neurons face is a ‘coding cost’, which limits how much information they encode about other neurons and external inputs. In reality, biological networks face many other constraints, such as the metabolic cost of spiking and constraints on the wiring between cells. Some of these constraints could be included explicitly as part of the inference process. For example, by assuming a specific form of approximate value function (e.g. a quadratic function of the neural responses), we can include explicit constraints about the network connectivity (e.g. pairwise connections between neurons). Other constraints (e.g. the metabolic cost of firing a spike), that are not assumed explicitly can be incorporated implicitly as part of the inferred reward function (e.g. a lower inferred reward for metabolically costly states, associated with high firing rates).</p><p id="p-50">In addition to assumptions about the network dynamics, for large networks we will also need to assume a particular parametric form for the reward function. For example, in the context of efficient coding (<a id="xref-fig-4-12" class="xref-fig" href="#F4">Fig 4</a>), this is equivalent to assuming a particular form for the decoder model (e.g. a linear decoder) that ‘reads-out’ the encoded variable from neural responses [<a id="xref-ref-10-3" class="xref-bibr" href="#ref-10">10</a>]. To test the validity of such assumptions, we could see how well the reward function, inferred in one context, was able to generalise to predict neural responses in other contexts.</p><p id="p-51">For our framework to make predictions, the network must adapt its dynamics to perform the same function under different internal/external constraints. Previous work suggests that this may hold in certain cases, in response to changes in stimulus statistics [<a id="xref-ref-42-1" class="xref-bibr" href="#ref-42">42</a>, <a id="xref-ref-43-1" class="xref-bibr" href="#ref-43">43</a>], or optogonetic perturbations [<a id="xref-ref-44-1" class="xref-bibr" href="#ref-44">44</a>]. Even so, it may be that real neural networks only partially adapt to their new environments. It would thus be interesting, in the future, to extend our framework to deal with this. For example, we could assess whether contextual changes in the neural responses enable the network to ascend the gradient of the objective function (given the new constraints) as predicted by our model.</p><p id="p-52">A central tenet of our work is that the neural network has evolved to perform a specific function optimally, given constraints. As such, we could obtain misleading results if the recorded network is only approximately optimal. To deal with this, recent work by one of the present authors [<a id="xref-ref-45-1" class="xref-bibr" href="#ref-45">45</a>] proposed a framework in which the neural network is assumed to satisfy a <em>known</em> optimality criterion <em>approximately</em>. In this work, the optimality criterion is formulated as a Bayesian prior, which serves to nudge the network towards desirable solutions. This contrasts with the work presented here, where the network is assumed to satisfy an <em>unknown</em> optimality criterion <em>exactly</em>. Future work could explore the intersection between these two approaches, where the network is assumed to perform an <em>unknown</em> optimality criterion <em>approximately</em>. In this case, one will likely need to limit the space of possible reward functions, so that inference problem remains well-posed.</p><p id="p-53">Our work unifies several influential theories of neural coding, that were considered separately in previous work. For example, we show a direct link between entropy-regularised RL [<a id="xref-ref-15-2" class="xref-bibr" href="#ref-15">15</a>–<a id="xref-ref-18-4" class="xref-bibr" href="#ref-18">18</a>] (<a id="xref-fig-1-13" class="xref-fig" href="#F1">Fig 1</a>), ring-attractor networks [<a id="xref-ref-29-2" class="xref-bibr" href="#ref-29">29</a>–<a id="xref-ref-31-2" class="xref-bibr" href="#ref-31">31</a>] (<a id="xref-fig-5-6" class="xref-fig" href="#F5">Fig 5</a>), and efficient sensory coding [<a id="xref-ref-7-3" class="xref-bibr" href="#ref-7">7</a>–<a id="xref-ref-13-4" class="xref-bibr" href="#ref-13">13</a>] (<a id="xref-fig-4-13" class="xref-fig" href="#F4">Fig 4</a>). Further, given a static network without dynamics, our framework is directly equivalent to rate-distortion theory [<a id="xref-ref-25-2" class="xref-bibr" href="#ref-25">25</a>, <a id="xref-ref-26-2" class="xref-bibr" href="#ref-26">26</a>]. Many of these connections are non-trivial. For example, the problem of how to train a network to efficiently code its inputs remains an open avenue of research. Thus, the realisation that efficient sensory coding by a recurrent network can be formulated as a multi-agent RL problem could help develop of future algorithms for learning efficient sensory representations (e.g., in contrast with brute force numerical optimization as in [<a id="xref-ref-9-1" class="xref-bibr" href="#ref-9">9</a>]). Indeed, recent work has shown how, by treating a feed-forward neural network as a multi-agent RL system, one can efficiently train the network to perform certain using only local learning rules [<a id="xref-ref-46-1" class="xref-bibr" href="#ref-46">46</a>]. However, while interesting in its own right, the generality of our framework means that we could potentially apply our theory to infer the function performed by diverse neural circuits, that have evolved to perform a broad range of different functional objectives. This contrasts with previous work, where neural data is often used to test a single top-down hypothesis, formulated in advance.</p></div><div class="section" id="sec-9"><h2 class="">Methods</h2><div id="sec-10" class="subsection"><h3>Entropy-regularised RL</h3><p id="p-54">We consider a Markov Decision Process (MDP) where each state of the agent <em>s</em>, is associated with a reward, <em>r</em>(<em>s</em>). At each time, the agent performs an action, <em>a</em>, sampled from a probability distribution, <em>π</em>(<em>a</em>|<em>s</em>), called their policy. A new state, <em>s</em>′, then occurs with a probability, <em>p</em>(<em>s</em>′ |<em>s, a</em>).</p><p id="p-55">We seek a policy, <em>π</em> (<em>a</em>|<em>s</em>), that maximises the average reward, constrained on the mutual information between between actions and states. This corresponds to maximising the Lagrangian:
<span class="disp-formula" id="disp-formula-11"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-17.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-17.gif"/></noscript></span>

</span>
<span class="disp-formula" id="disp-formula-12"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-18.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-18.gif"/></noscript></span>

</span>
where <em>λ</em> is a lagrange-multiplier that determines the strength of the constraint, and <em>c</em><sub><em>π</em></sub> (<em>s</em>) = <em>D</em><sub><em>KL</em></sub> [<em>π</em> (<em>a</em>|<em>s</em>)‖<em>p</em>(<em>a</em>)].</p><p id="p-56">Note that while in the rest of the paper we consider continuous tasks, where the Lagrangian is obtained by averaging over the steady-state distribution, our framework can also be applied without little changes to finite tasks, which occur within a time window, and where the Lagrangian is given by:</p><p id="p-57"><span class="inline-formula" id="inline-formula-15"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-15.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-15.gif"/></noscript></span></span>. Unlike the continuous task described above, the optimal network dynamics may not have a corresponding equilibrium distribution [<a id="xref-ref-14-3" class="xref-bibr" href="#ref-14">14</a>].</p><p id="p-58">Now, let us can define a value function:
<span class="disp-formula" id="disp-formula-13"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-19.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-19.gif"/></noscript></span>

</span>
where <em>s, s</em>′ and <em>s</em>″ denote the agent’s state in three consecutive time-steps. We can write the following Bellmann equality for the value function:
<span class="disp-formula" id="disp-formula-14"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-20.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-20.gif"/></noscript></span>

</span>
</p><p id="p-59">Now, consider the following greedy update of the policy, <em>π</em> (<em>a</em>|<em>s</em>):
<span class="disp-formula" id="disp-formula-15"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-21.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-21.gif"/></noscript></span>

</span>
<span class="disp-formula" id="disp-formula-16"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-22.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-22.gif"/></noscript></span>

</span>
</p><p id="p-60">To preform this maximisation, we write the following Lagrangian:
<span class="disp-formula" id="disp-formula-17"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-23.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-23.gif"/></noscript></span>

</span>
where the last term is required to enforce the constraint that ∑<sub><em>a</em></sub> <em>π</em> (<em>a</em>|<em>s</em>) = 1. Setting the derivative to zero with respect to <em>γ</em> (<em>s</em>) and <em>π</em> (<em>a</em>|<em>s</em>), we have:
<span class="disp-formula" id="disp-formula-18"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-24.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-24.gif"/></noscript></span>

</span>
<span class="disp-formula" id="disp-formula-19"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-25.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-25.gif"/></noscript></span>

</span>
</p><p id="p-61">We can solve these equations to obtain the optimal greedy update:
<span class="disp-formula" id="disp-formula-20"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-26.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-26.gif"/></noscript></span>

</span>
where <em>Z</em><sub><em>π</em></sub> (<em>s</em>) is a normalisation constant.</p><p id="p-62">Using the policy improvement theorem [<a id="xref-ref-14-4" class="xref-bibr" href="#ref-14">14</a>], we can show that this greedy policy update is guaranteed to increase <em>L</em><sub><em>π</em></sub>. To see this, we can substitute <em>π</em>* into the Bellmann equation to give the following inequality:
<span class="disp-formula" id="disp-formula-21"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-27.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-27.gif"/></noscript></span>

</span>
<span class="disp-formula" id="disp-formula-22"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-28.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-28.gif"/></noscript></span>

</span>
</p><p id="p-63">Next, we take the average of both sides with respect to the steady-state distribution, <em>p</em><sub><em>π</em>*</sub> (<em>s</em>):
<span class="disp-formula" id="disp-formula-23"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-29.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-29.gif"/></noscript></span>

</span>
<span class="disp-formula" id="disp-formula-24"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-30.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-30.gif"/></noscript></span>

</span>
</p><p id="p-64">Thus, repeated application of the Bellmann recursion (<a id="xref-disp-formula-14-1" class="xref-disp-formula" href="#disp-formula-14">Eqn 14</a>) and greedy policy update (<a id="xref-disp-formula-20-1" class="xref-disp-formula" href="#disp-formula-20">Eqn 20</a>) will return the optimal policy, <em>π</em>*(<em>a</em>|<em>s</em>), which maximises <em>L</em><sub><em>π</em></sub>.</p><div id="sec-11" class="subsection"><h4>Inverse entropy-regularized RL</h4><p id="p-65">We can write the Bellmann recursion in <a id="xref-disp-formula-14-2" class="xref-disp-formula" href="#disp-formula-14">Eqn 14</a> in vector form:
<span class="disp-formula" id="disp-formula-25"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-31.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-31.gif"/></noscript></span>

</span>
where <strong><em>v, c</em></strong> and <strong><em>r</em></strong> are vectors with elements, <em>v</em><sub><em>s</em></sub> ≡ <em>v</em> (<em>s</em>), <em>c</em><sub><em>s</em></sub> ≡ <em>c</em> (<em>s</em>), and <em>r</em><sub><em>s</em></sub> ≡ <em>r</em> (<em>s</em>). <strong><em>P</em></strong> is a matrix with elements <em>P</em><sub><em>ss</em></sub>′ = <em>p</em><sub><em>π</em></sub> (<em>s</em>′|<em>s</em>). We have defined <span class="inline-formula" id="inline-formula-16"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-16.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-16.gif"/></noscript></span></span> and <span class="inline-formula" id="inline-formula-17"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-17.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-17.gif"/></noscript></span></span> (and thus <em>L</em><sub><em>π</em></sub> = <em>r</em><sub>0</sub> − <em>λc</em><sub>0</sub>). Rearranging:
<span class="disp-formula" id="disp-formula-26"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-32.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-32.gif"/></noscript></span>

</span>
</p><p id="p-66">We can solve this system of equations (up to an arbitrary constant, <em>v</em><sub>0</sub>) to find an expression for <strong><em>v</em></strong> as a linear function of the reward:
<span class="disp-formula" id="disp-formula-27"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-33.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-33.gif"/></noscript></span>

</span>
</p><p id="p-67">Substituting into <a id="xref-disp-formula-20-2" class="xref-disp-formula" href="#disp-formula-20">Eqn 20</a>, we can express the agent’s policy directly as a function of the reward:
<span class="disp-formula" id="disp-formula-28"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-34.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-34.gif"/></noscript></span>

</span>
where <strong><em>π</em></strong><sub><em>a</em></sub> is a vector with elements, (<strong><em>π</em></strong><sub><em>a</em></sub>)<sub><em>s</em></sub> ≡ <em>π</em> (<em>a</em>|<em>s</em>) and <strong><em>P</em></strong><sub><em>a</em></sub> is a matrix, with elements (<strong><em>P</em></strong> <sub><em>a</em></sub>)<sub><em>ss</em>′</sub> ≡ <em>p</em> (<em>s</em>′ |<em>a, s</em>).</p><p id="p-68">To infer the reward function, <em>r</em> (<em>s</em>) (up to an irrelevant constant and multiplicative factor, <em>λ</em>), we use the observed policy, <em>π</em> (<em>a</em>|<em>s</em>) and transition probabilities, <em>p</em> (<em>s</em>′|<em>a, s</em>), to estimate <strong><em>b, A, P</em></strong><sub><em>a</em></sub> and <em>p</em><sub><em>a</em></sub>. We then perform numerical optimisation to find the reward that maximises the log-likelihood of the optimal policy in <a id="xref-disp-formula-28-1" class="xref-disp-formula" href="#disp-formula-28">Eqn 28</a>, ⟨log <em>π</em>* (<em>a</em>|<em>s</em>) ⟩<sub>𝒟</sub>, averaged over observed data, 𝒟.</p></div></div><div id="sec-12" class="subsection"><h3>Optimising a neural network via RL</h3><p id="p-69">We consider a recurrent neural network, with <em>n</em> neurons, each described by a binary variable, <em>σ</em><sub><em>i</em></sub> = −1<em>/</em>1, denoting whether a given neuron is silent/fires a spike in each temporal window. The network receives an external input, <strong><em>x</em></strong>. The network state is described by a vector of <em>n</em> binary values, <strong><em>σ</em></strong> = (<em>σ</em><sub>1</sub>, <em>σ</em><sub>2</sub>, …, <em>σ</em><sub><em>n</em></sub>)<sup><em>T</em></sup>. Both the network and input are assumed to have Markov dynamics. Neurons are updated asynchronously, by updating a random neuron at each time-step with probability <span class="inline-formula" id="inline-formula-18"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-18.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-18.gif"/></noscript></span></span>. The network dynamics are thus described by:
<span class="disp-formula" id="disp-formula-29"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-35.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-35.gif"/></noscript></span>

</span>
where <span class="inline-formula" id="inline-formula-19"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-19.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-19.gif"/></noscript></span></span> if <span class="inline-formula" id="inline-formula-20"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-20.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-20.gif"/></noscript></span></span> and 0 otherwise.</p><p id="p-70">Equivalently, we can say that at each time, a set of proposed updates, <span class="inline-formula" id="inline-formula-21"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-21.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-21.gif"/></noscript></span></span>, are independently sampled from <span class="inline-formula" id="inline-formula-22"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-22.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-22.gif"/></noscript></span></span>, and then a neuron <em>i</em> is selected at random to be updated, such that <span class="inline-formula" id="inline-formula-23"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-23.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-23.gif"/></noscript></span></span>.</p><p id="p-71">We define a reward function, <em>r</em> (<strong><em>σ, x</em></strong>), describing which states are ‘desirable’ for the network to perform a given function. The network dynamics are said to be optimal if they maximise the average reward, ⟨<em>r</em> (<strong><em>σ, x</em></strong>)⟩<sub><em>p</em>(<strong><em>σ</em></strong>,<strong><em>x</em></strong>)</sub> given a constraint on how much each neuron encodes about its inputs <span class="inline-formula" id="inline-formula-24"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-24.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-24.gif"/></noscript></span></span>. This corresponds to maximising the objective function:
<span class="disp-formula" id="disp-formula-30"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-36.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-36.gif"/></noscript></span>

</span>
<span class="disp-formula" id="disp-formula-31"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-37.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-37.gif"/></noscript></span>

</span>
where <span class="inline-formula" id="inline-formula-25"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-25.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-25.gif"/></noscript></span></span> is the coding cost associated with each state, and penalises deviations from each neuron’s average firing rate.</p><p id="p-72">We can decompose the transition probability for the network (<a id="xref-disp-formula-29-1" class="xref-disp-formula" href="#disp-formula-29">Eqn 29</a>), into the probability that a given neuron proposes an update, <span class="inline-formula" id="inline-formula-26"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-26.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-26.gif"/></noscript></span></span>, given the network state, <strong><em>σ</em></strong>, and the probability of the new network state, <strong><em>σ</em></strong>′, given <span class="inline-formula" id="inline-formula-27"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-27.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-27.gif"/></noscript></span></span> and <strong><em>σ</em></strong>:
<span class="disp-formula" id="disp-formula-32"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-38.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-38.gif"/></noscript></span>

</span>
<span class="disp-formula" id="disp-formula-33"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-39.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-39.gif"/></noscript></span>

</span>
</p><p id="p-73">Thus, the problem faced by each neuron, of optimising <span class="inline-formula" id="inline-formula-28"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-28.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-28.gif"/></noscript></span></span> so as to maximise <em>L</em>, is equivalent to the MDP described in Methods section, where the action <em>a</em>, and state <em>s</em> correspond to the neuron’s proposed update <span class="inline-formula" id="inline-formula-29"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-29.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-29.gif"/></noscript></span></span> and state of the network and external inputs {<strong><em>σ, x</em></strong>}. Thus, we can follow the exact same steps as in Methods section, to show that <span class="inline-formula" id="inline-formula-30"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-30.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-30.gif"/></noscript></span></span> is optimised via the following updates:
<span class="disp-formula" id="disp-formula-34"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-40.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-40.gif"/></noscript></span>

</span>
<span class="disp-formula" id="disp-formula-35"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-41.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-41.gif"/></noscript></span>

</span>
where <strong><em>σ</em></strong><sub><em>/i</em></sub> denotes the state of all neurons except for neuron <em>i</em>. As updating the policy for any given neuron increases the objective function, <em>L</em>, we can alternate updates for different neurons to optimise the dynamics of the network.</p></div><div id="sec-13" class="subsection"><h3>Inferring network function via inverse RL</h3><p id="p-74">After convergence, we can substitute the expression for the optimal policy into the Bellman equality, to obtain:
<span class="disp-formula" id="disp-formula-36"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-42.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-42.gif"/></noscript></span>

</span>
</p><p id="p-75">Rearranging, we have:
<span class="disp-formula" id="disp-formula-37"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-43.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-43.gif"/></noscript></span>

</span>
</p><p id="p-76">Thus, if we can infer the value function from the observed neural responses, then we can recover the associated reward function through <a id="xref-disp-formula-37-1" class="xref-disp-formula" href="#disp-formula-37">Eqn 37</a>.</p><p id="p-77">To derive an expression for the reward, we first consider the case where there is no external input. In this case, the optimal neural dynamics (<a id="xref-disp-formula-35-1" class="xref-disp-formula" href="#disp-formula-35">Eqn 35</a>) correspond to Gibbs sampling from:
<span class="disp-formula" id="disp-formula-38"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-44.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-44.gif"/></noscript></span>

</span>
</p><p id="p-78">Rearranging, we have a closed-form expression for the value function, <em>v</em> (<strong><em>σ</em></strong>), in terms of the steady-state distribution:
<span class="disp-formula" id="disp-formula-39"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-45.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-45.gif"/></noscript></span>

</span>
</p><p id="p-79">We can then combine <a id="xref-disp-formula-39-1" class="xref-disp-formula" href="#disp-formula-39">Eqn 39</a> and <a id="xref-disp-formula-37-2" class="xref-disp-formula" href="#disp-formula-37">Eqn 37</a> to obtain a closed-form expression for the reward function (up to an irrelevant constant):
<span class="disp-formula" id="disp-formula-40"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-46.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-46.gif"/></noscript></span>

</span>
</p><p id="p-80">Since we don’t know the true value of <em>λ</em>, we can simply set it to unity. In this case, our inferred reward will differ from the true reward by a factor of <span class="inline-formula" id="inline-formula-31"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-31.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-31.gif"/></noscript></span></span>. However, since dividing both the reward and coding cost by the same factor has no effect on the shape of the objective function, <em>L</em> (but only alters its magnitude), this will not effect any predictions we make using the inferred reward.</p><p id="p-81">With an external input, there is no closed-form solution for the value function. Instead, we can infer <em>v</em> (<strong><em>σ, x</em></strong>) numerically by maximising the log-likelihood, <span class="inline-formula" id="inline-formula-32"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-32.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-32.gif"/></noscript></span></span>, where <span class="inline-formula" id="inline-formula-33"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-33.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-33.gif"/></noscript></span></span> denote optimal response probabilities. Once we know <em>v</em> (<strong><em>σ, x</em></strong>) we can compute the reward from <a id="xref-disp-formula-37-3" class="xref-disp-formula" href="#disp-formula-37">Eqn 37</a>.</p></div><div id="sec-14" class="subsection"><h3>Approximate method for larger networks</h3><div id="sec-15" class="subsection"><h4>RL model</h4><p id="p-82">To scale our framework to larger networks we approximate the value function, <em>v</em> (<strong><em>σ, x</em></strong>), by a parametric function of the network activity, <strong><em>σ</em></strong> and input, <strong><em>x</em></strong>. Without loss of generality, we can parameterise the value function as a linear combination of basis functions: <em>v</em><sub><strong><em>ϕ</em></strong></sub> (<strong><em>σ, x</em></strong>) ≡ <strong><em>ϕ</em></strong><sup><em>T</em></sup> <strong><em>f</em></strong> (<strong><em>σ, x</em></strong>). From <a id="xref-disp-formula-36-1" class="xref-disp-formula" href="#disp-formula-36">Eqn 36</a>, if the network is optimal, then the value function equals:
<span class="disp-formula" id="disp-formula-41"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-47.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-47.gif"/></noscript></span>

</span>
</p><p id="p-83">In the exact algorithm, we updated the value function by setting it equal to <span class="inline-formula" id="inline-formula-34"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-34.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-34.gif"/></noscript></span></span> (<a id="xref-disp-formula-34-1" class="xref-disp-formula" href="#disp-formula-34">Eqn 34</a>). Since in the parametric case this is not possible, we can instead update <strong><em>ϕ</em></strong> to minimise:
<span class="disp-formula" id="disp-formula-42"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-48.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-48.gif"/></noscript></span>

</span>
where <span class="inline-formula" id="inline-formula-35"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-35.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-35.gif"/></noscript></span></span> is the target value function, defined as in <a id="xref-disp-formula-41-1" class="xref-disp-formula" href="#disp-formula-41">Eqn 41</a>, with parameters, <span class="inline-formula" id="inline-formula-36"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-36.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-36.gif"/></noscript></span></span>.</p><p id="p-84">We follow the procedure set out in [<a id="xref-ref-16-1" class="xref-bibr" href="#ref-16">16</a>, <a id="xref-ref-17-1" class="xref-bibr" href="#ref-17">17</a>] to transform this into a stochastic gradient descent algorithm. First, we perform <em>n</em><sub><em>batch</em></sub> samples from the current policy. Next, we perform a stochastic gradient descent update:
<span class="disp-formula" id="disp-formula-43"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-49.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-49.gif"/></noscript></span>

</span>
where <em>η</em> is a constant that determines the learning rate. Finally, after doing this <em>n</em><sub><em>epoch</em></sub> times, we update the target parameters, <span class="inline-formula" id="inline-formula-37"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-37.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-37.gif"/></noscript></span></span>. These steps are repeated until convergence.</p></div><div id="sec-16" class="subsection"><h4>Inverse RL</h4><p id="p-85">We can infer the parameters of the value function, <strong><em>ϕ</em></strong>, by maximising the log-likelihood: ⟨log <em>p</em><sub><strong><em>ϕ</em></strong></sub> (<strong><em>σ</em></strong>′|<strong><em>σ, x</em></strong>)⟩<sub>𝒟</sub>. We can choose the form of the value function to ensure that this is tractable. For example, if the value function is quadratic in the responses, then this corresponds to inferring the parameters of a pairwise Ising model [<a id="xref-ref-27-3" class="xref-bibr" href="#ref-27">27</a>, <a id="xref-ref-28-4" class="xref-bibr" href="#ref-28">28</a>].</p><p id="p-86">After inferring <strong><em>ϕ</em></strong>, we want to infer the reward function. At convergence, ∇<sub><em>ϕ</em></sub> <em>F</em> (<em>ϕ</em>) = 0 and <span class="inline-formula" id="inline-formula-38"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-38.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-38.gif"/></noscript></span></span>, so that:
<span class="disp-formula" id="disp-formula-44"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-50.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-50.gif"/></noscript></span>

</span>
<span class="disp-formula" id="disp-formula-45"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-51.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-51.gif"/></noscript></span>

</span>
where
<span class="disp-formula" id="disp-formula-46"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-52.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-52.gif"/></noscript></span>

</span>
</p><p id="p-87">In the exact case, where <span class="inline-formula" id="inline-formula-39"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-39.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-39.gif"/></noscript></span></span> (and thus, <span class="inline-formula" id="inline-formula-40"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-40.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-40.gif"/></noscript></span></span>), then the inferred reward equals <span class="inline-formula" id="inline-formula-41"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-41.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-41.gif"/></noscript></span></span>. However, this is not necessarily true when we assume an approximate value function.</p><p id="p-88">Just as we did for the value function, we can express the reward function as a linear combination of basis functions: <em>r</em> (<strong><em>σ, x</em></strong>) = <strong><em>θ</em></strong><sup><em>T</em></sup> <strong><em>g</em></strong> (<strong><em>σ, x</em></strong>). Thus, <a id="xref-disp-formula-45-1" class="xref-disp-formula" href="#disp-formula-45">Eqn 45</a> becomes:
<span class="disp-formula" id="disp-formula-47"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-53.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-53.gif"/></noscript></span>

</span>
</p><p id="p-89">If the reward function has the same number of parameters than the approximate value function (i.e. <strong><em>f</em></strong> (<strong><em>σ</em></strong>) and <strong><em>g</em></strong> (<strong><em>σ</em></strong>) have the same size), then we can solve this equation to find <strong><em>θ</em></strong>. Alternatively, if the reward function has more parameters than the value function, then we require additional assumptions to unambiguously infer the reward.</p></div></div><div id="sec-17" class="subsection"><h3>Simulation details</h3><div id="sec-18" class="subsection"><h4>Agent navigating a maze</h4><p id="p-90">We considered an agent navigating a 15 × 15 maze. The agent’s state corresponded to their position in the maze. The agent could choose to move up, down, left or right in the maze. At each time there was a 5% probability that the agent moved in a random direction, independent of their selected action. Moving in the direction of a barrier (shown in blue in <a id="xref-fig-1-14" class="xref-fig" href="#F1">Fig 1D</a>) would result in the agent remaining in the same location. After reaching the ‘rewarded’ location (bottom right of the maze), the agent was immediately transported to a starting location in the top left of the maze. We optimised the agent’s policy at both low and high coding cost (<em>λ</em> = 0.013<em>/</em>0.13 respectively) using the entropy-regularised RL algorithm described in Methods section. The reward was inferred from the agent’s policy after optimisation as described in Methods section.</p></div><div id="sec-19" class="subsection"><h4>Network with single binary input</h4><p id="p-91">We simulated a network of 8 neurons that receive a single binary input, <em>x</em>. The stimulus has a transition probability: <em>p</em> (<em>x</em>′ = 1|<em>x</em> = −1) = <em>p</em> (<em>x</em>′ = −1|<em>x</em> = 1) = 0.02. The reward function was unity when <em>x</em> = −1 and the network fired exactly 2 spikes, or when <em>x</em> = −1 and the network fired exactly 6 spikes. We set <em>λ</em> = 0.114.</p><p id="p-92">To avoid trivial solutions where a subset of neurons spike continuously while other neurons are silent, we defined the coding cost to penalise deviations from the population averaged firing rate (rather than the average firing rate for each neuron). Thus, the coding cost was defined as <span class="inline-formula" id="inline-formula-42"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-42.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-42.gif"/></noscript></span></span>, where <span class="inline-formula" id="inline-formula-43"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-43.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/inline-graphic-43.gif"/></noscript></span></span> is the average spiking probability, across all neurons.</p><p id="p-93">We inferred the reward <em>r</em> (<strong><em>σ</em></strong>) from neural responses as described in Methods section. Note that, when inferring the reward, it did not matter if we assumed that the constraint included the population averaged firing rate or the average firing rate for each neuron individually, since after optimisation all neurons had the same mean firing rate. In <a id="xref-fig-2-11" class="xref-fig" href="#F2">Fig 2E</a> we rescaled and shifted the inferred reward to have the same mean and variance as the true reward.</p><p id="p-94">We used the inferred reward to predict how neural tuning curves should adapt when we alter the stimulus statistics (<a id="xref-fig-2-12" class="xref-fig" href="#F2">Fig 2F</a>, upper) or remove a cell (<a id="xref-fig-2-13" class="xref-fig" href="#F2">Fig 2F</a>, lower). For <a id="xref-fig-2-14" class="xref-fig" href="#F2">fig 2F</a> (upper), we altered the stimulus statistics by setting <em>p</em> (<em>x</em>′ = 1|<em>x</em> = −1) = 0.01 and <em>p</em> (<em>x</em>′ = −1|<em>x</em> = 1) = 0.03. For <a id="xref-fig-2-15" class="xref-fig" href="#F2">figure 2C</a> (lower), we removed one cell from the network. In both cases, we manually adjusted <em>λ</em> to keep the average coding cost constant.</p></div><div id="sec-20" class="subsection"><h4>Efficient coding</h4><p id="p-95">We considered a stimulus consisting of <em>m</em> = 7 binary variables, <em>x</em><sub><em>i</em></sub> = −1<em>/</em>1. The stimulus had Markov dynamics, with each unit updated asynchronously. The stimulus dynamics were given by:
<span class="disp-formula" id="disp-formula-48"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-54.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-54.gif"/></noscript></span>

</span>
where <em>J</em> = 1.5 is a coupling constant. A ‘relevant’ variable, <em>y</em> (<strong><em>x</em></strong>) was equal to 1 if 4 or more inputs equalled 1, and equal to −1 otherwise.</p><p id="p-96">We optimised a network of <em>n</em> = 7 neurons to efficiently code the relevant variable <em>y</em> (<strong><em>x</em></strong>), using the algorithm described in the main text. For <a id="xref-fig-4-14" class="xref-fig" href="#F4">Fig 4B-C</a> we set <em>λ</em> = 0.167. For <a id="xref-fig-4-15" class="xref-fig" href="#F4">Fig 4D</a> we varied <em>λ</em> between 0.1 and 0.5. For <a id="xref-fig-4-16" class="xref-fig" href="#F4">Fig 4E</a> we altered the stimulus statistics so that,
<span class="disp-formula" id="disp-formula-49"><span class="highwire-responsive-lazyload"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-55.gif"/><noscript><img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/02/06/598086/embed/graphic-55.gif"/></noscript></span>

</span>
where <em>J</em><sub>0</sub> was a bias term that we varied between 0 and 0.4. For each value of <em>J</em><sub>0</sub> we adjust <em>λ</em> so as to keep the average coding cost constant.</p></div><div id="sec-21" class="subsection"><h4>Pairwise coupled network</h4><p id="p-97">We considered a network of 12 neurons arranged in a ring. We defined a reward function that was equal to 1 if exactly 4 adjacent neurons were active, and 0 otherwise. We defined the coding cost as described in the previous section, to penalise deviations from the population averaged mean firing rate.</p><p id="p-98">We approximated the value function by a quadratic function, <em>v</em> (<strong><em>σ</em></strong>) = ∑ <sub><em>i,j</em>≠<em>i</em></sub><em>J</em><sub><em>ij</em></sub><em>σ</em><sub><em>i</em></sub><em>σ</em><sub><em>j</em></sub> + ∑ <sub><em>i</em></sub><em>h</em><sub><em>i</em></sub><em>σ</em><sub><em>i</em></sub>. We optimised the parameters of this value function using the algorithm described in Methods section, with <em>λ</em> = 0.05. We used batches of <em>n</em><sub><em>batch</em></sub> = 40 samples, and updated the target parameters after every <em>n</em><sub><em>epoch</em></sub> = 100 batches.</p><p id="p-99">We inferred the reward function from the inferred network couplings, <strong><em>J</em></strong> and <strong><em>h</em></strong>. As described in Methods section, this problem is only well-posed if we assume a low-d parametric form for the reward function, or add additional assumptions. We therefore considered several different sets of assumptions. For our initial ‘sparse model’, we set up a linear programming problem in which we minimised <em>l</em><sub>1</sub> = ∑<sub><strong><em>σ</em></strong></sub> <em>r</em> (<strong><em>σ</em></strong>), under the constraint that the reward was always greater than 0 while satisfying the optimality criterion given by <a id="xref-disp-formula-47-1" class="xref-disp-formula" href="#disp-formula-47">Eqn 47</a>. For the ‘pairwise model’ we assumed that <em>r</em> = ∑<sub><em>i,j</em></sub><em>W</em><sub><em>ij</em></sub><em>σ</em><sub><em>i</em></sub><em>σ</em><sub><em>j</em></sub>. We fitted the parameters, <em>W</em><sub><em>ij</em></sub>, so as to minimise the squared difference between the left and right hand side of Eqn <a id="xref-disp-formula-47-2" class="xref-disp-formula" href="#disp-formula-47">Eqn 47</a>. Finally, for the ‘global model’ we assumed that <em>r</em> =∑<sub><em>j</em></sub> <em>δ</em><sub><em>j,m</em></sub><em>W</em><sub><em>j</em></sub>, where <em>m</em> is the total number of active neurons and <em>δ</em><sub><em>ij</em></sub> is the kronecker-delta. Parameters, <em>W</em><sub><em>j</em></sub> were fitted to the data as for the pairwise model.</p><p id="p-100">Finally, for the simulations shown in <a id="xref-fig-5-7" class="xref-fig" href="#F5">Figure 5</a>, panels B-D, we ran the optimisation with <em>λ</em> = 0.1, 0.01 and 0.1, respectively. For panel 3C we removed connections between neurons separated by a distance of 3 or more on the ring. For panel 3D we forced two of the neurons to be continuously active.</p></div></div></div><div class="section ack" id="ack-1"><h2 class="">Acknowledgements</h2><p id="p-101">This work was supported by ANR JCJC grant (ANR-17-CE37-0013) to M.C, ANR Trajectory (ANR-15-CE37-0011), ANR DECORE (ANR-18-CE37-0011), the French State program Investissements d’Avenir managed by the Agence Nationale de la Recherche (LIFESENSES; ANR-10-LABX-65), EC Grant No. H2020-785907 from the Human Brain Project (SGA2), and an AVIESAN-UNADEV grant to O.M. The authors would like to thank Ulisse Ferrari for useful discussions and feedback.</p></div><div class="section fn-group" id="fn-group-1"><h2>Footnotes</h2><ul><li class="fn-update fn-group-summary-of-updates" id="fn-2"><p id="p-4">We have changed the title. We have added two additional figures (fig 2 and fig 6). We have made changes to the text to highlight the limitations/scope of our approach.</p></li></ul></div><div class="section ref-list" id="ref-list-1"><h2 class="">References</h2><ol class="cit-list ref-use-labels"><li><span class="ref-label">1.</span><a class="rev-xref-ref" href="#xref-ref-1-1" title="View reference 1. in text" id="ref-1">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.1"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Yang</span>  <span class="cit-name-given-names">G R</span></span>, <span class="cit-auth"><span class="cit-name-surname">Joglekar</span>  <span class="cit-name-given-names">MR</span></span>, <span class="cit-auth"><span class="cit-name-surname">Song</span>  <span class="cit-name-given-names">HF</span></span>, <span class="cit-auth"><span class="cit-name-surname">Newsome</span>  <span class="cit-name-given-names">WT</span></span>, <span class="cit-auth"><span class="cit-name-surname">Wang</span>  <span class="cit-name-given-names">XJ</span></span>. (<span class="cit-pub-date">2019</span>). <span class="cit-article-title">Task representations in neural networks trained to perform many cognitive tasks</span>. <abbr class="cit-jnl-abbrev">Nat Neurosci</abbr>, <span class="cit-vol">22</span>:<span class="cit-fpage">297</span>–<span class="cit-lpage">306s</span></cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DNat%2BNeurosci%26rft.volume%253D22%26rft.spage%253D297%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a></div></div></li><li><span class="ref-label">2.</span><div class="cit ref-cit ref-journal no-rev-xref" id="cit-598086v3.2" data-doi="10.1073/pnas.1619788114"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Heeger</span>  <span class="cit-name-given-names">D J</span></span> (<span class="cit-pub-date">2017</span>) <span class="cit-article-title">Theory of cortical function</span>. <abbr class="cit-jnl-abbrev">Proc Natl Acad Sci USA</abbr> <span class="cit-vol">114</span>:<span class="cit-fpage">1773</span>–<span class="cit-lpage">1782</span></cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DProc%2BNatl%2BAcad%2BSci%2BUSA%26rft_id%253Dinfo%253Adoi%252F10.1073%252Fpnas.1619788114%26rft_id%253Dinfo%253Apmid%252F28167793%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a><a href="/lookup/ijlink/YTozOntzOjQ6InBhdGgiO3M6MTQ6Ii9sb29rdXAvaWpsaW5rIjtzOjU6InF1ZXJ5IjthOjQ6e3M6ODoibGlua1R5cGUiO3M6NDoiQUJTVCI7czoxMToiam91cm5hbENvZGUiO3M6NDoicG5hcyI7czo1OiJyZXNpZCI7czoxMDoiMTE0LzgvMTc3MyI7czo0OiJhdG9tIjtzOjM3OiIvYmlvcnhpdi9lYXJseS8yMDIwLzAyLzA2LzU5ODA4Ni5hdG9tIjt9czo4OiJmcmFnbWVudCI7czowOiIiO30=" class="cit-ref-sprinkles cit-ref-sprinkles-ijlink"><span><span class="cit-reflinks-abstract">Abstract</span><span class="cit-sep cit-reflinks-variant-name-sep">/</span><span class="cit-reflinks-full-text"><span class="free-full-text">FREE </span>Full Text</span></span></a></div></div></li><li><span class="ref-label">3.</span><a class="rev-xref-ref" href="#xref-ref-3-1" title="View reference 3. in text" id="ref-3">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.3" data-doi="10.1016/j.neuron.2009.07.018"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Sussillo</span>  <span class="cit-name-given-names">D</span></span>, <span class="cit-auth"><span class="cit-name-surname">Abbott</span>  <span class="cit-name-given-names">L F</span></span> (<span class="cit-pub-date">2009</span>) <span class="cit-article-title">Generating coherent patterns of activity from chaotic neural networks</span>. <abbr class="cit-jnl-abbrev">Neuron</abbr> <span class="cit-vol">63</span>:<span class="cit-fpage">544</span>–<span class="cit-lpage">557</span>.</cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DNeuron%26rft.volume%253D63%26rft.spage%253D544%26rft_id%253Dinfo%253Adoi%252F10.1016%252Fj.neuron.2009.07.018%26rft_id%253Dinfo%253Apmid%252F19709635%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a><a href="/lookup/external-ref?access_num=10.1016/j.neuron.2009.07.018&amp;link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref"><span>CrossRef</span></a><a href="/lookup/external-ref?access_num=19709635&amp;link_type=MED&amp;atom=%2Fbiorxiv%2Fearly%2F2020%2F02%2F06%2F598086.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline"><span>PubMed</span></a><a href="/lookup/external-ref?access_num=000269570400013&amp;link_type=ISI" class="cit-ref-sprinkles cit-ref-sprinkles-newisilink cit-ref-sprinkles-webofscience"><span>Web of Science</span></a></div></div></li><li><span class="ref-label">4.</span><a class="rev-xref-ref" href="#xref-ref-4-1" title="View reference 4. in text" id="ref-4">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.4" data-doi="10.1126/science.aab4113"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Gütig</span>  <span class="cit-name-given-names">R</span></span> (<span class="cit-pub-date">2016</span>) <span class="cit-article-title">Spiking neurons can discover predictive features by aggregate–label learning</span>. <abbr class="cit-jnl-abbrev">Science</abbr> <span class="cit-vol">351</span>(<span class="cit-issue">6277</span>):<span class="cit-fpage">aab4113</span></cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DScience%26rft.stitle%253DScience%26rft.aulast%253DGutig%26rft.auinit1%253DR.%26rft.volume%253D351%26rft.issue%253D6277%26rft.spage%253Daab4113%26rft.epage%253Daab4113%26rft.atitle%253DSpiking%2Bneurons%2Bcan%2Bdiscover%2Bpredictive%2Bfeatures%2Bby%2Baggregate-label%2Blearning%26rft_id%253Dinfo%253Adoi%252F10.1126%252Fscience.aab4113%26rft_id%253Dinfo%253Apmid%252F26941324%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a><a href="/lookup/ijlink/YTozOntzOjQ6InBhdGgiO3M6MTQ6Ii9sb29rdXAvaWpsaW5rIjtzOjU6InF1ZXJ5IjthOjQ6e3M6ODoibGlua1R5cGUiO3M6NDoiQUJTVCI7czoxMToiam91cm5hbENvZGUiO3M6Mzoic2NpIjtzOjU6InJlc2lkIjtzOjE2OiIzNTEvNjI3Ny9hYWI0MTEzIjtzOjQ6ImF0b20iO3M6Mzc6Ii9iaW9yeGl2L2Vhcmx5LzIwMjAvMDIvMDYvNTk4MDg2LmF0b20iO31zOjg6ImZyYWdtZW50IjtzOjA6IiI7fQ==" class="cit-ref-sprinkles cit-ref-sprinkles-ijlink"><span><span class="cit-reflinks-abstract">Abstract</span><span class="cit-sep cit-reflinks-variant-name-sep">/</span><span class="cit-reflinks-full-text"><span class="free-full-text">FREE </span>Full Text</span></span></a></div></div></li><li><span class="ref-label">5.</span><a class="rev-xref-ref" href="#xref-ref-5-1" title="View reference 5. in text" id="ref-5">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.5" data-doi="10.1073/pnas.79.8.2554"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Hopfield</span>  <span class="cit-name-given-names">JJ</span></span> (<span class="cit-pub-date">1982</span>) <span class="cit-article-title">Neural networks and physical systems with emergent collective computational abilities</span> <abbr class="cit-jnl-abbrev">Proc Natl Acad Sci USA</abbr> <span class="cit-vol">79</span>:<span class="cit-fpage">2554</span>–<span class="cit-lpage">2558</span></cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DPNAS%26rft.stitle%253DProc.%2BNatl.%2BAcad.%2BSci.%2BUSA%26rft.aulast%253DHopfield%26rft.auinit1%253DJ.%2BJ.%26rft.volume%253D79%26rft.issue%253D8%26rft.spage%253D2554%26rft.epage%253D2558%26rft.atitle%253DNeural%2Bnetworks%2Band%2Bphysical%2Bsystems%2Bwith%2Bemergent%2Bcollective%2Bcomputational%2Babilities.%26rft_id%253Dinfo%253Adoi%252F10.1073%252Fpnas.79.8.2554%26rft_id%253Dinfo%253Apmid%252F6953413%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a><a href="/lookup/ijlink/YTozOntzOjQ6InBhdGgiO3M6MTQ6Ii9sb29rdXAvaWpsaW5rIjtzOjU6InF1ZXJ5IjthOjQ6e3M6ODoibGlua1R5cGUiO3M6NDoiQUJTVCI7czoxMToiam91cm5hbENvZGUiO3M6NDoicG5hcyI7czo1OiJyZXNpZCI7czo5OiI3OS84LzI1NTQiO3M6NDoiYXRvbSI7czozNzoiL2Jpb3J4aXYvZWFybHkvMjAyMC8wMi8wNi81OTgwODYuYXRvbSI7fXM6ODoiZnJhZ21lbnQiO3M6MDoiIjt9" class="cit-ref-sprinkles cit-ref-sprinkles-ijlink"><span><span class="cit-reflinks-abstract">Abstract</span><span class="cit-sep cit-reflinks-variant-name-sep">/</span><span class="cit-reflinks-full-text"><span class="free-full-text">FREE </span>Full Text</span></span></a></div></div></li><li><span class="ref-label">6.</span><a class="rev-xref-ref" href="#xref-ref-6-1" title="View reference 6. in text" id="ref-6">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.6" data-doi="10.1126/science.1142998"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Körding</span>  <span class="cit-name-given-names">K</span></span> (<span class="cit-pub-date">2007</span>) <span class="cit-article-title">Decision theory: what should the nervous system do?</span> <abbr class="cit-jnl-abbrev">Science</abbr> <span class="cit-vol">318</span>:<span class="cit-fpage">606</span>–<span class="cit-lpage">610</span></cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DScience%26rft.stitle%253DScience%26rft.aulast%253DKo%2526%2523x0308%253Brding%26rft.auinit1%253DK.%26rft.volume%253D318%26rft.issue%253D5850%26rft.spage%253D606%26rft.epage%253D610%26rft.atitle%253DDecision%2BTheory%253A%2BWhat%2B%2522Should%2522%2Bthe%2BNervous%2BSystem%2BDo%253F%26rft_id%253Dinfo%253Adoi%252F10.1126%252Fscience.1142998%26rft_id%253Dinfo%253Apmid%252F17962554%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a><a href="/lookup/ijlink/YTozOntzOjQ6InBhdGgiO3M6MTQ6Ii9sb29rdXAvaWpsaW5rIjtzOjU6InF1ZXJ5IjthOjQ6e3M6ODoibGlua1R5cGUiO3M6NDoiQUJTVCI7czoxMToiam91cm5hbENvZGUiO3M6Mzoic2NpIjtzOjU6InJlc2lkIjtzOjEyOiIzMTgvNTg1MC82MDYiO3M6NDoiYXRvbSI7czozNzoiL2Jpb3J4aXYvZWFybHkvMjAyMC8wMi8wNi81OTgwODYuYXRvbSI7fXM6ODoiZnJhZ21lbnQiO3M6MDoiIjt9" class="cit-ref-sprinkles cit-ref-sprinkles-ijlink"><span><span class="cit-reflinks-abstract">Abstract</span><span class="cit-sep cit-reflinks-variant-name-sep">/</span><span class="cit-reflinks-full-text"><span class="free-full-text">FREE </span>Full Text</span></span></a></div></div></li><li><span class="ref-label">7.</span><a class="rev-xref-ref" href="#xref-ref-7-1" title="View reference 7. in text" id="ref-7">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.7"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Boerlin</span>  <span class="cit-name-given-names">M</span></span>, <span class="cit-auth"><span class="cit-name-surname">Machens</span>  <span class="cit-name-given-names">CK</span></span>, <span class="cit-auth"><span class="cit-name-surname">Denève</span>  <span class="cit-name-given-names">S</span></span> (<span class="cit-pub-date">2013</span>) <span class="cit-article-title">Predictive coding of dynamical variables in balanced spiking networks</span>. <abbr class="cit-jnl-abbrev">PLoS Comp Bio</abbr> <span class="cit-vol">9</span> <span class="cit-fpage">e1003258</span>.</cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DPLoS%2BComp%2BBio%26rft.volume%253D9%26rft.spage%253D1003258e%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a></div></div></li><li><span class="ref-label">8.</span><a class="rev-xref-ref" href="#xref-ref-8-1" title="View reference 8. in text" id="ref-8">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.8" data-doi="10.1146/annurev.neuro.24.1.1193"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Simoncelli</span>  <span class="cit-name-given-names">EP</span></span>, <span class="cit-auth"><span class="cit-name-surname">Olshausen</span>  <span class="cit-name-given-names">BA</span></span> (<span class="cit-pub-date">2001</span>) <span class="cit-article-title">Natural image statistics and neural representation</span>. <abbr class="cit-jnl-abbrev">Ann Rev Neurosci</abbr> <span class="cit-vol">24</span>:<span class="cit-fpage">1193</span>–<span class="cit-lpage">1216</span></cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DAnnual%2Breview%2Bof%2Bneuroscience%26rft.stitle%253DAnnu%2BRev%2BNeurosci%26rft.aulast%253DSimoncelli%26rft.auinit1%253DE.%2BP%26rft.volume%253D24%26rft.issue%253D1%26rft.spage%253D1193%26rft.epage%253D1216%26rft.atitle%253DNATURAL%2BIMAGE%2BSTATISTICS%2BAND%2BNEURAL%2BREPRESENTATION%26rft_id%253Dinfo%253Adoi%252F10.1146%252Fannurev.neuro.24.1.1193%26rft_id%253Dinfo%253Apmid%252F11520932%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a><a href="/lookup/external-ref?access_num=10.1146/annurev.neuro.24.1.1193&amp;link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref"><span>CrossRef</span></a><a href="/lookup/external-ref?access_num=11520932&amp;link_type=MED&amp;atom=%2Fbiorxiv%2Fearly%2F2020%2F02%2F06%2F598086.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline"><span>PubMed</span></a><a href="/lookup/external-ref?access_num=000170109100039&amp;link_type=ISI" class="cit-ref-sprinkles cit-ref-sprinkles-newisilink cit-ref-sprinkles-webofscience"><span>Web of Science</span></a></div></div></li><li><span class="ref-label">9.</span><a class="rev-xref-ref" href="#xref-ref-9-1" title="View reference 9. in text" id="ref-9">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.9" data-doi="10.1073/pnas.1004906107"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Tkačik</span>  <span class="cit-name-given-names">G</span></span>, <span class="cit-auth"><span class="cit-name-surname">Prentice</span>  <span class="cit-name-given-names">JS</span></span>, <span class="cit-auth"><span class="cit-name-surname">Balasubramanian</span>  <span class="cit-name-given-names">V</span></span>, <span class="cit-auth"><span class="cit-name-surname">Schneidman</span>  <span class="cit-name-given-names">E</span></span> (<span class="cit-pub-date">2010</span>) <span class="cit-article-title">Optimal population coding by noisy spiking neurons</span>. <abbr class="cit-jnl-abbrev">Proc Natl Acad Sci USA</abbr> <span class="cit-vol">107</span>:<span class="cit-fpage">14419</span>–<span class="cit-lpage">14424</span>.</cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DProc%2BNatl%2BAcad%2BSci%2BUSA%26rft_id%253Dinfo%253Adoi%252F10.1073%252Fpnas.1004906107%26rft_id%253Dinfo%253Apmid%252F20660781%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a><a href="/lookup/ijlink/YTozOntzOjQ6InBhdGgiO3M6MTQ6Ii9sb29rdXAvaWpsaW5rIjtzOjU6InF1ZXJ5IjthOjQ6e3M6ODoibGlua1R5cGUiO3M6NDoiQUJTVCI7czoxMToiam91cm5hbENvZGUiO3M6NDoicG5hcyI7czo1OiJyZXNpZCI7czoxMjoiMTA3LzMyLzE0NDE5IjtzOjQ6ImF0b20iO3M6Mzc6Ii9iaW9yeGl2L2Vhcmx5LzIwMjAvMDIvMDYvNTk4MDg2LmF0b20iO31zOjg6ImZyYWdtZW50IjtzOjA6IiI7fQ==" class="cit-ref-sprinkles cit-ref-sprinkles-ijlink"><span><span class="cit-reflinks-abstract">Abstract</span><span class="cit-sep cit-reflinks-variant-name-sep">/</span><span class="cit-reflinks-full-text"><span class="free-full-text">FREE </span>Full Text</span></span></a></div></div></li><li><span class="ref-label">10.</span><a class="rev-xref-ref" href="#xref-ref-10-1" title="View reference 10. in text" id="ref-10">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.10" data-doi="10.1073/pnas.1711114115"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Chalk</span>  <span class="cit-name-given-names">M</span></span>, <span class="cit-auth"><span class="cit-name-surname">Marre</span>  <span class="cit-name-given-names">O</span></span>, <span class="cit-auth"><span class="cit-name-surname">Tkačik</span>  <span class="cit-name-given-names">G</span></span> (<span class="cit-pub-date">2018</span>) <span class="cit-article-title">Toward a unified theory of efficient, predictive, and sparse coding</span>. <abbr class="cit-jnl-abbrev">Proc Natl Acad Sci USA</abbr> <span class="cit-vol">115</span>:<span class="cit-fpage">186</span>–<span class="cit-lpage">191</span>.</cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DProc%2BNatl%2BAcad%2BSci%2BUSA%26rft_id%253Dinfo%253Adoi%252F10.1073%252Fpnas.1711114115%26rft_id%253Dinfo%253Apmid%252F29259111%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a><a href="/lookup/ijlink/YTozOntzOjQ6InBhdGgiO3M6MTQ6Ii9sb29rdXAvaWpsaW5rIjtzOjU6InF1ZXJ5IjthOjQ6e3M6ODoibGlua1R5cGUiO3M6NDoiQUJTVCI7czoxMToiam91cm5hbENvZGUiO3M6NDoicG5hcyI7czo1OiJyZXNpZCI7czo5OiIxMTUvMS8xODYiO3M6NDoiYXRvbSI7czozNzoiL2Jpb3J4aXYvZWFybHkvMjAyMC8wMi8wNi81OTgwODYuYXRvbSI7fXM6ODoiZnJhZ21lbnQiO3M6MDoiIjt9" class="cit-ref-sprinkles cit-ref-sprinkles-ijlink"><span><span class="cit-reflinks-abstract">Abstract</span><span class="cit-sep cit-reflinks-variant-name-sep">/</span><span class="cit-reflinks-full-text"><span class="free-full-text">FREE </span>Full Text</span></span></a></div></div></li><li><span class="ref-label">11.</span><div class="cit ref-cit ref-book no-rev-xref" id="cit-598086v3.11"><div class="cit-metadata"><ol class="duplicate"><li><span class="cit-ed"><span class="cit-name-surname">Rosenblith</span>  <span class="cit-name-given-names">WA</span></span></li></ol><cite><span class="cit-auth"><span class="cit-name-surname">Barlow</span>,  <span class="cit-name-given-names">HB</span></span> (<span class="cit-pub-date">1961</span>) <span class="cit-chapter-title">Possible principles underlying the transformations of sensory messages</span>. <span class="cit-source">Sensory Communication</span>, ed <span class="cit-ed"><span class="cit-name-surname">Rosenblith</span>  <span class="cit-name-given-names">WA</span></span> (<span class="cit-publ-name">MIT Press, Cambridge</span>, <span class="cit-publ-loc">MA</span>), pp <span class="cit-fpage">217</span>–<span class="cit-lpage">234</span></cite></div><div class="cit-extra"></div></div></li><li><span class="ref-label">12.</span><div class="cit ref-cit ref-journal no-rev-xref" id="cit-598086v3.12" data-doi="10.1162/neco.1994.6.4.559"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Field</span>  <span class="cit-name-given-names">DJ</span></span> (<span class="cit-pub-date">1994</span>) <span class="cit-article-title">What is the goal of sensory coding?</span> <abbr class="cit-jnl-abbrev">Neural Comput</abbr> <span class="cit-vol">6</span>:<span class="cit-fpage">559</span>–<span class="cit-lpage">601</span>.</cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DNeural%2BComput%26rft.volume%253D6%26rft.spage%253D559%26rft_id%253Dinfo%253Adoi%252F10.1162%252Fneco.1994.6.4.559%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a><a href="/lookup/external-ref?access_num=10.1162/neco.1994.6.4.559&amp;link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref"><span>CrossRef</span></a><a href="/lookup/external-ref?access_num=A1994PF91000001&amp;link_type=ISI" class="cit-ref-sprinkles cit-ref-sprinkles-newisilink cit-ref-sprinkles-webofscience"><span>Web of Science</span></a></div></div></li><li><span class="ref-label">13.</span><a class="rev-xref-ref" href="#xref-ref-13-1" title="View reference 13. in text" id="ref-13">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.13" data-doi="10.1523/JNEUROSCI.1032-14.2014"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Gjorgjieva</span>  <span class="cit-name-given-names">J</span></span>, <span class="cit-auth"><span class="cit-name-surname">Sompolinsky</span>  <span class="cit-name-given-names">H</span></span>, <span class="cit-auth"><span class="cit-name-surname">Meister</span>  <span class="cit-name-given-names">M</span></span> (<span class="cit-pub-date">2014</span>) <span class="cit-article-title">Benefits of pathway splitting in sensory coding</span>. <abbr class="cit-jnl-abbrev">J Neurosci</abbr> <span class="cit-vol">34</span>:<span class="cit-fpage">12127</span>–<span class="cit-lpage">12144</span>.</cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DJournal%2Bof%2BNeuroscience%26rft.stitle%253DJ.%2BNeurosci.%26rft.aulast%253DGjorgjieva%26rft.auinit1%253DJ.%26rft.volume%253D34%26rft.issue%253D36%26rft.spage%253D12127%26rft.epage%253D12144%26rft.atitle%253DBenefits%2Bof%2BPathway%2BSplitting%2Bin%2BSensory%2BCoding%26rft_id%253Dinfo%253Adoi%252F10.1523%252FJNEUROSCI.1032-14.2014%26rft_id%253Dinfo%253Apmid%252F25186757%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a><a href="/lookup/ijlink/YTozOntzOjQ6InBhdGgiO3M6MTQ6Ii9sb29rdXAvaWpsaW5rIjtzOjU6InF1ZXJ5IjthOjQ6e3M6ODoibGlua1R5cGUiO3M6NDoiQUJTVCI7czoxMToiam91cm5hbENvZGUiO3M6Njoiam5ldXJvIjtzOjU6InJlc2lkIjtzOjExOiIzNC8zNi8xMjEyNyI7czo0OiJhdG9tIjtzOjM3OiIvYmlvcnhpdi9lYXJseS8yMDIwLzAyLzA2LzU5ODA4Ni5hdG9tIjt9czo4OiJmcmFnbWVudCI7czowOiIiO30=" class="cit-ref-sprinkles cit-ref-sprinkles-ijlink"><span><span class="cit-reflinks-abstract">Abstract</span><span class="cit-sep cit-reflinks-variant-name-sep">/</span><span class="cit-reflinks-full-text"><span class="free-full-text">FREE </span>Full Text</span></span></a></div></div></li><li><span class="ref-label">14.</span><a class="rev-xref-ref" href="#xref-ref-14-1" title="View reference 14. in text" id="ref-14">↵</a><div class="cit ref-cit ref-book" id="cit-598086v3.14"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Sutton</span>  <span class="cit-name-given-names">RS</span></span>, <span class="cit-auth"><span class="cit-name-surname">Barto</span>  <span class="cit-name-given-names">AG</span></span> (<span class="cit-pub-date">2018</span>) <span class="cit-source">Reinforcement learning: An introduction</span>. <span class="cit-publ-name">MIT press</span>.</cite></div><div class="cit-extra"></div></div></li><li><span class="ref-label">15.</span><a class="rev-xref-ref" href="#xref-ref-15-1" title="View reference 15. in text" id="ref-15">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.15"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Todorov</span>  <span class="cit-name-given-names">E</span></span> (<span class="cit-pub-date">2008</span>) <span class="cit-article-title">General duality between optimal control and estimation</span>. <abbr class="cit-jnl-abbrev">Proc of the 47th IEEE Conference on Decision and Control</abbr> <span class="cit-fpage">4286</span>–<span class="cit-lpage">4292</span></cite></div><div class="cit-extra"></div></div></li><li><span class="ref-label">16.</span><a class="rev-xref-ref" href="#xref-ref-16-1" title="View reference 16. in text" id="ref-16">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.16"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Schulman</span>  <span class="cit-name-given-names">J</span></span>, <span class="cit-auth"><span class="cit-name-surname">Chen</span>  <span class="cit-name-given-names">X</span></span>, <span class="cit-auth"><span class="cit-name-surname">Abbeel</span>  <span class="cit-name-given-names">P</span></span> (<span class="cit-pub-date">2017</span>) <abbr class="cit-jnl-abbrev">Equivalence between policy gradients and soft Q-learning</abbr>.<span class="cit-pub-id-sep cit-pub-id-arxiv-sep"> </span><span class="cit-pub-id-scheme">arXiv:</span><span class="cit-pub-id cit-pub-id-arxiv">1704.06440</span></cite></div><div class="cit-extra"></div></div></li><li><span class="ref-label">17.</span><a class="rev-xref-ref" href="#xref-ref-17-1" title="View reference 17. in text" id="ref-17">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.17"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Haarnoja</span>  <span class="cit-name-given-names">T</span></span>, <span class="cit-auth"><span class="cit-name-surname">Tang</span>  <span class="cit-name-given-names">H</span></span>, <span class="cit-auth"><span class="cit-name-surname">Abbeel</span>  <span class="cit-name-given-names">P</span></span>, <span class="cit-auth"><span class="cit-name-surname">Levine</span>  <span class="cit-name-given-names">S</span></span> (<span class="cit-pub-date">2017</span>). <span class="cit-article-title">Reinforcement learning with deep energy-based policies</span>. <abbr class="cit-jnl-abbrev">Proc 34th International Conf on Machine Learning</abbr> <span class="cit-vol">70</span>:<span class="cit-fpage">1352</span>–<span class="cit-lpage">1361</span></cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DProc%2B34th%2BInternational%2BConf%2Bon%2BMachine%2BLearning%26rft.volume%253D70%26rft.spage%253D1352%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a></div></div></li><li><span class="ref-label">18.</span><a class="rev-xref-ref" href="#xref-ref-18-1" title="View reference 18. in text" id="ref-18">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.18"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Tiomkin</span>  <span class="cit-name-given-names">S</span></span>, <span class="cit-auth"><span class="cit-name-surname">Tishby</span>  <span class="cit-name-given-names">N</span></span> (<span class="cit-pub-date">2017</span>). <abbr class="cit-jnl-abbrev">A Unified Bellman Equation for Causal Information and Value in Markov Decision Processes</abbr>.<span class="cit-pub-id-sep cit-pub-id-arxiv-sep"> </span><span class="cit-pub-id-scheme">arXiv:</span><span class="cit-pub-id cit-pub-id-arxiv">1703.01585</span>.</cite></div><div class="cit-extra"></div></div></li><li><span class="ref-label">19.</span><a class="rev-xref-ref" href="#xref-ref-19-1" title="View reference 19. in text" id="ref-19">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.19"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Mahadevan</span>  <span class="cit-name-given-names">S.</span></span> (<span class="cit-pub-date">1996</span>). <span class="cit-article-title">Average reward reinforcement learning: Foundations, algorithms, and empirical results</span>. <abbr class="cit-jnl-abbrev">Machine learning</abbr> <span class="cit-vol">22</span>:<span class="cit-fpage">159</span>–<span class="cit-lpage">195</span>.</cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DMachine%2Blearning%26rft.volume%253D22%26rft.spage%253D159%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a></div></div></li><li><span class="ref-label">20.</span><a class="rev-xref-ref" href="#xref-ref-20-1" title="View reference 20. in text" id="ref-20">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.20"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Ng</span>  <span class="cit-name-given-names">AY</span></span>, <span class="cit-auth"><span class="cit-name-surname">Russell</span>  <span class="cit-name-given-names">SJ</span></span> (<span class="cit-pub-date">2000</span>) <span class="cit-article-title">Algorithms for inverse reinforcement learning</span>. <abbr class="cit-jnl-abbrev">Proc of the 17th International Con on Machine Learning</abbr> pp. <span class="cit-fpage">663</span>–<span class="cit-lpage">670</span></cite></div><div class="cit-extra"></div></div></li><li><span class="ref-label">21.</span><div class="cit ref-cit ref-confproc no-rev-xref" id="cit-598086v3.21"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Rothkopf</span>  <span class="cit-name-given-names">CA</span></span>, <span class="cit-auth"><span class="cit-name-surname">Dimitrakakis</span>  <span class="cit-name-given-names">C</span></span> (<span class="cit-pub-date">2011</span>) <span class="cit-article-title">Preference elicitation and inverse reinforcement learning</span>. <span class="cit-conf-name">In Joint European conference on machine learning and knowledge discovery in databases</span> <span class="cit-publ-name">Springer</span> pp. <span class="cit-fpage">34</span>–<span class="cit-lpage">48</span>.</cite></div><div class="cit-extra"></div></div></li><li><span class="ref-label">22.</span><div class="cit ref-cit ref-journal no-rev-xref" id="cit-598086v3.22"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Herman</span>  <span class="cit-name-given-names">M</span></span>, <span class="cit-auth"><span class="cit-name-surname">Gindele</span>  <span class="cit-name-given-names">T</span></span>, <span class="cit-auth"><span class="cit-name-surname">Wagner</span>  <span class="cit-name-given-names">J</span></span>, <span class="cit-auth"><span class="cit-name-surname">Schmitt</span>  <span class="cit-name-given-names">F</span></span>, <span class="cit-auth"><span class="cit-name-surname">Burgard</span>  <span class="cit-name-given-names">W</span></span> (<span class="cit-pub-date">2016</span>) <span class="cit-article-title">Inverse reinforcement learning with simultaneous estimation of rewards and dynamics</span>. <abbr class="cit-jnl-abbrev">Artificial Intelligence and Statistics</abbr> <span class="cit-fpage">102</span>–<span class="cit-lpage">110</span></cite></div><div class="cit-extra"></div></div></li><li><span class="ref-label">23.</span><div class="cit ref-cit ref-journal no-rev-xref" id="cit-598086v3.23"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Wu</span>  <span class="cit-name-given-names">Z</span></span>, <span class="cit-auth"><span class="cit-name-surname">Schrater</span>  <span class="cit-name-given-names">P</span></span>, <span class="cit-auth"><span class="cit-name-surname">Pitkow</span>  <span class="cit-name-given-names">X</span></span> (<span class="cit-pub-date">2018</span>) <abbr class="cit-jnl-abbrev">Inverse POMDP: Inferring What You Think from What You Do</abbr>.<span class="cit-pub-id-sep cit-pub-id-arxiv-sep"> </span><span class="cit-pub-id-scheme">arXiv:</span><span class="cit-pub-id cit-pub-id-arxiv">1805.09864</span>.</cite></div><div class="cit-extra"></div></div></li><li><span class="ref-label">24.</span><a class="rev-xref-ref" href="#xref-ref-24-1" title="View reference 24. in text" id="ref-24">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.24"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Reddy</span>  <span class="cit-name-given-names">S</span></span>, <span class="cit-auth"><span class="cit-name-surname">Dragan</span>  <span class="cit-name-given-names">AD</span></span>, <span class="cit-auth"><span class="cit-name-surname">Levine</span>  <span class="cit-name-given-names">S</span></span> (<span class="cit-pub-date">2018</span>) <abbr class="cit-jnl-abbrev">Where Do You Think You’re Going?: Inferring Beliefs about Dynamics from Behavior</abbr>.<span class="cit-pub-id-sep cit-pub-id-arxiv-sep"> </span><span class="cit-pub-id-scheme">arXiv:</span><span class="cit-pub-id cit-pub-id-arxiv">1805.08010</span>.</cite></div><div class="cit-extra"></div></div></li><li><span class="ref-label">25.</span><a class="rev-xref-ref" href="#xref-ref-25-1" title="View reference 25. in text" id="ref-25">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.25"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Berger</span>  <span class="cit-name-given-names">T.</span></span> <span class="cit-article-title">Rate Distortion Theory</span>. (<span class="cit-pub-date">1971</span>) <abbr class="cit-jnl-abbrev">Englewood Clis</abbr>.</cite></div><div class="cit-extra"></div></div></li><li><span class="ref-label">26.</span><a class="rev-xref-ref" href="#xref-ref-26-1" title="View reference 26. in text" id="ref-26">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.26"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Bialek</span>  <span class="cit-name-given-names">W</span></span>, <span class="cit-auth"><span class="cit-name-surname">van Steveninck</span>  <span class="cit-name-given-names">RRDR</span></span>, <span class="cit-auth"><span class="cit-name-surname">Tishby</span>  <span class="cit-name-given-names">N</span></span> (<span class="cit-pub-date">2006</span>) <span class="cit-article-title">Efficient representation as a design principle for neural coding and computation</span>. <abbr class="cit-jnl-abbrev">IEEE international symposium on information theory</abbr> <span class="cit-fpage">659</span>–<span class="cit-lpage">663</span></cite></div><div class="cit-extra"></div></div></li><li><span class="ref-label">27.</span><a class="rev-xref-ref" href="#xref-ref-27-1" title="View reference 27. in text" id="ref-27">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.27" data-doi="10.1038/nature04701"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Schneidman</span>  <span class="cit-name-given-names">E</span></span>, <span class="cit-auth"><span class="cit-name-surname">Berry</span>  <span class="cit-name-given-names">MJ</span></span>, <span class="cit-auth"><span class="cit-name-surname">Segev</span>  <span class="cit-name-given-names">R</span></span>, <span class="cit-auth"><span class="cit-name-surname">Bialek</span>  <span class="cit-name-given-names">W</span></span> (<span class="cit-pub-date">2006</span>) <span class="cit-article-title">Weak pairwise correlations imply strongly correlated network states in a neural population</span>. <abbr class="cit-jnl-abbrev">Nature</abbr> <span class="cit-vol">440</span>:<span class="cit-fpage">1007</span>–<span class="cit-lpage">1012</span></cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DNature%26rft.stitle%253DNature%26rft.aulast%253DSchneidman%26rft.auinit1%253DE.%26rft.volume%253D440%26rft.issue%253D7087%26rft.spage%253D1007%26rft.epage%253D1012%26rft.atitle%253DWeak%2Bpairwise%2Bcorrelations%2Bimply%2Bstrongly%2Bcorrelated%2Bnetwork%2Bstates%2Bin%2Ba%2Bneural%2Bpopulation.%26rft_id%253Dinfo%253Adoi%252F10.1038%252Fnature04701%26rft_id%253Dinfo%253Apmid%252F16625187%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a><a href="/lookup/external-ref?access_num=10.1038/nature04701&amp;link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref"><span>CrossRef</span></a><a href="/lookup/external-ref?access_num=16625187&amp;link_type=MED&amp;atom=%2Fbiorxiv%2Fearly%2F2020%2F02%2F06%2F598086.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline"><span>PubMed</span></a><a href="/lookup/external-ref?access_num=000236906000026&amp;link_type=ISI" class="cit-ref-sprinkles cit-ref-sprinkles-newisilink cit-ref-sprinkles-webofscience"><span>Web of Science</span></a></div></div></li><li><span class="ref-label">28.</span><a class="rev-xref-ref" href="#xref-ref-28-1" title="View reference 28. in text" id="ref-28">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.28"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Tkačik</span>  <span class="cit-name-given-names">G</span></span>, <span class="cit-auth"><span class="cit-name-surname">Marre</span>  <span class="cit-name-given-names">O</span></span>, <span class="cit-auth"><span class="cit-name-surname">Amodei</span>  <span class="cit-name-given-names">D</span></span>, <span class="cit-auth"><span class="cit-name-surname">Schneidman</span>  <span class="cit-name-given-names">E</span></span>, <span class="cit-auth"><span class="cit-name-surname">Bialek</span>  <span class="cit-name-given-names">W</span></span>, <span class="cit-auth"><span class="cit-name-surname">Berry</span>  <span class="cit-name-given-names">MJ</span></span> (<span class="cit-pub-date">2014</span>) <span class="cit-article-title">Searching for collective behavior in a large network of sensory neurons</span>. <abbr class="cit-jnl-abbrev">PLoS Comp Bio</abbr> <span class="cit-vol">10</span>:<span class="cit-fpage">e1003408</span>.</cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DPLoS%2BComp%2BBio%26rft.volume%253D10%26rft.spage%253De1003408%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a></div></div></li><li><span class="ref-label">29.</span><a class="rev-xref-ref" href="#xref-ref-29-1" title="View reference 29. in text" id="ref-29">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.29" data-doi="10.1073/pnas.92.9.3844"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Ben-Yishai</span>  <span class="cit-name-given-names">R</span></span>, <span class="cit-auth"><span class="cit-name-surname">Bar-Or</span>  <span class="cit-name-given-names">RL</span></span>, <span class="cit-auth"><span class="cit-name-surname">Sompolinsky</span>  <span class="cit-name-given-names">H</span></span> (<span class="cit-pub-date">1995</span>) <span class="cit-article-title">Theory of orientation tuning in visual cortex</span>. <abbr class="cit-jnl-abbrev">Proc Natl Acad Sci</abbr>, <span class="cit-vol">92</span>:<span class="cit-fpage">3844</span>–<span class="cit-lpage">3848</span></cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DPNAS%26rft.stitle%253DProc.%2BNatl.%2BAcad.%2BSci.%2BUSA%26rft.aulast%253DBen-Yishai%26rft.auinit1%253DR.%26rft.volume%253D92%26rft.issue%253D9%26rft.spage%253D3844%26rft.epage%253D3848%26rft.atitle%253DTheory%2Bof%2Borientation%2Btuning%2Bin%2Bvisual%2Bcortex.%26rft_id%253Dinfo%253Adoi%252F10.1073%252Fpnas.92.9.3844%26rft_id%253Dinfo%253Apmid%252F7731993%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a><a href="/lookup/ijlink/YTozOntzOjQ6InBhdGgiO3M6MTQ6Ii9sb29rdXAvaWpsaW5rIjtzOjU6InF1ZXJ5IjthOjQ6e3M6ODoibGlua1R5cGUiO3M6NDoiQUJTVCI7czoxMToiam91cm5hbENvZGUiO3M6NDoicG5hcyI7czo1OiJyZXNpZCI7czo5OiI5Mi85LzM4NDQiO3M6NDoiYXRvbSI7czozNzoiL2Jpb3J4aXYvZWFybHkvMjAyMC8wMi8wNi81OTgwODYuYXRvbSI7fXM6ODoiZnJhZ21lbnQiO3M6MDoiIjt9" class="cit-ref-sprinkles cit-ref-sprinkles-ijlink"><span><span class="cit-reflinks-abstract">Abstract</span><span class="cit-sep cit-reflinks-variant-name-sep">/</span><span class="cit-reflinks-full-text"><span class="free-full-text">FREE </span>Full Text</span></span></a></div></div></li><li><span class="ref-label">30.</span><div class="cit ref-cit ref-journal no-rev-xref" id="cit-598086v3.30" data-doi="10.1523/JNEUROSCI.16-06-02112.1996"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Zhang</span>  <span class="cit-name-given-names">K</span></span> (<span class="cit-pub-date">1996</span>) <span class="cit-article-title">Representation of spatial orientation by the intrinsic dynamics of the head-direction cell ensemble: a theory</span>. <abbr class="cit-jnl-abbrev">J Neurosci</abbr> <span class="cit-vol">16</span>:<span class="cit-fpage">2112</span>–<span class="cit-lpage">2126</span>.</cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DJournal%2Bof%2BNeuroscience%26rft.stitle%253DJ.%2BNeurosci.%26rft.aulast%253DZhang%26rft.auinit1%253DK.%26rft.volume%253D16%26rft.issue%253D6%26rft.spage%253D2112%26rft.epage%253D2126%26rft.atitle%253DRepresentation%2Bof%2Bspatial%2Borientation%2Bby%2Bthe%2Bintrinsic%2Bdynamics%2Bof%2Bthe%2Bhead-direction%2Bcell%2Bensemble%253A%2Ba%2Btheory%26rft_id%253Dinfo%253Adoi%252F10.1523%252FJNEUROSCI.16-06-02112.1996%26rft_id%253Dinfo%253Apmid%252F8604055%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a><a href="/lookup/ijlink/YTozOntzOjQ6InBhdGgiO3M6MTQ6Ii9sb29rdXAvaWpsaW5rIjtzOjU6InF1ZXJ5IjthOjQ6e3M6ODoibGlua1R5cGUiO3M6NDoiQUJTVCI7czoxMToiam91cm5hbENvZGUiO3M6Njoiam5ldXJvIjtzOjU6InJlc2lkIjtzOjk6IjE2LzYvMjExMiI7czo0OiJhdG9tIjtzOjM3OiIvYmlvcnhpdi9lYXJseS8yMDIwLzAyLzA2LzU5ODA4Ni5hdG9tIjt9czo4OiJmcmFnbWVudCI7czowOiIiO30=" class="cit-ref-sprinkles cit-ref-sprinkles-ijlink"><span><span class="cit-reflinks-abstract">Abstract</span><span class="cit-sep cit-reflinks-variant-name-sep">/</span><span class="cit-reflinks-full-text"><span class="free-full-text">FREE </span>Full Text</span></span></a></div></div></li><li><span class="ref-label">31.</span><a class="rev-xref-ref" href="#xref-ref-31-1" title="View reference 31. in text" id="ref-31">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.31" data-doi="10.1126/science.aal4835"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Kim</span>  <span class="cit-name-given-names">SS</span></span>, <span class="cit-auth"><span class="cit-name-surname">Rouault</span>  <span class="cit-name-given-names">H</span></span>, <span class="cit-auth"><span class="cit-name-surname">Druckmann</span>  <span class="cit-name-given-names">S</span></span>, <span class="cit-auth"><span class="cit-name-surname">Jayaraman</span>  <span class="cit-name-given-names">V</span></span> (<span class="cit-pub-date">2017</span>) <span class="cit-article-title">Ring attractor dynamics in the Drosophila central brain</span>. <abbr class="cit-jnl-abbrev">Science</abbr> <span class="cit-vol">356</span>:<span class="cit-fpage">849</span>–<span class="cit-lpage">853</span>.</cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DScience%26rft_id%253Dinfo%253Adoi%252F10.1126%252Fscience.aal4835%26rft_id%253Dinfo%253Apmid%252F28473639%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a><a href="/lookup/ijlink/YTozOntzOjQ6InBhdGgiO3M6MTQ6Ii9sb29rdXAvaWpsaW5rIjtzOjU6InF1ZXJ5IjthOjQ6e3M6ODoibGlua1R5cGUiO3M6NDoiQUJTVCI7czoxMToiam91cm5hbENvZGUiO3M6Mzoic2NpIjtzOjU6InJlc2lkIjtzOjEyOiIzNTYvNjM0MC84NDkiO3M6NDoiYXRvbSI7czozNzoiL2Jpb3J4aXYvZWFybHkvMjAyMC8wMi8wNi81OTgwODYuYXRvbSI7fXM6ODoiZnJhZ21lbnQiO3M6MDoiIjt9" class="cit-ref-sprinkles cit-ref-sprinkles-ijlink"><span><span class="cit-reflinks-abstract">Abstract</span><span class="cit-sep cit-reflinks-variant-name-sep">/</span><span class="cit-reflinks-full-text"><span class="free-full-text">FREE </span>Full Text</span></span></a></div></div></li><li><span class="ref-label">32.</span><a class="rev-xref-ref" href="#xref-ref-32-1" title="View reference 32. in text" id="ref-32">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.32" data-doi="10.1038/nature07140"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Pillow</span>  <span class="cit-name-given-names">JW</span></span>, <span class="cit-auth"><span class="cit-name-surname">Shlens</span>  <span class="cit-name-given-names">J</span></span>, <span class="cit-auth"><span class="cit-name-surname">Paninski</span>  <span class="cit-name-given-names">L</span></span>, <span class="cit-auth"><span class="cit-name-surname">Sher</span>  <span class="cit-name-given-names">A</span></span>, <span class="cit-auth"><span class="cit-name-surname">Litke</span>  <span class="cit-name-given-names">AM</span></span>, <span class="cit-auth"><span class="cit-name-surname">Chichilnisky</span>  <span class="cit-name-given-names">EJ</span></span>, <span class="cit-auth"><span class="cit-name-surname">Simoncelli</span>  <span class="cit-name-given-names">EP</span></span> (<span class="cit-pub-date">2008</span>) <span class="cit-article-title">Spatio-temporal correlations and visual signalling in a complete neuronal population</span>. <abbr class="cit-jnl-abbrev">Nature</abbr> <span class="cit-vol">454</span>:<span class="cit-fpage">995</span>–<span class="cit-lpage">999</span></cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DNature%26rft.stitle%253DNature%26rft.aulast%253DPillow%26rft.auinit1%253DJ.%2BW.%26rft.volume%253D454%26rft.issue%253D7207%26rft.spage%253D995%26rft.epage%253D999%26rft.atitle%253DSpatio-temporal%2Bcorrelations%2Band%2Bvisual%2Bsignalling%2Bin%2Ba%2Bcomplete%2Bneuronal%2Bpopulation.%26rft_id%253Dinfo%253Adoi%252F10.1038%252Fnature07140%26rft_id%253Dinfo%253Apmid%252F18650810%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a><a href="/lookup/external-ref?access_num=10.1038/nature07140&amp;link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref"><span>CrossRef</span></a><a href="/lookup/external-ref?access_num=18650810&amp;link_type=MED&amp;atom=%2Fbiorxiv%2Fearly%2F2020%2F02%2F06%2F598086.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline"><span>PubMed</span></a><a href="/lookup/external-ref?access_num=000258591000039&amp;link_type=ISI" class="cit-ref-sprinkles cit-ref-sprinkles-newisilink cit-ref-sprinkles-webofscience"><span>Web of Science</span></a></div></div></li><li><span class="ref-label">33.</span><a class="rev-xref-ref" href="#xref-ref-33-1" title="View reference 33. in text" id="ref-33">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.33"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">McIntosh</span>  <span class="cit-name-given-names">L</span></span>, <span class="cit-auth"><span class="cit-name-surname">Maheswaranathan</span>  <span class="cit-name-given-names">N</span></span>, <span class="cit-auth"><span class="cit-name-surname">Nayebi</span>  <span class="cit-name-given-names">A</span></span>, <span class="cit-auth"><span class="cit-name-surname">Ganguli</span>  <span class="cit-name-given-names">S</span></span>, <span class="cit-auth"><span class="cit-name-surname">Baccus</span>  <span class="cit-name-given-names">S</span></span> (<span class="cit-pub-date">2016</span>) <span class="cit-article-title">Deep learning models of the retinal response to natural scenes</span>. <abbr class="cit-jnl-abbrev">Adv Neur Inf Proc Sys</abbr> <span class="cit-vol">29</span>:<span class="cit-fpage">1369</span>–<span class="cit-lpage">1377</span></cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DAdv%2BNeur%2BInf%2BProc%2BSys%26rft.volume%253D29%26rft.spage%253D1369%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a></div></div></li><li><span class="ref-label">34.</span><a class="rev-xref-ref" href="#xref-ref-34-1" title="View reference 34. in text" id="ref-34">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.34" data-doi="10.1038/nn.3776"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Cunningham</span>  <span class="cit-name-given-names">JP</span></span>, <span class="cit-auth"><span class="cit-name-surname">Yu</span>  <span class="cit-name-given-names">BM</span></span> (<span class="cit-pub-date">2014</span>) <span class="cit-article-title">Dimensionality reduction for large-scale neural recordings</span>. <abbr class="cit-jnl-abbrev">Nat Neurosci</abbr> <span class="cit-vol">17</span>:<span class="cit-fpage">1500</span>–<span class="cit-lpage">1509</span></cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DNat%2BNeurosci%26rft.volume%253D17%26rft.spage%253D1500%26rft_id%253Dinfo%253Adoi%252F10.1038%252Fnn.3776%26rft_id%253Dinfo%253Apmid%252F25151264%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a><a href="/lookup/external-ref?access_num=10.1038/nn.3776&amp;link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref"><span>CrossRef</span></a><a href="/lookup/external-ref?access_num=25151264&amp;link_type=MED&amp;atom=%2Fbiorxiv%2Fearly%2F2020%2F02%2F06%2F598086.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline"><span>PubMed</span></a></div></div></li><li><span class="ref-label">35.</span><div class="cit ref-cit ref-journal no-rev-xref" id="cit-598086v3.35"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Rubin</span>  <span class="cit-name-given-names">A</span></span>, <span class="cit-auth"><span class="cit-name-surname">Sheintuch</span>  <span class="cit-name-given-names">L</span></span>, <span class="cit-auth"><span class="cit-name-surname">Brande-Eilat</span>  <span class="cit-name-given-names">N</span></span>, <span class="cit-auth"><span class="cit-name-surname">Pinchasof</span>  <span class="cit-name-given-names">O</span></span>, <span class="cit-auth"><span class="cit-name-surname">Rechavi</span>  <span class="cit-name-given-names">Y</span></span>, <span class="cit-auth"><span class="cit-name-surname">Geva</span>  <span class="cit-name-given-names">N</span></span>, <span class="cit-auth"><span class="cit-name-surname">Ziv</span>  <span class="cit-name-given-names">Y</span></span> (<span class="cit-pub-date">2019</span>) <span class="cit-article-title">Revealing neural correlates of behavior without behavioral measurements</span>. <abbr class="cit-jnl-abbrev">bioRxiv:540195</abbr></cite></div><div class="cit-extra"></div></div></li><li><span class="ref-label">36.</span><div class="cit ref-cit ref-journal no-rev-xref" id="cit-598086v3.36"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Chaudhuri</span>  <span class="cit-name-given-names">R</span></span>, <span class="cit-auth"><span class="cit-name-surname">Gercek</span>  <span class="cit-name-given-names">B</span></span>, <span class="cit-auth"><span class="cit-name-surname">Pandey</span>  <span class="cit-name-given-names">B</span></span>, <span class="cit-auth"><span class="cit-name-surname">Peyrache</span>  <span class="cit-name-given-names">A</span></span>, <span class="cit-auth"><span class="cit-name-surname">Fiete</span>  <span class="cit-name-given-names">I</span></span> (<span class="cit-pub-date">2019</span>) <span class="cit-article-title">The population dynamics of a canonical cognitive circuit</span>. <abbr class="cit-jnl-abbrev">bioRxiv: 516021</abbr></cite></div><div class="cit-extra"></div></div></li><li><span class="ref-label">37.</span><a class="rev-xref-ref" href="#xref-ref-37-1" title="View reference 37. in text" id="ref-37">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.37"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Goddard</span>  <span class="cit-name-given-names">E</span></span>, <span class="cit-auth"><span class="cit-name-surname">Klein</span>  <span class="cit-name-given-names">C</span></span>, <span class="cit-auth"><span class="cit-name-surname">Solomon</span>  <span class="cit-name-given-names">SG</span></span>, <span class="cit-auth"><span class="cit-name-surname">Hogendoorn</span>  <span class="cit-name-given-names">H</span></span>, <span class="cit-auth"><span class="cit-name-surname">Carlson</span>  <span class="cit-name-given-names">TA</span></span> (<span class="cit-pub-date">2018</span>) <span class="cit-article-title">Interpreting the dimensions of neural feature representations revealed by dimensionality reduction</span> <abbr class="cit-jnl-abbrev">NeuroImage</abbr> <span class="cit-vol">180</span>:<span class="cit-fpage">41</span>–<span class="cit-lpage">67</span></cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DNeuroImage%26rft.volume%253D180%26rft.spage%253D41%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a></div></div></li><li><span class="ref-label">38.</span><a class="rev-xref-ref" href="#xref-ref-38-1" title="View reference 38. in text" id="ref-38">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.38"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Sharpee</span>  <span class="cit-name-given-names">T</span></span>, <span class="cit-auth"><span class="cit-name-surname">Rust</span>  <span class="cit-name-given-names">NT</span></span>, <span class="cit-auth"><span class="cit-name-surname">Bialek</span>  <span class="cit-name-given-names">W</span></span> (<span class="cit-pub-date">2003</span>) <span class="cit-article-title">Maximally informative dimensions: analyzing neural responses to natural signals</span>. <abbr class="cit-jnl-abbrev">Adv Neur Inf Proc Sys</abbr> <span class="cit-fpage">277</span>–<span class="cit-lpage">284</span></cite></div><div class="cit-extra"></div></div></li><li><span class="ref-label">39.</span><a class="rev-xref-ref" href="#xref-ref-39-1" title="View reference 39. in text" id="ref-39">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.39"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Niv</span>  <span class="cit-name-given-names">Y</span></span> (<span class="cit-pub-date">2009</span>) <span class="cit-article-title">Reinforcement learning in the brain</span>. <abbr class="cit-jnl-abbrev">J Mathemat Psychol</abbr> <span class="cit-vol">53</span>:<span class="cit-fpage">139</span>–<span class="cit-lpage">154</span></cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DJ%2BMathemat%2BPsychol%26rft.volume%253D53%26rft.spage%253D139%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a></div></div></li><li><span class="ref-label">40.</span><div class="cit ref-cit ref-journal no-rev-xref" id="cit-598086v3.40"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Dayan</span>  <span class="cit-name-given-names">P</span></span>, <span class="cit-auth"><span class="cit-name-surname">Niv</span>  <span class="cit-name-given-names">Y</span></span> (<span class="cit-pub-date">2008</span>) <span class="cit-article-title">Reinforcement learning: the good, the bad and the ugly</span>. <abbr class="cit-jnl-abbrev">Curr Op Neurobio</abbr> <span class="cit-vol">18</span>:<span class="cit-fpage">185</span>–<span class="cit-lpage">196</span>.</cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DCurr%2BOp%2BNeurobio%26rft.volume%253D18%26rft.spage%253D185%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a></div></div></li><li><span class="ref-label">41.</span><a class="rev-xref-ref" href="#xref-ref-41-1" title="View reference 41. in text" id="ref-41">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.41"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Daw</span>  <span class="cit-name-given-names">ND</span></span>, <span class="cit-auth"><span class="cit-name-surname">Doya</span>  <span class="cit-name-given-names">K</span></span> (<span class="cit-pub-date">2006</span>) <span class="cit-article-title">The computational neurobiology of learning and reward</span>. <abbr class="cit-jnl-abbrev">Curr Op Neurobio</abbr> <span class="cit-vol">16</span>:<span class="cit-fpage">199</span>–<span class="cit-lpage">204</span>.</cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DCurr%2BOp%2BNeurobio%26rft.volume%253D16%26rft.spage%253D199%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a></div></div></li><li><span class="ref-label">42.</span><a class="rev-xref-ref" href="#xref-ref-42-1" title="View reference 42. in text" id="ref-42">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.42" data-doi="10.1038/35090500"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Fairhall</span>  <span class="cit-name-given-names">AL</span></span>, <span class="cit-auth"><span class="cit-name-surname">Geoffrey</span>  <span class="cit-name-given-names">DL</span></span>, <span class="cit-auth"><span class="cit-name-surname">William</span>  <span class="cit-name-given-names">B</span></span>, <span class="cit-auth"><span class="cit-name-surname">de Ruyter van Steveninck</span>  <span class="cit-name-given-names">RR.</span></span> (<span class="cit-pub-date">2001</span>) <span class="cit-article-title">Efficiency and ambiguity in an adaptive neural code</span>. <abbr class="cit-jnl-abbrev">Nature</abbr> <span class="cit-vol">412</span>:<span class="cit-fpage">787</span>.</cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DNature%26rft.stitle%253DNature%26rft.aulast%253DFairhall%26rft.auinit1%253DA.%2BL.%26rft.volume%253D412%26rft.issue%253D6849%26rft.spage%253D787%26rft.epage%253D792%26rft.atitle%253DEfficiency%2Band%2Bambiguity%2Bin%2Ban%2Badaptive%2Bneural%2Bcode.%26rft_id%253Dinfo%253Adoi%252F10.1038%252F35090500%26rft_id%253Dinfo%253Apmid%252F11518957%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a><a href="/lookup/external-ref?access_num=10.1038/35090500&amp;link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref"><span>CrossRef</span></a><a href="/lookup/external-ref?access_num=11518957&amp;link_type=MED&amp;atom=%2Fbiorxiv%2Fearly%2F2020%2F02%2F06%2F598086.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline"><span>PubMed</span></a><a href="/lookup/external-ref?access_num=000170577200028&amp;link_type=ISI" class="cit-ref-sprinkles cit-ref-sprinkles-newisilink cit-ref-sprinkles-webofscience"><span>Web of Science</span></a></div></div></li><li><span class="ref-label">43.</span><a class="rev-xref-ref" href="#xref-ref-43-1" title="View reference 43. in text" id="ref-43">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.43" data-doi="10.1038/nn.3382"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Benucci</span>  <span class="cit-name-given-names">A</span></span>, <span class="cit-auth"><span class="cit-name-surname">Saleem</span>  <span class="cit-name-given-names">AB</span></span>, <span class="cit-auth"><span class="cit-name-surname">Carandini</span>  <span class="cit-name-given-names">M.</span></span> (<span class="cit-pub-date">2013</span>). <span class="cit-article-title">Adaptation maintains population homeostasis in primary visual cortex</span>. <abbr class="cit-jnl-abbrev">Nat Neurosci</abbr> <span class="cit-vol">16</span>:<span class="cit-fpage">724</span>.</cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DNat%2BNeurosci%26rft.volume%253D16%26rft.spage%253D724%26rft_id%253Dinfo%253Adoi%252F10.1038%252Fnn.3382%26rft_id%253Dinfo%253Apmid%252F23603708%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a><a href="/lookup/external-ref?access_num=10.1038/nn.3382&amp;link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref"><span>CrossRef</span></a><a href="/lookup/external-ref?access_num=23603708&amp;link_type=MED&amp;atom=%2Fbiorxiv%2Fearly%2F2020%2F02%2F06%2F598086.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline"><span>PubMed</span></a></div></div></li><li><span class="ref-label">44.</span><a class="rev-xref-ref" href="#xref-ref-44-1" title="View reference 44. in text" id="ref-44">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.44" data-doi="10.1038/nature17643"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Li</span>  <span class="cit-name-given-names">N</span></span>, <span class="cit-auth"><span class="cit-name-surname">Kayvon</span>  <span class="cit-name-given-names">D</span></span>, <span class="cit-auth"><span class="cit-name-surname">Karel</span>  <span class="cit-name-given-names">S</span></span>, and <span class="cit-auth"><span class="cit-name-surname">Shaul</span>  <span class="cit-name-given-names">D.</span></span> (<span class="cit-pub-date">2016</span>) <span class="cit-article-title">Robust neuronal dynamics in premotor cortex during motor planning</span>. <abbr class="cit-jnl-abbrev">Nature</abbr>. <span class="cit-vol">532</span>:<span class="cit-fpage">459</span>.</cite></div><div class="cit-extra"><a href="{openurl}?query=rft.jtitle%253DNature%26rft.volume%253D532%26rft.spage%253D459%26rft_id%253Dinfo%253Adoi%252F10.1038%252Fnature17643%26rft_id%253Dinfo%253Apmid%252F27074502%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url"><span>OpenUrl</span></a><a href="/lookup/external-ref?access_num=10.1038/nature17643&amp;link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref"><span>CrossRef</span></a><a href="/lookup/external-ref?access_num=27074502&amp;link_type=MED&amp;atom=%2Fbiorxiv%2Fearly%2F2020%2F02%2F06%2F598086.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline"><span>PubMed</span></a></div></div></li><li><span class="ref-label">45.</span><a class="rev-xref-ref" href="#xref-ref-45-1" title="View reference 45. in text" id="ref-45">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.45"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Mlynarski</span>  <span class="cit-name-given-names">W</span></span>, <span class="cit-auth"><span class="cit-name-surname">Hledik</span>  <span class="cit-name-given-names">M</span></span>, <span class="cit-auth"><span class="cit-name-surname">Sokolowski</span>  <span class="cit-name-given-names">TR</span></span>, <span class="cit-auth"><span class="cit-name-surname">Tkacik</span>  <span class="cit-name-given-names">G</span></span> (<span class="cit-pub-date">2019</span>). <span class="cit-article-title">Statistical analysis and optimality of biological systems</span>. <abbr class="cit-jnl-abbrev">bioRxiv:848374</abbr>.</cite></div><div class="cit-extra"></div></div></li><li><span class="ref-label">46.</span><a class="rev-xref-ref" href="#xref-ref-46-1" title="View reference 46. in text" id="ref-46">↵</a><div class="cit ref-cit ref-journal" id="cit-598086v3.46"><div class="cit-metadata"><cite><span class="cit-auth"><span class="cit-name-surname">Aenugu</span>  <span class="cit-name-given-names">S</span></span>, <span class="cit-auth"><span class="cit-name-surname">Abhishek</span>  <span class="cit-name-given-names">S</span></span>, <span class="cit-auth"><span class="cit-name-surname">Sasikiran</span>  <span class="cit-name-given-names">Y</span></span>, <span class="cit-auth"><span class="cit-name-surname">Hananel</span>  <span class="cit-name-given-names">H</span></span>, <span class="cit-auth"><span class="cit-name-surname">Thomas</span>  <span class="cit-name-given-names">PS</span></span>, <span class="cit-auth"><span class="cit-name-surname">Kozma</span>  <span class="cit-name-given-names">R.</span></span> (<span class="cit-pub-date">2019</span>) <abbr class="cit-jnl-abbrev">Reinforcement learning with spiking coagents</abbr>.<span class="cit-pub-id-sep cit-pub-id-arxiv-sep"> </span><span class="cit-pub-id-scheme">arXiv:</span><span class="cit-pub-id cit-pub-id-arxiv">1910.06489</span></cite></div><div class="cit-extra"></div></div></li></ol></div><span class="highwire-journal-article-marker-end"></span></div><span id="related-urls"></span></div><a href="https://www.biorxiv.org/content/10.1101/598086v3.abstract" class="hw-link hw-link-article-abstract" data-icon-position="" data-hide-link-title="0">View Abstract</a></div>  </div>

  
  </div>
</div>
  </div>
</div>
</div></div>  </div>

  
  </div>
<div class="panel-separator"></div><div class="panel-pane pane-disqus-comment" >
  
      
  
  <div class="pane-content">
    <div id="disqus_thread"><noscript><p><a href="http://biorxivstage.disqus.com/?url=https%3A%2F%2Fwww.biorxiv.org%2Fcontent%2F10.1101%2F598086v3" class="" data-icon-position="" data-hide-link-title="0">View the discussion thread.</a></p></noscript></div>  </div>

  
  </div>
<div class="panel-separator"></div><div class="panel-pane pane-highwire-back-to-top" >
  
      
  
  <div class="pane-content">
    <a href="#page" class="back-to-top" data-icon-position="" data-hide-link-title="0"><i class="icon-chevron-up"></i> Back to top</a>  </div>

  
  </div>
</div>
			</div>
		</div>
		
		<div class="sidebar-right-wrapper grid-10 omega">
			<div class="panel-panel panel-region-sidebar-right">
			  <div class="inside"><div class="panel-pane pane-highwire-node-pager" >
  
      
  
  <div class="pane-content">
    <div class="pager highwire-pager pager-mini clearfix highwire-node-pager highwire-article-pager"><span class="pager-prev"><a href="/content/10.1101/511402v2" title="Reconciling the potentially irreconcilable? Genotypic and phenotypic amoxicillin-clavulanate resistance in Escherichia coli" rel="prev" class="pager-link-prev link-icon"><i class="icon-circle-arrow-left"></i> <span class="title">Previous</span></a></span><span class="pager-next"><a href="/content/10.1101/707489v3" title="BIAFLOWS: A collaborative framework to reproducibly deploy and benchmark bioimage analysis workflows" rel="next" class="pager-link-next link-icon-right link-icon"><span class="title">Next</span> <i class="icon-circle-arrow-right"></i></a></span></div>  </div>

  
  </div>
<div class="panel-separator"></div><div class="panel-pane pane-custom pane-1" >
  
      
  
  <div class="pane-content">
    Posted&nbsp;February 06, 2020.  </div>

  
  </div>
<div class="panel-separator"></div><div class="panel-pane pane-panels-mini pane-biorxiv-art-tools" >
  
      
  
  <div class="pane-content">
    <div id="mini-panel-biorxiv_art_tools" class="highwire-2col-stacked panel-display">
	  
  <div class="panel-row-wrapper clearfix">
		<div class="content-left-wrapper content-column">
          <div class="panel-panel panel-region-content-left">
            <div class="inside"><div class="panel-pane pane-highwire-variant-link" >
  
      
  
  <div class="pane-content">
    <a href="/content/10.1101/598086v3.full.pdf" target="_self" class="article-dl-pdf-link link-icon"><i class="icon-external-link-sign"></i> <span class="title">Download PDF</span></a>  </div>

  
  </div>
</div>
          </div>
        </div>
        
        <div class="content-right-wrapper content-column">
          <div class="panel-panel panel-region-content-right">
            <div class="inside"><div class="panel-pane pane-minipanel-dialog-link pane-biorxiv-art-email" >
  
      
  
  <div class="pane-content">
    <div class='minipanel-dialog-wrapper'><div class='minipanel-dialog-link-link'><a href="/" oncontextmenu="javascript: return false;" class="minipanel-dialog-link-trigger" title="Email this Article" data-icon-position="" data-hide-link-title="0"><i class = 'icon-envelope'></i> Email</a></div><div class='minipanel-dialog-link-mini' style='display:none'><div class="panel-display panel-1col clearfix" id="mini-panel-biorxiv_art_email">
  <div class="panel-panel panel-col">
    <div><div class="panel-pane pane-block pane-forward-form pane-forward" >
  
      
  
  <div class="pane-content">
    <form action="/content/10.1101/598086v3.full" method="post" id="forward-form" accept-charset="UTF-8"><div><div id="edit-instructions" class="form-item form-item-label-before form-type-item">
 <p>Thank you for your interest in spreading the word about bioRxiv.</p><p>NOTE: Your email address is requested solely to identify you as the sender of this article.</p>
</div>
<div class="form-item form-item-label-before form-type-textfield form-item-email">
  <label for="edit-email">Your Email <span class="form-required" title="This field is required.">*</span></label>
 <input type="text" id="edit-email" name="email" value="" size="58" maxlength="256" class="form-text required" />
</div>
<div class="form-item form-item-label-before form-type-textfield form-item-name">
  <label for="edit-name">Your Name <span class="form-required" title="This field is required.">*</span></label>
 <input type="text" id="edit-name" name="name" value="" size="58" maxlength="256" class="form-text required" />
</div>
<div class="form-item form-item-label-before form-type-textarea form-item-recipients">
  <label for="edit-recipients">Send To <span class="form-required" title="This field is required.">*</span></label>
 <div class="form-textarea-wrapper resizable"><textarea id="edit-recipients" name="recipients" cols="50" rows="5" class="form-textarea required"></textarea></div>
<div class="description">Enter multiple addresses on separate lines or separate them with commas.</div>
</div>
<div id="edit-page" class="form-item form-item-label-before form-type-item">
  <label for="edit-page">You are going to email the following </label>
 <a href="/content/10.1101/598086v3" class="active" data-icon-position="" data-hide-link-title="0">Training and inferring neural network function with multi-agent reinforcement learning</a>
</div>
<div id="edit-subject" class="form-item form-item-label-before form-type-item">
  <label for="edit-subject">Message Subject </label>
 (Your Name) has forwarded a page to you from bioRxiv
</div>
<div id="edit-body" class="form-item form-item-label-before form-type-item">
  <label for="edit-body">Message Body </label>
 (Your Name) thought you would like to see this page from the bioRxiv website.
</div>
<div class="form-item form-item-label-before form-type-textarea form-item-message">
  <label for="edit-message--2">Your Personal Message </label>
 <div class="form-textarea-wrapper resizable"><textarea id="edit-message--2" name="message" cols="50" rows="10" class="form-textarea"></textarea></div>
</div>
<input type="hidden" name="path" value="node/1137937" />
<input type="hidden" name="path_cid" value="" />
<input type="hidden" name="forward_footer" value=" " />
<input type="hidden" name="form_build_id" value="form-zHe7RnWa6ZxnpP5vrsZgAprZX9BZUHCA8jVNqRYHaJU" />
<input type="hidden" name="form_id" value="forward_form" />
<div class="form-actions form-wrapper" id="edit-actions"><input type="submit" id="edit-submit" name="op" value="Send Message" class="form-submit" /></div></div></form>  </div>

  
  </div>
</div>
  </div>
</div>
</div></div>  </div>

  
  </div>
<div class="panel-separator"></div><div class="panel-pane pane-highwire-share-link highwire_clipboard_link_ajax"  id="shareit">
  
      
  
  <div class="pane-content">
    <a href="/" class="link-icon"><i class="icon-share-alt"></i> <span class="title">Share</span></a>  </div>

  
  </div>
<div class="panel-separator"></div><div class="panel-pane pane-panels-mini pane-biorxiv-share highwire_clipboard_form_ajax_shareit" >
  
      
  
  <div class="pane-content">
    
<div class="panel-display omega-12-onecol" id="mini-panel-biorxiv_share">

		  <div class="panel-panel grid-12 panel-region-preface">
		  <div class="inside"><div class="panel-pane pane-highwire-article-citation" >
  
      
  
  <div class="pane-content">
    <div class="highwire-article-citation highwire-citation-type-highwire-article" data-node-nid="1137937" id="node1137937--2" data-pisa="biorxiv;598086v3" data-pisa-master="biorxiv;598086" data-seqnum="1137937" data-apath="/biorxiv/early/2020/02/06/598086.atom"><div  class="highwire-cite highwire-cite-highwire-article highwire-citation-biorxiv-article-pap-list clearfix" >

      <div class="highwire-cite-title" >
      <div class="highwire-cite-title">Training and inferring neural network function with multi-agent reinforcement learning</div>    </div>
  
      <div  class="highwire-cite-authors" ><span  class="highwire-citation-authors"><span class="highwire-citation-author first" data-delta="0"><span class="nlm-given-names">Matthew</span> <span class="nlm-surname">Chalk</span></span>, <span class="highwire-citation-author" data-delta="1"><span class="nlm-given-names">Gasper</span> <span class="nlm-surname">Tkacik</span></span>, <span class="highwire-citation-author" data-delta="2"><span class="nlm-given-names">Olivier</span> <span class="nlm-surname">Marre</span></span></span></div>
  
      <div  class="highwire-cite-metadata" ><span  class="highwire-cite-metadata-journal highwire-cite-metadata">bioRxiv </span><span  class="highwire-cite-metadata-pages highwire-cite-metadata">598086; </span><span  class="highwire-cite-metadata-doi highwire-cite-metadata"><span class="doi_label">doi:</span> https://doi.org/10.1101/598086 </span></div>
  
  
  </div>
</div>  </div>

  
  </div>
</div>
	  </div>
	
  <div class="panel-panel grid-12 panel-region-content">
  	<div class="inside"><div class="panel-pane pane-highwire-article-clipboard-copy" >
  
      
  
  <div class="pane-content">
    <div class = "clipboard-copy">
  <span class="label-url">
    <label for="dynamic">Share This Article:</label>
  </span>
  <span class="input-text-url">
    <input type="text" id="dynamic" value="https://www.biorxiv.org/content/10.1101/598086v3" size="50"/>
  </span>
  <span class="copy-button button">
    <button id="copy-dynamic" class="clipboardjs-button" data-clipboard-target="#dynamic" data-clipboard-alert-style="tooltip" data-clipboard-alert-text="Copied!">Copy</button>
  </span>
</div>
  </div>

  
  </div>
</div>
  </div>
	
		  <div class="panel-panel grid-12 panel-region-postscript">
	    <div class="inside"><div class="panel-pane pane-service-links" >
  
      
  
  <div class="pane-content">
    <div class="service-links"><a href="http://digg.com/submit?phase=2&amp;url=https%3A//www.biorxiv.org/content/10.1101/598086v3&amp;title=Training%20and%20inferring%20neural%20network%20function%20with%20multi-agent%20reinforcement%20learning" id="digg" title="Digg this post on digg.com" class="service-links-digg" rel="nofollow" data-icon-position="" data-hide-link-title="0"><img src="https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/digg.png" alt="Digg logo" /></a> <a href="http://reddit.com/submit?url=https%3A//www.biorxiv.org/content/10.1101/598086v3&amp;title=Training%20and%20inferring%20neural%20network%20function%20with%20multi-agent%20reinforcement%20learning" id="reddit" title="Submit this post on reddit.com" class="service-links-reddit" rel="nofollow" data-icon-position="" data-hide-link-title="0"><img src="https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/reddit.png" alt="Reddit logo" /></a> <a href="http://twitter.com/share?url=https%3A//www.biorxiv.org/content/10.1101/598086v3&amp;text=Training%20and%20inferring%20neural%20network%20function%20with%20multi-agent%20reinforcement%20learning" id="twitter" title="Share this on Twitter" class="service-links-twitter" rel="nofollow" data-icon-position="" data-hide-link-title="0"><img src="https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/twitter.png" alt="Twitter logo" /></a> <a href="http://www.citeulike.org/posturl?url=https%3A//www.biorxiv.org/content/10.1101/598086v3&amp;title=Training%20and%20inferring%20neural%20network%20function%20with%20multi-agent%20reinforcement%20learning" id="citeulike" title="Share on CiteULike" class="service-links-citeulike" rel="nofollow" data-icon-position="" data-hide-link-title="0"><img src="https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/citeyoulike.png" alt="CiteULike logo" /></a> <a href="http://www.facebook.com/sharer.php?u=https%3A//www.biorxiv.org/content/10.1101/598086v3&amp;t=Training%20and%20inferring%20neural%20network%20function%20with%20multi-agent%20reinforcement%20learning" id="facebook" title="Share on Facebook" class="service-links-facebook" rel="nofollow" data-icon-position="" data-hide-link-title="0"><img src="https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/fb-blue.png" alt="Facebook logo" /></a> <a href="http://www.google.com/bookmarks/mark?op=add&amp;bkmk=https%3A//www.biorxiv.org/content/10.1101/598086v3&amp;title=Training%20and%20inferring%20neural%20network%20function%20with%20multi-agent%20reinforcement%20learning" id="google" title="Bookmark this post on Google" class="service-links-google" rel="nofollow" data-icon-position="" data-hide-link-title="0"><img src="https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/google-32.png" alt="Google logo" /></a> <a href="http://www.mendeley.com/import/?url=https%3A//www.biorxiv.org/content/10.1101/598086v3&amp;title=Training%20and%20inferring%20neural%20network%20function%20with%20multi-agent%20reinforcement%20learning" id="mendeley" title="Share on Mendeley" class="service-links-mendeley" rel="nofollow" data-icon-position="" data-hide-link-title="0"><img src="https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/mendeley.png" alt="Mendeley logo" /></a></div>  </div>

  
  </div>
</div>
	  </div>
	
</div>
  </div>

  
  </div>
<div class="panel-separator"></div><div class="panel-pane pane-minipanel-dialog-link pane-biorxiv-cite-tool" >
  
      
  
  <div class="pane-content">
    <div class='minipanel-dialog-wrapper'><div class='minipanel-dialog-link-link'><a href="/" oncontextmenu="javascript: return false;" class="minipanel-dialog-link-trigger link-icon" title="Citation Tools"><i class="icon-globe"></i> <span class="title">Citation Tools</span></a></div><div class='minipanel-dialog-link-mini' style='display:none'><div class="panel-display panel-1col clearfix" id="mini-panel-biorxiv_cite_tool">
  <div class="panel-panel panel-col">
    <div><div class="panel-pane pane-highwire-citation-export" >
  
      
  
  <div class="pane-content">
    <div class="highwire-citation-export">  
  <div class="highwire-citation-info">
      <div class="highwire-article-citation highwire-citation-type-highwire-article" data-node-nid="1137937" id="node1137937--3" data-pisa="biorxiv;598086v3" data-pisa-master="biorxiv;598086" data-seqnum="1137937" data-apath="/biorxiv/early/2020/02/06/598086.atom"><div  class="highwire-cite highwire-cite-highwire-article highwire-citation-biorxiv-article-pap-list clearfix" >

      <div class="highwire-cite-title" >
      <div class="highwire-cite-title">Training and inferring neural network function with multi-agent reinforcement learning</div>    </div>
  
      <div  class="highwire-cite-authors" ><span  class="highwire-citation-authors"><span class="highwire-citation-author first" data-delta="0"><span class="nlm-given-names">Matthew</span> <span class="nlm-surname">Chalk</span></span>, <span class="highwire-citation-author" data-delta="1"><span class="nlm-given-names">Gasper</span> <span class="nlm-surname">Tkacik</span></span>, <span class="highwire-citation-author" data-delta="2"><span class="nlm-given-names">Olivier</span> <span class="nlm-surname">Marre</span></span></span></div>
  
      <div  class="highwire-cite-metadata" ><span  class="highwire-cite-metadata-journal highwire-cite-metadata">bioRxiv </span><span  class="highwire-cite-metadata-pages highwire-cite-metadata">598086; </span><span  class="highwire-cite-metadata-doi highwire-cite-metadata"><span class="doi_label">doi:</span> https://doi.org/10.1101/598086 </span></div>
  
  
  </div>
</div>  </div>
  <div class="highwire-citation-formats">
  	      <h2>Citation Manager Formats</h2>
        <div class="highwire-citation-formats-links">
      <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.spage&amp;rft.epage&amp;rft.atitle=Training%20and%20inferring%20neural%20network%20function%20with%20multi-agent%20reinforcement%20learning&amp;rft.volume&amp;rft.issue&amp;rft.date=2020-01-01%2000%3A00%3A00&amp;rft.stitle&amp;rft.jtitle=bioRxiv&amp;rft.au=Chalk%2C+Matthew&amp;rft.au=Tkacik%2C+Gasper&amp;rft.au=Marre%2C+Olivier"></span><ul class="hw-citation-links inline button button-alt button-grid clearfix"><li class="bibtext first"><a href="/highwire/citation/1137937/bibtext" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">BibTeX</a></li><li class="bookends"><a href="/highwire/citation/1137937/bookends" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">Bookends</a></li><li class="easybib"><a href="/highwire/citation/1137937/easybib" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">EasyBib</a></li><li class="endnote-tagged"><a href="/highwire/citation/1137937/endnote-tagged" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">EndNote (tagged)</a></li><li class="endnote-8-xml"><a href="/highwire/citation/1137937/endnote-8-xml" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">EndNote 8 (xml)</a></li><li class="medlars"><a href="/highwire/citation/1137937/medlars" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">Medlars</a></li><li class="mendeley"><a href="/highwire/citation/1137937/mendeley" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">Mendeley</a></li><li class="papers"><a href="/highwire/citation/1137937/papers" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">Papers</a></li><li class="refworks-tagged"><a href="/highwire/citation/1137937/refworks-tagged" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">RefWorks Tagged</a></li><li class="reference-manager"><a href="/highwire/citation/1137937/reference-manager" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">Ref Manager</a></li><li class="ris"><a href="/highwire/citation/1137937/ris" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">RIS</a></li><li class="zotero last"><a href="/highwire/citation/1137937/zotero" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">Zotero</a></li></ul>    </div>
  </div>
</div>
  </div>

  
  </div>
</div>
  </div>
</div>
</div></div>  </div>

  
  </div>
</div>
          </div>
        </div>
	</div> <!-- /.panel-row-wrapper -->	
	
	</div>

  </div>

  
  </div>
<div class="panel-separator"></div><div class="panel-pane pane-service-links" >
  
      
  
  <div class="pane-content">
    <div class="service-links"><div class="item-list"><ul><li class="first"><a href="http://twitter.com/share?url=https%3A//www.biorxiv.org/content/10.1101/598086v3&amp;count=horizontal&amp;via=&amp;text=Training%20and%20inferring%20neural%20network%20function%20with%20multi-agent%20reinforcement%20learning&amp;counturl=https%3A//www.biorxiv.org/content/10.1101/598086v3" class="twitter-share-button service-links-twitter-widget" id="twitter_widget" title="Tweet This" rel="nofollow" data-icon-position="" data-hide-link-title="0"><span class="element-invisible">Tweet Widget</span></a></li><li><a href="http://www.facebook.com/plugins/like.php?href=https%3A//www.biorxiv.org/content/10.1101/598086v3&amp;layout=button_count&amp;show_faces=false&amp;action=like&amp;colorscheme=light&amp;width=100&amp;height=21&amp;font=&amp;locale=" id="facebook_like" title="I Like it" class="service-links-facebook-like" rel="nofollow" data-icon-position="" data-hide-link-title="0"><span class="element-invisible">Facebook Like</span></a></li><li class="last"><a href="https://www.biorxiv.org/content/10.1101/598086v3" id="google_plus_one" title="Plus it" class="service-links-google-plus-one" rel="nofollow" data-icon-position="" data-hide-link-title="0"><span class="element-invisible">Google Plus One</span></a></li></ul></div></div>  </div>

  
  </div>
<div class="panel-separator"></div><div class="panel-pane pane-highwire-article-collections" >
  
        <h2 class="pane-title">Subject Area</h2>
    
  
  <div class="pane-content">
    <div class="highwire-list-wrapper highwire-article-collections"><div class="highwire-list"><ul class="highwire-article-collection-term-list"><li class="first last odd"><span class="highwire-article-collection-term"><a href="/collection/neuroscience" class="highlight" data-icon-position="" data-hide-link-title="0">Neuroscience<i class="icon-caret-right"></i>
</a></span></li></ul></div></div>  </div>

  
  </div>
<div class="panel-separator"></div><div class="panel-pane pane-panels-mini pane-biorxiv-subject-collections block-style-col2" >
  
      
  
  <div class="pane-content">
    <div class="panel-flexible panels-flexible-8 clearfix" id="mini-panel-biorxiv_subject_collections">
<div class="panel-flexible-inside panels-flexible-8-inside">
<div class="panels-flexible-region panels-flexible-region-8-center panels-flexible-region-first panels-flexible-region-last">
  <div class="inside panels-flexible-region-inside panels-flexible-region-8-center-inside panels-flexible-region-inside-first panels-flexible-region-inside-last">
<div class="panel-pane pane-snippet" >
  
      
  
  <div class="pane-content">
    <div class="snippet biorxiv-subject-areas-table-title" id="biorxiv-subject-areas-table-title">
  
      
  <div class="snippet-content">
    <b>Subject Areas</b>  </div>

</div>
  </div>

  
  </div>
<div class="panel-separator"></div><div class="panel-pane pane-snippet" >
  
      
  
  <div class="pane-content">
    <div class="snippet biorxiv-subject-areas-view-papers" id="biorxiv-subject-areas-view-papers">
  
      
  <div class="snippet-content">
    <a href="/content/early/recent"><strong>All Articles</strong></a>  </div>

</div>
  </div>

  
  </div>
<div class="panel-separator"></div><div class="panel-pane pane-highwire-subject-collections" >
  
      
  
  <div class="pane-content">
    <ul id="collection" class="collection highwire-list-expand"><li class="outer collection depth-2  child first"><div class = "data-wrapper"><a href="/collection/animal-behavior-and-cognition" class="" data-icon-position="" data-hide-link-title="0">Animal Behavior and Cognition</a> <span class = "article-count">(1675)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/biochemistry" class="" data-icon-position="" data-hide-link-title="0">Biochemistry</a> <span class = "article-count">(2828)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/bioengineering" class="" data-icon-position="" data-hide-link-title="0">Bioengineering</a> <span class = "article-count">(1964)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/bioinformatics" class="" data-icon-position="" data-hide-link-title="0">Bioinformatics</a> <span class = "article-count">(10460)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/biophysics" class="" data-icon-position="" data-hide-link-title="0">Biophysics</a> <span class = "article-count">(4278)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/cancer-biology" class="" data-icon-position="" data-hide-link-title="0">Cancer Biology</a> <span class = "article-count">(3304)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/cell-biology" class="" data-icon-position="" data-hide-link-title="0">Cell Biology</a> <span class = "article-count">(4688)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/clinical-trials" class="" data-icon-position="" data-hide-link-title="0">Clinical Trials</a> <span class = "article-count">(136)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/developmental-biology" class="" data-icon-position="" data-hide-link-title="0">Developmental Biology</a> <span class = "article-count">(2911)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/ecology" class="" data-icon-position="" data-hide-link-title="0">Ecology</a> <span class = "article-count">(4577)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/epidemiology" class="" data-icon-position="" data-hide-link-title="0">Epidemiology</a> <span class = "article-count">(2041)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/evolutionary-biology" class="" data-icon-position="" data-hide-link-title="0">Evolutionary Biology</a> <span class = "article-count">(7339)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/genetics" class="" data-icon-position="" data-hide-link-title="0">Genetics</a> <span class = "article-count">(5576)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/genomics" class="" data-icon-position="" data-hide-link-title="0">Genomics</a> <span class = "article-count">(6942)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/immunology" class="" data-icon-position="" data-hide-link-title="0">Immunology</a> <span class = "article-count">(2466)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/microbiology" class="" data-icon-position="" data-hide-link-title="0">Microbiology</a> <span class = "article-count">(7700)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/molecular-biology" class="" data-icon-position="" data-hide-link-title="0">Molecular Biology</a> <span class = "article-count">(3083)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/neuroscience" class="" data-icon-position="" data-hide-link-title="0">Neuroscience</a> <span class = "article-count">(19049)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/paleontology" class="" data-icon-position="" data-hide-link-title="0">Paleontology</a> <span class = "article-count">(142)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/pathology" class="" data-icon-position="" data-hide-link-title="0">Pathology</a> <span class = "article-count">(481)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/pharmacology-and-toxicology" class="" data-icon-position="" data-hide-link-title="0">Pharmacology and Toxicology</a> <span class = "article-count">(798)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/physiology" class="" data-icon-position="" data-hide-link-title="0">Physiology</a> <span class = "article-count">(1190)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/plant-biology" class="" data-icon-position="" data-hide-link-title="0">Plant Biology</a> <span class = "article-count">(2805)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/scientific-communication-and-education" class="" data-icon-position="" data-hide-link-title="0">Scientific Communication and Education</a> <span class = "article-count">(699)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/synthetic-biology" class="" data-icon-position="" data-hide-link-title="0">Synthetic Biology</a> <span class = "article-count">(919)</span></div></li>
<li class="outer collection depth-2  child"><div class = "data-wrapper"><a href="/collection/systems-biology" class="" data-icon-position="" data-hide-link-title="0">Systems Biology</a> <span class = "article-count">(2899)</span></div></li>
<li class="outer collection depth-2  child last"><div class = "data-wrapper"><a href="/collection/zoology" class="" data-icon-position="" data-hide-link-title="0">Zoology</a> <span class = "article-count">(473)</span></div></li>
</ul>  </div>

  
  </div>
  </div>
</div>
</div>
</div>
  </div>

  
  </div>
</div>
			</div>
		</div>
	
	</div> <!-- /.panel-row-wrapper -->	
	
	</div>

    </div>
  </div>
</div>      </div>
</div>  </div>
</section>    
  
  </div>    <div class="region region-page-bottom" id="region-page-bottom">
  <div class="region-inner region-page-bottom-inner">
      </div>
</div><script type="text/javascript" src="https://www.biorxiv.org/sites/default/files/advagg_js/js__VNH5GD8Zz7g6_hGCOZjVjdMndxxC6naiExSSJKX3k_A__kzmEdtbqCsJx5Z9yg2Qy0PptB__ufXm-TxUOl0KZzUw__dPhHATyREQrnntSIgxU8XNoYny4rjInxP7lLKSNxQyY.js"></script>
<script type="text/javascript" src="https://www.biorxiv.org/sites/default/files/advagg_js/js__2WRbxlwOW0MEUc_hSWU5MBepQg6Lch6O5SZwefpJ6IE__HCL0YQJqLkOhrLPZZYGqosGvtFsEHMGghHIkSx4y9vA__dPhHATyREQrnntSIgxU8XNoYny4rjInxP7lLKSNxQyY.js" defer="defer"></script>
<script type="text/javascript" src="https://www.biorxiv.org/sites/default/files/advagg_js/js__llgGUcz2-k3Jmyyyog-_4yI1wJfGWwx0eQ2lbnM9IIo__t6UjIjIKIAD6nKc6BAU9qmlVV2VR4CBTNQbxlbJPgN0__dPhHATyREQrnntSIgxU8XNoYny4rjInxP7lLKSNxQyY.js"></script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
function euCookieComplianceLoadScripts() {}
//--><!]]>
</script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
var eu_cookie_compliance_cookie_name = "";
//--><!]]>
</script>
<script type="text/javascript" src="//d33xdlntwy0kbs.cloudfront.net/cshl_custom.js"></script>
<script type="text/javascript" src="https://www.biorxiv.org/sites/default/files/advagg_js/js__BmqjBnkz3MgYCAoc25s1lDRMEjLhC3mEPVonUFIHi08__Unwv5-ZIuHBfFwytsjEx1niBVJ7n1T4lPws7VrkHXM4__dPhHATyREQrnntSIgxU8XNoYny4rjInxP7lLKSNxQyY.js"></script>
  </body>
</html>
